// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: FederationProtocol.proto

package org.apache.hadoop.hdfs.federation.protocol.proto;

public final class HdfsServerFederationProtos {
  private HdfsServerFederationProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface NamenodeMembershipStatsRecordProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint64 totalSpace = 1;
    /**
     * <code>optional uint64 totalSpace = 1;</code>
     */
    boolean hasTotalSpace();
    /**
     * <code>optional uint64 totalSpace = 1;</code>
     */
    long getTotalSpace();

    // optional uint64 availableSpace = 2;
    /**
     * <code>optional uint64 availableSpace = 2;</code>
     */
    boolean hasAvailableSpace();
    /**
     * <code>optional uint64 availableSpace = 2;</code>
     */
    long getAvailableSpace();

    // optional uint64 providedSpace = 3;
    /**
     * <code>optional uint64 providedSpace = 3;</code>
     */
    boolean hasProvidedSpace();
    /**
     * <code>optional uint64 providedSpace = 3;</code>
     */
    long getProvidedSpace();

    // optional uint64 numOfFiles = 10;
    /**
     * <code>optional uint64 numOfFiles = 10;</code>
     */
    boolean hasNumOfFiles();
    /**
     * <code>optional uint64 numOfFiles = 10;</code>
     */
    long getNumOfFiles();

    // optional uint64 numOfBlocks = 11;
    /**
     * <code>optional uint64 numOfBlocks = 11;</code>
     */
    boolean hasNumOfBlocks();
    /**
     * <code>optional uint64 numOfBlocks = 11;</code>
     */
    long getNumOfBlocks();

    // optional uint64 numOfBlocksMissing = 12;
    /**
     * <code>optional uint64 numOfBlocksMissing = 12;</code>
     */
    boolean hasNumOfBlocksMissing();
    /**
     * <code>optional uint64 numOfBlocksMissing = 12;</code>
     */
    long getNumOfBlocksMissing();

    // optional uint64 numOfBlocksPendingReplication = 13;
    /**
     * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
     */
    boolean hasNumOfBlocksPendingReplication();
    /**
     * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
     */
    long getNumOfBlocksPendingReplication();

    // optional uint64 numOfBlocksUnderReplicated = 14;
    /**
     * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
     */
    boolean hasNumOfBlocksUnderReplicated();
    /**
     * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
     */
    long getNumOfBlocksUnderReplicated();

    // optional uint64 numOfBlocksPendingDeletion = 15;
    /**
     * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
     */
    boolean hasNumOfBlocksPendingDeletion();
    /**
     * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
     */
    long getNumOfBlocksPendingDeletion();

    // optional uint32 numOfActiveDatanodes = 20;
    /**
     * <code>optional uint32 numOfActiveDatanodes = 20;</code>
     */
    boolean hasNumOfActiveDatanodes();
    /**
     * <code>optional uint32 numOfActiveDatanodes = 20;</code>
     */
    int getNumOfActiveDatanodes();

    // optional uint32 numOfDeadDatanodes = 21;
    /**
     * <code>optional uint32 numOfDeadDatanodes = 21;</code>
     */
    boolean hasNumOfDeadDatanodes();
    /**
     * <code>optional uint32 numOfDeadDatanodes = 21;</code>
     */
    int getNumOfDeadDatanodes();

    // optional uint32 numOfDecommissioningDatanodes = 22;
    /**
     * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
     */
    boolean hasNumOfDecommissioningDatanodes();
    /**
     * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
     */
    int getNumOfDecommissioningDatanodes();

    // optional uint32 numOfDecomActiveDatanodes = 23;
    /**
     * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
     */
    boolean hasNumOfDecomActiveDatanodes();
    /**
     * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
     */
    int getNumOfDecomActiveDatanodes();

    // optional uint32 numOfDecomDeadDatanodes = 24;
    /**
     * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
     */
    boolean hasNumOfDecomDeadDatanodes();
    /**
     * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
     */
    int getNumOfDecomDeadDatanodes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.NamenodeMembershipStatsRecordProto}
   */
  public static final class NamenodeMembershipStatsRecordProto extends
      com.google.protobuf.GeneratedMessage
      implements NamenodeMembershipStatsRecordProtoOrBuilder {
    // Use NamenodeMembershipStatsRecordProto.newBuilder() to construct.
    private NamenodeMembershipStatsRecordProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private NamenodeMembershipStatsRecordProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final NamenodeMembershipStatsRecordProto defaultInstance;
    public static NamenodeMembershipStatsRecordProto getDefaultInstance() {
      return defaultInstance;
    }

    public NamenodeMembershipStatsRecordProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private NamenodeMembershipStatsRecordProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              totalSpace_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              availableSpace_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              providedSpace_ = input.readUInt64();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000008;
              numOfFiles_ = input.readUInt64();
              break;
            }
            case 88: {
              bitField0_ |= 0x00000010;
              numOfBlocks_ = input.readUInt64();
              break;
            }
            case 96: {
              bitField0_ |= 0x00000020;
              numOfBlocksMissing_ = input.readUInt64();
              break;
            }
            case 104: {
              bitField0_ |= 0x00000040;
              numOfBlocksPendingReplication_ = input.readUInt64();
              break;
            }
            case 112: {
              bitField0_ |= 0x00000080;
              numOfBlocksUnderReplicated_ = input.readUInt64();
              break;
            }
            case 120: {
              bitField0_ |= 0x00000100;
              numOfBlocksPendingDeletion_ = input.readUInt64();
              break;
            }
            case 160: {
              bitField0_ |= 0x00000200;
              numOfActiveDatanodes_ = input.readUInt32();
              break;
            }
            case 168: {
              bitField0_ |= 0x00000400;
              numOfDeadDatanodes_ = input.readUInt32();
              break;
            }
            case 176: {
              bitField0_ |= 0x00000800;
              numOfDecommissioningDatanodes_ = input.readUInt32();
              break;
            }
            case 184: {
              bitField0_ |= 0x00001000;
              numOfDecomActiveDatanodes_ = input.readUInt32();
              break;
            }
            case 192: {
              bitField0_ |= 0x00002000;
              numOfDecomDeadDatanodes_ = input.readUInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder.class);
    }

    public static com.google.protobuf.Parser<NamenodeMembershipStatsRecordProto> PARSER =
        new com.google.protobuf.AbstractParser<NamenodeMembershipStatsRecordProto>() {
      public NamenodeMembershipStatsRecordProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NamenodeMembershipStatsRecordProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<NamenodeMembershipStatsRecordProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint64 totalSpace = 1;
    public static final int TOTALSPACE_FIELD_NUMBER = 1;
    private long totalSpace_;
    /**
     * <code>optional uint64 totalSpace = 1;</code>
     */
    public boolean hasTotalSpace() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 totalSpace = 1;</code>
     */
    public long getTotalSpace() {
      return totalSpace_;
    }

    // optional uint64 availableSpace = 2;
    public static final int AVAILABLESPACE_FIELD_NUMBER = 2;
    private long availableSpace_;
    /**
     * <code>optional uint64 availableSpace = 2;</code>
     */
    public boolean hasAvailableSpace() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 availableSpace = 2;</code>
     */
    public long getAvailableSpace() {
      return availableSpace_;
    }

    // optional uint64 providedSpace = 3;
    public static final int PROVIDEDSPACE_FIELD_NUMBER = 3;
    private long providedSpace_;
    /**
     * <code>optional uint64 providedSpace = 3;</code>
     */
    public boolean hasProvidedSpace() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint64 providedSpace = 3;</code>
     */
    public long getProvidedSpace() {
      return providedSpace_;
    }

    // optional uint64 numOfFiles = 10;
    public static final int NUMOFFILES_FIELD_NUMBER = 10;
    private long numOfFiles_;
    /**
     * <code>optional uint64 numOfFiles = 10;</code>
     */
    public boolean hasNumOfFiles() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint64 numOfFiles = 10;</code>
     */
    public long getNumOfFiles() {
      return numOfFiles_;
    }

    // optional uint64 numOfBlocks = 11;
    public static final int NUMOFBLOCKS_FIELD_NUMBER = 11;
    private long numOfBlocks_;
    /**
     * <code>optional uint64 numOfBlocks = 11;</code>
     */
    public boolean hasNumOfBlocks() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional uint64 numOfBlocks = 11;</code>
     */
    public long getNumOfBlocks() {
      return numOfBlocks_;
    }

    // optional uint64 numOfBlocksMissing = 12;
    public static final int NUMOFBLOCKSMISSING_FIELD_NUMBER = 12;
    private long numOfBlocksMissing_;
    /**
     * <code>optional uint64 numOfBlocksMissing = 12;</code>
     */
    public boolean hasNumOfBlocksMissing() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional uint64 numOfBlocksMissing = 12;</code>
     */
    public long getNumOfBlocksMissing() {
      return numOfBlocksMissing_;
    }

    // optional uint64 numOfBlocksPendingReplication = 13;
    public static final int NUMOFBLOCKSPENDINGREPLICATION_FIELD_NUMBER = 13;
    private long numOfBlocksPendingReplication_;
    /**
     * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
     */
    public boolean hasNumOfBlocksPendingReplication() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
     */
    public long getNumOfBlocksPendingReplication() {
      return numOfBlocksPendingReplication_;
    }

    // optional uint64 numOfBlocksUnderReplicated = 14;
    public static final int NUMOFBLOCKSUNDERREPLICATED_FIELD_NUMBER = 14;
    private long numOfBlocksUnderReplicated_;
    /**
     * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
     */
    public boolean hasNumOfBlocksUnderReplicated() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
     */
    public long getNumOfBlocksUnderReplicated() {
      return numOfBlocksUnderReplicated_;
    }

    // optional uint64 numOfBlocksPendingDeletion = 15;
    public static final int NUMOFBLOCKSPENDINGDELETION_FIELD_NUMBER = 15;
    private long numOfBlocksPendingDeletion_;
    /**
     * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
     */
    public boolean hasNumOfBlocksPendingDeletion() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
     */
    public long getNumOfBlocksPendingDeletion() {
      return numOfBlocksPendingDeletion_;
    }

    // optional uint32 numOfActiveDatanodes = 20;
    public static final int NUMOFACTIVEDATANODES_FIELD_NUMBER = 20;
    private int numOfActiveDatanodes_;
    /**
     * <code>optional uint32 numOfActiveDatanodes = 20;</code>
     */
    public boolean hasNumOfActiveDatanodes() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    /**
     * <code>optional uint32 numOfActiveDatanodes = 20;</code>
     */
    public int getNumOfActiveDatanodes() {
      return numOfActiveDatanodes_;
    }

    // optional uint32 numOfDeadDatanodes = 21;
    public static final int NUMOFDEADDATANODES_FIELD_NUMBER = 21;
    private int numOfDeadDatanodes_;
    /**
     * <code>optional uint32 numOfDeadDatanodes = 21;</code>
     */
    public boolean hasNumOfDeadDatanodes() {
      return ((bitField0_ & 0x00000400) == 0x00000400);
    }
    /**
     * <code>optional uint32 numOfDeadDatanodes = 21;</code>
     */
    public int getNumOfDeadDatanodes() {
      return numOfDeadDatanodes_;
    }

    // optional uint32 numOfDecommissioningDatanodes = 22;
    public static final int NUMOFDECOMMISSIONINGDATANODES_FIELD_NUMBER = 22;
    private int numOfDecommissioningDatanodes_;
    /**
     * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
     */
    public boolean hasNumOfDecommissioningDatanodes() {
      return ((bitField0_ & 0x00000800) == 0x00000800);
    }
    /**
     * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
     */
    public int getNumOfDecommissioningDatanodes() {
      return numOfDecommissioningDatanodes_;
    }

    // optional uint32 numOfDecomActiveDatanodes = 23;
    public static final int NUMOFDECOMACTIVEDATANODES_FIELD_NUMBER = 23;
    private int numOfDecomActiveDatanodes_;
    /**
     * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
     */
    public boolean hasNumOfDecomActiveDatanodes() {
      return ((bitField0_ & 0x00001000) == 0x00001000);
    }
    /**
     * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
     */
    public int getNumOfDecomActiveDatanodes() {
      return numOfDecomActiveDatanodes_;
    }

    // optional uint32 numOfDecomDeadDatanodes = 24;
    public static final int NUMOFDECOMDEADDATANODES_FIELD_NUMBER = 24;
    private int numOfDecomDeadDatanodes_;
    /**
     * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
     */
    public boolean hasNumOfDecomDeadDatanodes() {
      return ((bitField0_ & 0x00002000) == 0x00002000);
    }
    /**
     * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
     */
    public int getNumOfDecomDeadDatanodes() {
      return numOfDecomDeadDatanodes_;
    }

    private void initFields() {
      totalSpace_ = 0L;
      availableSpace_ = 0L;
      providedSpace_ = 0L;
      numOfFiles_ = 0L;
      numOfBlocks_ = 0L;
      numOfBlocksMissing_ = 0L;
      numOfBlocksPendingReplication_ = 0L;
      numOfBlocksUnderReplicated_ = 0L;
      numOfBlocksPendingDeletion_ = 0L;
      numOfActiveDatanodes_ = 0;
      numOfDeadDatanodes_ = 0;
      numOfDecommissioningDatanodes_ = 0;
      numOfDecomActiveDatanodes_ = 0;
      numOfDecomDeadDatanodes_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, totalSpace_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, availableSpace_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, providedSpace_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(10, numOfFiles_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeUInt64(11, numOfBlocks_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt64(12, numOfBlocksMissing_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt64(13, numOfBlocksPendingReplication_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeUInt64(14, numOfBlocksUnderReplicated_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeUInt64(15, numOfBlocksPendingDeletion_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeUInt32(20, numOfActiveDatanodes_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        output.writeUInt32(21, numOfDeadDatanodes_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        output.writeUInt32(22, numOfDecommissioningDatanodes_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        output.writeUInt32(23, numOfDecomActiveDatanodes_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        output.writeUInt32(24, numOfDecomDeadDatanodes_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, totalSpace_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, availableSpace_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, providedSpace_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(10, numOfFiles_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(11, numOfBlocks_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(12, numOfBlocksMissing_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(13, numOfBlocksPendingReplication_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(14, numOfBlocksUnderReplicated_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(15, numOfBlocksPendingDeletion_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(20, numOfActiveDatanodes_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(21, numOfDeadDatanodes_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(22, numOfDecommissioningDatanodes_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(23, numOfDecomActiveDatanodes_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(24, numOfDecomDeadDatanodes_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto) obj;

      boolean result = true;
      result = result && (hasTotalSpace() == other.hasTotalSpace());
      if (hasTotalSpace()) {
        result = result && (getTotalSpace()
            == other.getTotalSpace());
      }
      result = result && (hasAvailableSpace() == other.hasAvailableSpace());
      if (hasAvailableSpace()) {
        result = result && (getAvailableSpace()
            == other.getAvailableSpace());
      }
      result = result && (hasProvidedSpace() == other.hasProvidedSpace());
      if (hasProvidedSpace()) {
        result = result && (getProvidedSpace()
            == other.getProvidedSpace());
      }
      result = result && (hasNumOfFiles() == other.hasNumOfFiles());
      if (hasNumOfFiles()) {
        result = result && (getNumOfFiles()
            == other.getNumOfFiles());
      }
      result = result && (hasNumOfBlocks() == other.hasNumOfBlocks());
      if (hasNumOfBlocks()) {
        result = result && (getNumOfBlocks()
            == other.getNumOfBlocks());
      }
      result = result && (hasNumOfBlocksMissing() == other.hasNumOfBlocksMissing());
      if (hasNumOfBlocksMissing()) {
        result = result && (getNumOfBlocksMissing()
            == other.getNumOfBlocksMissing());
      }
      result = result && (hasNumOfBlocksPendingReplication() == other.hasNumOfBlocksPendingReplication());
      if (hasNumOfBlocksPendingReplication()) {
        result = result && (getNumOfBlocksPendingReplication()
            == other.getNumOfBlocksPendingReplication());
      }
      result = result && (hasNumOfBlocksUnderReplicated() == other.hasNumOfBlocksUnderReplicated());
      if (hasNumOfBlocksUnderReplicated()) {
        result = result && (getNumOfBlocksUnderReplicated()
            == other.getNumOfBlocksUnderReplicated());
      }
      result = result && (hasNumOfBlocksPendingDeletion() == other.hasNumOfBlocksPendingDeletion());
      if (hasNumOfBlocksPendingDeletion()) {
        result = result && (getNumOfBlocksPendingDeletion()
            == other.getNumOfBlocksPendingDeletion());
      }
      result = result && (hasNumOfActiveDatanodes() == other.hasNumOfActiveDatanodes());
      if (hasNumOfActiveDatanodes()) {
        result = result && (getNumOfActiveDatanodes()
            == other.getNumOfActiveDatanodes());
      }
      result = result && (hasNumOfDeadDatanodes() == other.hasNumOfDeadDatanodes());
      if (hasNumOfDeadDatanodes()) {
        result = result && (getNumOfDeadDatanodes()
            == other.getNumOfDeadDatanodes());
      }
      result = result && (hasNumOfDecommissioningDatanodes() == other.hasNumOfDecommissioningDatanodes());
      if (hasNumOfDecommissioningDatanodes()) {
        result = result && (getNumOfDecommissioningDatanodes()
            == other.getNumOfDecommissioningDatanodes());
      }
      result = result && (hasNumOfDecomActiveDatanodes() == other.hasNumOfDecomActiveDatanodes());
      if (hasNumOfDecomActiveDatanodes()) {
        result = result && (getNumOfDecomActiveDatanodes()
            == other.getNumOfDecomActiveDatanodes());
      }
      result = result && (hasNumOfDecomDeadDatanodes() == other.hasNumOfDecomDeadDatanodes());
      if (hasNumOfDecomDeadDatanodes()) {
        result = result && (getNumOfDecomDeadDatanodes()
            == other.getNumOfDecomDeadDatanodes());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTotalSpace()) {
        hash = (37 * hash) + TOTALSPACE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTotalSpace());
      }
      if (hasAvailableSpace()) {
        hash = (37 * hash) + AVAILABLESPACE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getAvailableSpace());
      }
      if (hasProvidedSpace()) {
        hash = (37 * hash) + PROVIDEDSPACE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getProvidedSpace());
      }
      if (hasNumOfFiles()) {
        hash = (37 * hash) + NUMOFFILES_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNumOfFiles());
      }
      if (hasNumOfBlocks()) {
        hash = (37 * hash) + NUMOFBLOCKS_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNumOfBlocks());
      }
      if (hasNumOfBlocksMissing()) {
        hash = (37 * hash) + NUMOFBLOCKSMISSING_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNumOfBlocksMissing());
      }
      if (hasNumOfBlocksPendingReplication()) {
        hash = (37 * hash) + NUMOFBLOCKSPENDINGREPLICATION_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNumOfBlocksPendingReplication());
      }
      if (hasNumOfBlocksUnderReplicated()) {
        hash = (37 * hash) + NUMOFBLOCKSUNDERREPLICATED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNumOfBlocksUnderReplicated());
      }
      if (hasNumOfBlocksPendingDeletion()) {
        hash = (37 * hash) + NUMOFBLOCKSPENDINGDELETION_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNumOfBlocksPendingDeletion());
      }
      if (hasNumOfActiveDatanodes()) {
        hash = (37 * hash) + NUMOFACTIVEDATANODES_FIELD_NUMBER;
        hash = (53 * hash) + getNumOfActiveDatanodes();
      }
      if (hasNumOfDeadDatanodes()) {
        hash = (37 * hash) + NUMOFDEADDATANODES_FIELD_NUMBER;
        hash = (53 * hash) + getNumOfDeadDatanodes();
      }
      if (hasNumOfDecommissioningDatanodes()) {
        hash = (37 * hash) + NUMOFDECOMMISSIONINGDATANODES_FIELD_NUMBER;
        hash = (53 * hash) + getNumOfDecommissioningDatanodes();
      }
      if (hasNumOfDecomActiveDatanodes()) {
        hash = (37 * hash) + NUMOFDECOMACTIVEDATANODES_FIELD_NUMBER;
        hash = (53 * hash) + getNumOfDecomActiveDatanodes();
      }
      if (hasNumOfDecomDeadDatanodes()) {
        hash = (37 * hash) + NUMOFDECOMDEADDATANODES_FIELD_NUMBER;
        hash = (53 * hash) + getNumOfDecomDeadDatanodes();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.NamenodeMembershipStatsRecordProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        totalSpace_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        availableSpace_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        providedSpace_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        numOfFiles_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        numOfBlocks_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        numOfBlocksMissing_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        numOfBlocksPendingReplication_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        numOfBlocksUnderReplicated_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        numOfBlocksPendingDeletion_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000100);
        numOfActiveDatanodes_ = 0;
        bitField0_ = (bitField0_ & ~0x00000200);
        numOfDeadDatanodes_ = 0;
        bitField0_ = (bitField0_ & ~0x00000400);
        numOfDecommissioningDatanodes_ = 0;
        bitField0_ = (bitField0_ & ~0x00000800);
        numOfDecomActiveDatanodes_ = 0;
        bitField0_ = (bitField0_ & ~0x00001000);
        numOfDecomDeadDatanodes_ = 0;
        bitField0_ = (bitField0_ & ~0x00002000);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.totalSpace_ = totalSpace_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.availableSpace_ = availableSpace_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.providedSpace_ = providedSpace_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.numOfFiles_ = numOfFiles_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.numOfBlocks_ = numOfBlocks_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.numOfBlocksMissing_ = numOfBlocksMissing_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.numOfBlocksPendingReplication_ = numOfBlocksPendingReplication_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        result.numOfBlocksUnderReplicated_ = numOfBlocksUnderReplicated_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000100;
        }
        result.numOfBlocksPendingDeletion_ = numOfBlocksPendingDeletion_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000200;
        }
        result.numOfActiveDatanodes_ = numOfActiveDatanodes_;
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000400;
        }
        result.numOfDeadDatanodes_ = numOfDeadDatanodes_;
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000800;
        }
        result.numOfDecommissioningDatanodes_ = numOfDecommissioningDatanodes_;
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00001000;
        }
        result.numOfDecomActiveDatanodes_ = numOfDecomActiveDatanodes_;
        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
          to_bitField0_ |= 0x00002000;
        }
        result.numOfDecomDeadDatanodes_ = numOfDecomDeadDatanodes_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.getDefaultInstance()) return this;
        if (other.hasTotalSpace()) {
          setTotalSpace(other.getTotalSpace());
        }
        if (other.hasAvailableSpace()) {
          setAvailableSpace(other.getAvailableSpace());
        }
        if (other.hasProvidedSpace()) {
          setProvidedSpace(other.getProvidedSpace());
        }
        if (other.hasNumOfFiles()) {
          setNumOfFiles(other.getNumOfFiles());
        }
        if (other.hasNumOfBlocks()) {
          setNumOfBlocks(other.getNumOfBlocks());
        }
        if (other.hasNumOfBlocksMissing()) {
          setNumOfBlocksMissing(other.getNumOfBlocksMissing());
        }
        if (other.hasNumOfBlocksPendingReplication()) {
          setNumOfBlocksPendingReplication(other.getNumOfBlocksPendingReplication());
        }
        if (other.hasNumOfBlocksUnderReplicated()) {
          setNumOfBlocksUnderReplicated(other.getNumOfBlocksUnderReplicated());
        }
        if (other.hasNumOfBlocksPendingDeletion()) {
          setNumOfBlocksPendingDeletion(other.getNumOfBlocksPendingDeletion());
        }
        if (other.hasNumOfActiveDatanodes()) {
          setNumOfActiveDatanodes(other.getNumOfActiveDatanodes());
        }
        if (other.hasNumOfDeadDatanodes()) {
          setNumOfDeadDatanodes(other.getNumOfDeadDatanodes());
        }
        if (other.hasNumOfDecommissioningDatanodes()) {
          setNumOfDecommissioningDatanodes(other.getNumOfDecommissioningDatanodes());
        }
        if (other.hasNumOfDecomActiveDatanodes()) {
          setNumOfDecomActiveDatanodes(other.getNumOfDecomActiveDatanodes());
        }
        if (other.hasNumOfDecomDeadDatanodes()) {
          setNumOfDecomDeadDatanodes(other.getNumOfDecomDeadDatanodes());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint64 totalSpace = 1;
      private long totalSpace_ ;
      /**
       * <code>optional uint64 totalSpace = 1;</code>
       */
      public boolean hasTotalSpace() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 totalSpace = 1;</code>
       */
      public long getTotalSpace() {
        return totalSpace_;
      }
      /**
       * <code>optional uint64 totalSpace = 1;</code>
       */
      public Builder setTotalSpace(long value) {
        bitField0_ |= 0x00000001;
        totalSpace_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 totalSpace = 1;</code>
       */
      public Builder clearTotalSpace() {
        bitField0_ = (bitField0_ & ~0x00000001);
        totalSpace_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 availableSpace = 2;
      private long availableSpace_ ;
      /**
       * <code>optional uint64 availableSpace = 2;</code>
       */
      public boolean hasAvailableSpace() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 availableSpace = 2;</code>
       */
      public long getAvailableSpace() {
        return availableSpace_;
      }
      /**
       * <code>optional uint64 availableSpace = 2;</code>
       */
      public Builder setAvailableSpace(long value) {
        bitField0_ |= 0x00000002;
        availableSpace_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 availableSpace = 2;</code>
       */
      public Builder clearAvailableSpace() {
        bitField0_ = (bitField0_ & ~0x00000002);
        availableSpace_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 providedSpace = 3;
      private long providedSpace_ ;
      /**
       * <code>optional uint64 providedSpace = 3;</code>
       */
      public boolean hasProvidedSpace() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 providedSpace = 3;</code>
       */
      public long getProvidedSpace() {
        return providedSpace_;
      }
      /**
       * <code>optional uint64 providedSpace = 3;</code>
       */
      public Builder setProvidedSpace(long value) {
        bitField0_ |= 0x00000004;
        providedSpace_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 providedSpace = 3;</code>
       */
      public Builder clearProvidedSpace() {
        bitField0_ = (bitField0_ & ~0x00000004);
        providedSpace_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 numOfFiles = 10;
      private long numOfFiles_ ;
      /**
       * <code>optional uint64 numOfFiles = 10;</code>
       */
      public boolean hasNumOfFiles() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 numOfFiles = 10;</code>
       */
      public long getNumOfFiles() {
        return numOfFiles_;
      }
      /**
       * <code>optional uint64 numOfFiles = 10;</code>
       */
      public Builder setNumOfFiles(long value) {
        bitField0_ |= 0x00000008;
        numOfFiles_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numOfFiles = 10;</code>
       */
      public Builder clearNumOfFiles() {
        bitField0_ = (bitField0_ & ~0x00000008);
        numOfFiles_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 numOfBlocks = 11;
      private long numOfBlocks_ ;
      /**
       * <code>optional uint64 numOfBlocks = 11;</code>
       */
      public boolean hasNumOfBlocks() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional uint64 numOfBlocks = 11;</code>
       */
      public long getNumOfBlocks() {
        return numOfBlocks_;
      }
      /**
       * <code>optional uint64 numOfBlocks = 11;</code>
       */
      public Builder setNumOfBlocks(long value) {
        bitField0_ |= 0x00000010;
        numOfBlocks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numOfBlocks = 11;</code>
       */
      public Builder clearNumOfBlocks() {
        bitField0_ = (bitField0_ & ~0x00000010);
        numOfBlocks_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 numOfBlocksMissing = 12;
      private long numOfBlocksMissing_ ;
      /**
       * <code>optional uint64 numOfBlocksMissing = 12;</code>
       */
      public boolean hasNumOfBlocksMissing() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional uint64 numOfBlocksMissing = 12;</code>
       */
      public long getNumOfBlocksMissing() {
        return numOfBlocksMissing_;
      }
      /**
       * <code>optional uint64 numOfBlocksMissing = 12;</code>
       */
      public Builder setNumOfBlocksMissing(long value) {
        bitField0_ |= 0x00000020;
        numOfBlocksMissing_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numOfBlocksMissing = 12;</code>
       */
      public Builder clearNumOfBlocksMissing() {
        bitField0_ = (bitField0_ & ~0x00000020);
        numOfBlocksMissing_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 numOfBlocksPendingReplication = 13;
      private long numOfBlocksPendingReplication_ ;
      /**
       * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
       */
      public boolean hasNumOfBlocksPendingReplication() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
       */
      public long getNumOfBlocksPendingReplication() {
        return numOfBlocksPendingReplication_;
      }
      /**
       * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
       */
      public Builder setNumOfBlocksPendingReplication(long value) {
        bitField0_ |= 0x00000040;
        numOfBlocksPendingReplication_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numOfBlocksPendingReplication = 13;</code>
       */
      public Builder clearNumOfBlocksPendingReplication() {
        bitField0_ = (bitField0_ & ~0x00000040);
        numOfBlocksPendingReplication_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 numOfBlocksUnderReplicated = 14;
      private long numOfBlocksUnderReplicated_ ;
      /**
       * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
       */
      public boolean hasNumOfBlocksUnderReplicated() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
       */
      public long getNumOfBlocksUnderReplicated() {
        return numOfBlocksUnderReplicated_;
      }
      /**
       * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
       */
      public Builder setNumOfBlocksUnderReplicated(long value) {
        bitField0_ |= 0x00000080;
        numOfBlocksUnderReplicated_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numOfBlocksUnderReplicated = 14;</code>
       */
      public Builder clearNumOfBlocksUnderReplicated() {
        bitField0_ = (bitField0_ & ~0x00000080);
        numOfBlocksUnderReplicated_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 numOfBlocksPendingDeletion = 15;
      private long numOfBlocksPendingDeletion_ ;
      /**
       * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
       */
      public boolean hasNumOfBlocksPendingDeletion() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
       */
      public long getNumOfBlocksPendingDeletion() {
        return numOfBlocksPendingDeletion_;
      }
      /**
       * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
       */
      public Builder setNumOfBlocksPendingDeletion(long value) {
        bitField0_ |= 0x00000100;
        numOfBlocksPendingDeletion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numOfBlocksPendingDeletion = 15;</code>
       */
      public Builder clearNumOfBlocksPendingDeletion() {
        bitField0_ = (bitField0_ & ~0x00000100);
        numOfBlocksPendingDeletion_ = 0L;
        onChanged();
        return this;
      }

      // optional uint32 numOfActiveDatanodes = 20;
      private int numOfActiveDatanodes_ ;
      /**
       * <code>optional uint32 numOfActiveDatanodes = 20;</code>
       */
      public boolean hasNumOfActiveDatanodes() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional uint32 numOfActiveDatanodes = 20;</code>
       */
      public int getNumOfActiveDatanodes() {
        return numOfActiveDatanodes_;
      }
      /**
       * <code>optional uint32 numOfActiveDatanodes = 20;</code>
       */
      public Builder setNumOfActiveDatanodes(int value) {
        bitField0_ |= 0x00000200;
        numOfActiveDatanodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numOfActiveDatanodes = 20;</code>
       */
      public Builder clearNumOfActiveDatanodes() {
        bitField0_ = (bitField0_ & ~0x00000200);
        numOfActiveDatanodes_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 numOfDeadDatanodes = 21;
      private int numOfDeadDatanodes_ ;
      /**
       * <code>optional uint32 numOfDeadDatanodes = 21;</code>
       */
      public boolean hasNumOfDeadDatanodes() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional uint32 numOfDeadDatanodes = 21;</code>
       */
      public int getNumOfDeadDatanodes() {
        return numOfDeadDatanodes_;
      }
      /**
       * <code>optional uint32 numOfDeadDatanodes = 21;</code>
       */
      public Builder setNumOfDeadDatanodes(int value) {
        bitField0_ |= 0x00000400;
        numOfDeadDatanodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numOfDeadDatanodes = 21;</code>
       */
      public Builder clearNumOfDeadDatanodes() {
        bitField0_ = (bitField0_ & ~0x00000400);
        numOfDeadDatanodes_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 numOfDecommissioningDatanodes = 22;
      private int numOfDecommissioningDatanodes_ ;
      /**
       * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
       */
      public boolean hasNumOfDecommissioningDatanodes() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      /**
       * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
       */
      public int getNumOfDecommissioningDatanodes() {
        return numOfDecommissioningDatanodes_;
      }
      /**
       * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
       */
      public Builder setNumOfDecommissioningDatanodes(int value) {
        bitField0_ |= 0x00000800;
        numOfDecommissioningDatanodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numOfDecommissioningDatanodes = 22;</code>
       */
      public Builder clearNumOfDecommissioningDatanodes() {
        bitField0_ = (bitField0_ & ~0x00000800);
        numOfDecommissioningDatanodes_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 numOfDecomActiveDatanodes = 23;
      private int numOfDecomActiveDatanodes_ ;
      /**
       * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
       */
      public boolean hasNumOfDecomActiveDatanodes() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      /**
       * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
       */
      public int getNumOfDecomActiveDatanodes() {
        return numOfDecomActiveDatanodes_;
      }
      /**
       * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
       */
      public Builder setNumOfDecomActiveDatanodes(int value) {
        bitField0_ |= 0x00001000;
        numOfDecomActiveDatanodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numOfDecomActiveDatanodes = 23;</code>
       */
      public Builder clearNumOfDecomActiveDatanodes() {
        bitField0_ = (bitField0_ & ~0x00001000);
        numOfDecomActiveDatanodes_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 numOfDecomDeadDatanodes = 24;
      private int numOfDecomDeadDatanodes_ ;
      /**
       * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
       */
      public boolean hasNumOfDecomDeadDatanodes() {
        return ((bitField0_ & 0x00002000) == 0x00002000);
      }
      /**
       * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
       */
      public int getNumOfDecomDeadDatanodes() {
        return numOfDecomDeadDatanodes_;
      }
      /**
       * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
       */
      public Builder setNumOfDecomDeadDatanodes(int value) {
        bitField0_ |= 0x00002000;
        numOfDecomDeadDatanodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numOfDecomDeadDatanodes = 24;</code>
       */
      public Builder clearNumOfDecomDeadDatanodes() {
        bitField0_ = (bitField0_ & ~0x00002000);
        numOfDecomDeadDatanodes_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.NamenodeMembershipStatsRecordProto)
    }

    static {
      defaultInstance = new NamenodeMembershipStatsRecordProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.NamenodeMembershipStatsRecordProto)
  }

  public interface NamenodeMembershipRecordProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint64 dateCreated = 1;
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    boolean hasDateCreated();
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    long getDateCreated();

    // optional uint64 dateModified = 2;
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    boolean hasDateModified();
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    long getDateModified();

    // optional uint64 lastContact = 3;
    /**
     * <code>optional uint64 lastContact = 3;</code>
     */
    boolean hasLastContact();
    /**
     * <code>optional uint64 lastContact = 3;</code>
     */
    long getLastContact();

    // optional string routerId = 4;
    /**
     * <code>optional string routerId = 4;</code>
     */
    boolean hasRouterId();
    /**
     * <code>optional string routerId = 4;</code>
     */
    java.lang.String getRouterId();
    /**
     * <code>optional string routerId = 4;</code>
     */
    com.google.protobuf.ByteString
        getRouterIdBytes();

    // optional string nameserviceId = 5;
    /**
     * <code>optional string nameserviceId = 5;</code>
     */
    boolean hasNameserviceId();
    /**
     * <code>optional string nameserviceId = 5;</code>
     */
    java.lang.String getNameserviceId();
    /**
     * <code>optional string nameserviceId = 5;</code>
     */
    com.google.protobuf.ByteString
        getNameserviceIdBytes();

    // optional string namenodeId = 6;
    /**
     * <code>optional string namenodeId = 6;</code>
     */
    boolean hasNamenodeId();
    /**
     * <code>optional string namenodeId = 6;</code>
     */
    java.lang.String getNamenodeId();
    /**
     * <code>optional string namenodeId = 6;</code>
     */
    com.google.protobuf.ByteString
        getNamenodeIdBytes();

    // optional string clusterId = 7;
    /**
     * <code>optional string clusterId = 7;</code>
     */
    boolean hasClusterId();
    /**
     * <code>optional string clusterId = 7;</code>
     */
    java.lang.String getClusterId();
    /**
     * <code>optional string clusterId = 7;</code>
     */
    com.google.protobuf.ByteString
        getClusterIdBytes();

    // optional string blockPoolId = 8;
    /**
     * <code>optional string blockPoolId = 8;</code>
     */
    boolean hasBlockPoolId();
    /**
     * <code>optional string blockPoolId = 8;</code>
     */
    java.lang.String getBlockPoolId();
    /**
     * <code>optional string blockPoolId = 8;</code>
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();

    // optional string webAddress = 9;
    /**
     * <code>optional string webAddress = 9;</code>
     */
    boolean hasWebAddress();
    /**
     * <code>optional string webAddress = 9;</code>
     */
    java.lang.String getWebAddress();
    /**
     * <code>optional string webAddress = 9;</code>
     */
    com.google.protobuf.ByteString
        getWebAddressBytes();

    // optional string rpcAddress = 10;
    /**
     * <code>optional string rpcAddress = 10;</code>
     */
    boolean hasRpcAddress();
    /**
     * <code>optional string rpcAddress = 10;</code>
     */
    java.lang.String getRpcAddress();
    /**
     * <code>optional string rpcAddress = 10;</code>
     */
    com.google.protobuf.ByteString
        getRpcAddressBytes();

    // optional string serviceAddress = 11;
    /**
     * <code>optional string serviceAddress = 11;</code>
     */
    boolean hasServiceAddress();
    /**
     * <code>optional string serviceAddress = 11;</code>
     */
    java.lang.String getServiceAddress();
    /**
     * <code>optional string serviceAddress = 11;</code>
     */
    com.google.protobuf.ByteString
        getServiceAddressBytes();

    // optional string lifelineAddress = 12;
    /**
     * <code>optional string lifelineAddress = 12;</code>
     */
    boolean hasLifelineAddress();
    /**
     * <code>optional string lifelineAddress = 12;</code>
     */
    java.lang.String getLifelineAddress();
    /**
     * <code>optional string lifelineAddress = 12;</code>
     */
    com.google.protobuf.ByteString
        getLifelineAddressBytes();

    // optional string state = 13;
    /**
     * <code>optional string state = 13;</code>
     */
    boolean hasState();
    /**
     * <code>optional string state = 13;</code>
     */
    java.lang.String getState();
    /**
     * <code>optional string state = 13;</code>
     */
    com.google.protobuf.ByteString
        getStateBytes();

    // optional bool isSafeMode = 14;
    /**
     * <code>optional bool isSafeMode = 14;</code>
     */
    boolean hasIsSafeMode();
    /**
     * <code>optional bool isSafeMode = 14;</code>
     */
    boolean getIsSafeMode();

    // optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
     */
    boolean hasStats();
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto getStats();
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder getStatsOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.NamenodeMembershipRecordProto}
   */
  public static final class NamenodeMembershipRecordProto extends
      com.google.protobuf.GeneratedMessage
      implements NamenodeMembershipRecordProtoOrBuilder {
    // Use NamenodeMembershipRecordProto.newBuilder() to construct.
    private NamenodeMembershipRecordProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private NamenodeMembershipRecordProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final NamenodeMembershipRecordProto defaultInstance;
    public static NamenodeMembershipRecordProto getDefaultInstance() {
      return defaultInstance;
    }

    public NamenodeMembershipRecordProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private NamenodeMembershipRecordProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              dateCreated_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              dateModified_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              lastContact_ = input.readUInt64();
              break;
            }
            case 34: {
              bitField0_ |= 0x00000008;
              routerId_ = input.readBytes();
              break;
            }
            case 42: {
              bitField0_ |= 0x00000010;
              nameserviceId_ = input.readBytes();
              break;
            }
            case 50: {
              bitField0_ |= 0x00000020;
              namenodeId_ = input.readBytes();
              break;
            }
            case 58: {
              bitField0_ |= 0x00000040;
              clusterId_ = input.readBytes();
              break;
            }
            case 66: {
              bitField0_ |= 0x00000080;
              blockPoolId_ = input.readBytes();
              break;
            }
            case 74: {
              bitField0_ |= 0x00000100;
              webAddress_ = input.readBytes();
              break;
            }
            case 82: {
              bitField0_ |= 0x00000200;
              rpcAddress_ = input.readBytes();
              break;
            }
            case 90: {
              bitField0_ |= 0x00000400;
              serviceAddress_ = input.readBytes();
              break;
            }
            case 98: {
              bitField0_ |= 0x00000800;
              lifelineAddress_ = input.readBytes();
              break;
            }
            case 106: {
              bitField0_ |= 0x00001000;
              state_ = input.readBytes();
              break;
            }
            case 112: {
              bitField0_ |= 0x00002000;
              isSafeMode_ = input.readBool();
              break;
            }
            case 122: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00004000) == 0x00004000)) {
                subBuilder = stats_.toBuilder();
              }
              stats_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(stats_);
                stats_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00004000;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder.class);
    }

    public static com.google.protobuf.Parser<NamenodeMembershipRecordProto> PARSER =
        new com.google.protobuf.AbstractParser<NamenodeMembershipRecordProto>() {
      public NamenodeMembershipRecordProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NamenodeMembershipRecordProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<NamenodeMembershipRecordProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint64 dateCreated = 1;
    public static final int DATECREATED_FIELD_NUMBER = 1;
    private long dateCreated_;
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    public boolean hasDateCreated() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    public long getDateCreated() {
      return dateCreated_;
    }

    // optional uint64 dateModified = 2;
    public static final int DATEMODIFIED_FIELD_NUMBER = 2;
    private long dateModified_;
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    public boolean hasDateModified() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    public long getDateModified() {
      return dateModified_;
    }

    // optional uint64 lastContact = 3;
    public static final int LASTCONTACT_FIELD_NUMBER = 3;
    private long lastContact_;
    /**
     * <code>optional uint64 lastContact = 3;</code>
     */
    public boolean hasLastContact() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint64 lastContact = 3;</code>
     */
    public long getLastContact() {
      return lastContact_;
    }

    // optional string routerId = 4;
    public static final int ROUTERID_FIELD_NUMBER = 4;
    private java.lang.Object routerId_;
    /**
     * <code>optional string routerId = 4;</code>
     */
    public boolean hasRouterId() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional string routerId = 4;</code>
     */
    public java.lang.String getRouterId() {
      java.lang.Object ref = routerId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          routerId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string routerId = 4;</code>
     */
    public com.google.protobuf.ByteString
        getRouterIdBytes() {
      java.lang.Object ref = routerId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        routerId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string nameserviceId = 5;
    public static final int NAMESERVICEID_FIELD_NUMBER = 5;
    private java.lang.Object nameserviceId_;
    /**
     * <code>optional string nameserviceId = 5;</code>
     */
    public boolean hasNameserviceId() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional string nameserviceId = 5;</code>
     */
    public java.lang.String getNameserviceId() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nameserviceId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nameserviceId = 5;</code>
     */
    public com.google.protobuf.ByteString
        getNameserviceIdBytes() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nameserviceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string namenodeId = 6;
    public static final int NAMENODEID_FIELD_NUMBER = 6;
    private java.lang.Object namenodeId_;
    /**
     * <code>optional string namenodeId = 6;</code>
     */
    public boolean hasNamenodeId() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional string namenodeId = 6;</code>
     */
    public java.lang.String getNamenodeId() {
      java.lang.Object ref = namenodeId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          namenodeId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string namenodeId = 6;</code>
     */
    public com.google.protobuf.ByteString
        getNamenodeIdBytes() {
      java.lang.Object ref = namenodeId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        namenodeId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string clusterId = 7;
    public static final int CLUSTERID_FIELD_NUMBER = 7;
    private java.lang.Object clusterId_;
    /**
     * <code>optional string clusterId = 7;</code>
     */
    public boolean hasClusterId() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional string clusterId = 7;</code>
     */
    public java.lang.String getClusterId() {
      java.lang.Object ref = clusterId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          clusterId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string clusterId = 7;</code>
     */
    public com.google.protobuf.ByteString
        getClusterIdBytes() {
      java.lang.Object ref = clusterId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        clusterId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string blockPoolId = 8;
    public static final int BLOCKPOOLID_FIELD_NUMBER = 8;
    private java.lang.Object blockPoolId_;
    /**
     * <code>optional string blockPoolId = 8;</code>
     */
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional string blockPoolId = 8;</code>
     */
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string blockPoolId = 8;</code>
     */
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string webAddress = 9;
    public static final int WEBADDRESS_FIELD_NUMBER = 9;
    private java.lang.Object webAddress_;
    /**
     * <code>optional string webAddress = 9;</code>
     */
    public boolean hasWebAddress() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional string webAddress = 9;</code>
     */
    public java.lang.String getWebAddress() {
      java.lang.Object ref = webAddress_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          webAddress_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string webAddress = 9;</code>
     */
    public com.google.protobuf.ByteString
        getWebAddressBytes() {
      java.lang.Object ref = webAddress_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        webAddress_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string rpcAddress = 10;
    public static final int RPCADDRESS_FIELD_NUMBER = 10;
    private java.lang.Object rpcAddress_;
    /**
     * <code>optional string rpcAddress = 10;</code>
     */
    public boolean hasRpcAddress() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    /**
     * <code>optional string rpcAddress = 10;</code>
     */
    public java.lang.String getRpcAddress() {
      java.lang.Object ref = rpcAddress_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          rpcAddress_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string rpcAddress = 10;</code>
     */
    public com.google.protobuf.ByteString
        getRpcAddressBytes() {
      java.lang.Object ref = rpcAddress_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        rpcAddress_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string serviceAddress = 11;
    public static final int SERVICEADDRESS_FIELD_NUMBER = 11;
    private java.lang.Object serviceAddress_;
    /**
     * <code>optional string serviceAddress = 11;</code>
     */
    public boolean hasServiceAddress() {
      return ((bitField0_ & 0x00000400) == 0x00000400);
    }
    /**
     * <code>optional string serviceAddress = 11;</code>
     */
    public java.lang.String getServiceAddress() {
      java.lang.Object ref = serviceAddress_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          serviceAddress_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string serviceAddress = 11;</code>
     */
    public com.google.protobuf.ByteString
        getServiceAddressBytes() {
      java.lang.Object ref = serviceAddress_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        serviceAddress_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string lifelineAddress = 12;
    public static final int LIFELINEADDRESS_FIELD_NUMBER = 12;
    private java.lang.Object lifelineAddress_;
    /**
     * <code>optional string lifelineAddress = 12;</code>
     */
    public boolean hasLifelineAddress() {
      return ((bitField0_ & 0x00000800) == 0x00000800);
    }
    /**
     * <code>optional string lifelineAddress = 12;</code>
     */
    public java.lang.String getLifelineAddress() {
      java.lang.Object ref = lifelineAddress_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          lifelineAddress_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string lifelineAddress = 12;</code>
     */
    public com.google.protobuf.ByteString
        getLifelineAddressBytes() {
      java.lang.Object ref = lifelineAddress_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        lifelineAddress_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string state = 13;
    public static final int STATE_FIELD_NUMBER = 13;
    private java.lang.Object state_;
    /**
     * <code>optional string state = 13;</code>
     */
    public boolean hasState() {
      return ((bitField0_ & 0x00001000) == 0x00001000);
    }
    /**
     * <code>optional string state = 13;</code>
     */
    public java.lang.String getState() {
      java.lang.Object ref = state_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          state_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string state = 13;</code>
     */
    public com.google.protobuf.ByteString
        getStateBytes() {
      java.lang.Object ref = state_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        state_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional bool isSafeMode = 14;
    public static final int ISSAFEMODE_FIELD_NUMBER = 14;
    private boolean isSafeMode_;
    /**
     * <code>optional bool isSafeMode = 14;</code>
     */
    public boolean hasIsSafeMode() {
      return ((bitField0_ & 0x00002000) == 0x00002000);
    }
    /**
     * <code>optional bool isSafeMode = 14;</code>
     */
    public boolean getIsSafeMode() {
      return isSafeMode_;
    }

    // optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;
    public static final int STATS_FIELD_NUMBER = 15;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto stats_;
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
     */
    public boolean hasStats() {
      return ((bitField0_ & 0x00004000) == 0x00004000);
    }
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto getStats() {
      return stats_;
    }
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder getStatsOrBuilder() {
      return stats_;
    }

    private void initFields() {
      dateCreated_ = 0L;
      dateModified_ = 0L;
      lastContact_ = 0L;
      routerId_ = "";
      nameserviceId_ = "";
      namenodeId_ = "";
      clusterId_ = "";
      blockPoolId_ = "";
      webAddress_ = "";
      rpcAddress_ = "";
      serviceAddress_ = "";
      lifelineAddress_ = "";
      state_ = "";
      isSafeMode_ = false;
      stats_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, dateCreated_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, dateModified_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, lastContact_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(4, getRouterIdBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBytes(5, getNameserviceIdBytes());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeBytes(6, getNamenodeIdBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeBytes(7, getClusterIdBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeBytes(8, getBlockPoolIdBytes());
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeBytes(9, getWebAddressBytes());
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeBytes(10, getRpcAddressBytes());
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        output.writeBytes(11, getServiceAddressBytes());
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        output.writeBytes(12, getLifelineAddressBytes());
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        output.writeBytes(13, getStateBytes());
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        output.writeBool(14, isSafeMode_);
      }
      if (((bitField0_ & 0x00004000) == 0x00004000)) {
        output.writeMessage(15, stats_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, dateCreated_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, dateModified_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, lastContact_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(4, getRouterIdBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(5, getNameserviceIdBytes());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, getNamenodeIdBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(7, getClusterIdBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(8, getBlockPoolIdBytes());
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(9, getWebAddressBytes());
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(10, getRpcAddressBytes());
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(11, getServiceAddressBytes());
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(12, getLifelineAddressBytes());
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(13, getStateBytes());
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(14, isSafeMode_);
      }
      if (((bitField0_ & 0x00004000) == 0x00004000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(15, stats_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto) obj;

      boolean result = true;
      result = result && (hasDateCreated() == other.hasDateCreated());
      if (hasDateCreated()) {
        result = result && (getDateCreated()
            == other.getDateCreated());
      }
      result = result && (hasDateModified() == other.hasDateModified());
      if (hasDateModified()) {
        result = result && (getDateModified()
            == other.getDateModified());
      }
      result = result && (hasLastContact() == other.hasLastContact());
      if (hasLastContact()) {
        result = result && (getLastContact()
            == other.getLastContact());
      }
      result = result && (hasRouterId() == other.hasRouterId());
      if (hasRouterId()) {
        result = result && getRouterId()
            .equals(other.getRouterId());
      }
      result = result && (hasNameserviceId() == other.hasNameserviceId());
      if (hasNameserviceId()) {
        result = result && getNameserviceId()
            .equals(other.getNameserviceId());
      }
      result = result && (hasNamenodeId() == other.hasNamenodeId());
      if (hasNamenodeId()) {
        result = result && getNamenodeId()
            .equals(other.getNamenodeId());
      }
      result = result && (hasClusterId() == other.hasClusterId());
      if (hasClusterId()) {
        result = result && getClusterId()
            .equals(other.getClusterId());
      }
      result = result && (hasBlockPoolId() == other.hasBlockPoolId());
      if (hasBlockPoolId()) {
        result = result && getBlockPoolId()
            .equals(other.getBlockPoolId());
      }
      result = result && (hasWebAddress() == other.hasWebAddress());
      if (hasWebAddress()) {
        result = result && getWebAddress()
            .equals(other.getWebAddress());
      }
      result = result && (hasRpcAddress() == other.hasRpcAddress());
      if (hasRpcAddress()) {
        result = result && getRpcAddress()
            .equals(other.getRpcAddress());
      }
      result = result && (hasServiceAddress() == other.hasServiceAddress());
      if (hasServiceAddress()) {
        result = result && getServiceAddress()
            .equals(other.getServiceAddress());
      }
      result = result && (hasLifelineAddress() == other.hasLifelineAddress());
      if (hasLifelineAddress()) {
        result = result && getLifelineAddress()
            .equals(other.getLifelineAddress());
      }
      result = result && (hasState() == other.hasState());
      if (hasState()) {
        result = result && getState()
            .equals(other.getState());
      }
      result = result && (hasIsSafeMode() == other.hasIsSafeMode());
      if (hasIsSafeMode()) {
        result = result && (getIsSafeMode()
            == other.getIsSafeMode());
      }
      result = result && (hasStats() == other.hasStats());
      if (hasStats()) {
        result = result && getStats()
            .equals(other.getStats());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasDateCreated()) {
        hash = (37 * hash) + DATECREATED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateCreated());
      }
      if (hasDateModified()) {
        hash = (37 * hash) + DATEMODIFIED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateModified());
      }
      if (hasLastContact()) {
        hash = (37 * hash) + LASTCONTACT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getLastContact());
      }
      if (hasRouterId()) {
        hash = (37 * hash) + ROUTERID_FIELD_NUMBER;
        hash = (53 * hash) + getRouterId().hashCode();
      }
      if (hasNameserviceId()) {
        hash = (37 * hash) + NAMESERVICEID_FIELD_NUMBER;
        hash = (53 * hash) + getNameserviceId().hashCode();
      }
      if (hasNamenodeId()) {
        hash = (37 * hash) + NAMENODEID_FIELD_NUMBER;
        hash = (53 * hash) + getNamenodeId().hashCode();
      }
      if (hasClusterId()) {
        hash = (37 * hash) + CLUSTERID_FIELD_NUMBER;
        hash = (53 * hash) + getClusterId().hashCode();
      }
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      if (hasWebAddress()) {
        hash = (37 * hash) + WEBADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getWebAddress().hashCode();
      }
      if (hasRpcAddress()) {
        hash = (37 * hash) + RPCADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getRpcAddress().hashCode();
      }
      if (hasServiceAddress()) {
        hash = (37 * hash) + SERVICEADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getServiceAddress().hashCode();
      }
      if (hasLifelineAddress()) {
        hash = (37 * hash) + LIFELINEADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getLifelineAddress().hashCode();
      }
      if (hasState()) {
        hash = (37 * hash) + STATE_FIELD_NUMBER;
        hash = (53 * hash) + getState().hashCode();
      }
      if (hasIsSafeMode()) {
        hash = (37 * hash) + ISSAFEMODE_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIsSafeMode());
      }
      if (hasStats()) {
        hash = (37 * hash) + STATS_FIELD_NUMBER;
        hash = (53 * hash) + getStats().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.NamenodeMembershipRecordProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStatsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        dateCreated_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        dateModified_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        lastContact_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        routerId_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        nameserviceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        namenodeId_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        clusterId_ = "";
        bitField0_ = (bitField0_ & ~0x00000040);
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        webAddress_ = "";
        bitField0_ = (bitField0_ & ~0x00000100);
        rpcAddress_ = "";
        bitField0_ = (bitField0_ & ~0x00000200);
        serviceAddress_ = "";
        bitField0_ = (bitField0_ & ~0x00000400);
        lifelineAddress_ = "";
        bitField0_ = (bitField0_ & ~0x00000800);
        state_ = "";
        bitField0_ = (bitField0_ & ~0x00001000);
        isSafeMode_ = false;
        bitField0_ = (bitField0_ & ~0x00002000);
        if (statsBuilder_ == null) {
          stats_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.getDefaultInstance();
        } else {
          statsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.dateCreated_ = dateCreated_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.dateModified_ = dateModified_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.lastContact_ = lastContact_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.routerId_ = routerId_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.nameserviceId_ = nameserviceId_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.namenodeId_ = namenodeId_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.clusterId_ = clusterId_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        result.blockPoolId_ = blockPoolId_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000100;
        }
        result.webAddress_ = webAddress_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000200;
        }
        result.rpcAddress_ = rpcAddress_;
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000400;
        }
        result.serviceAddress_ = serviceAddress_;
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000800;
        }
        result.lifelineAddress_ = lifelineAddress_;
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00001000;
        }
        result.state_ = state_;
        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
          to_bitField0_ |= 0x00002000;
        }
        result.isSafeMode_ = isSafeMode_;
        if (((from_bitField0_ & 0x00004000) == 0x00004000)) {
          to_bitField0_ |= 0x00004000;
        }
        if (statsBuilder_ == null) {
          result.stats_ = stats_;
        } else {
          result.stats_ = statsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance()) return this;
        if (other.hasDateCreated()) {
          setDateCreated(other.getDateCreated());
        }
        if (other.hasDateModified()) {
          setDateModified(other.getDateModified());
        }
        if (other.hasLastContact()) {
          setLastContact(other.getLastContact());
        }
        if (other.hasRouterId()) {
          bitField0_ |= 0x00000008;
          routerId_ = other.routerId_;
          onChanged();
        }
        if (other.hasNameserviceId()) {
          bitField0_ |= 0x00000010;
          nameserviceId_ = other.nameserviceId_;
          onChanged();
        }
        if (other.hasNamenodeId()) {
          bitField0_ |= 0x00000020;
          namenodeId_ = other.namenodeId_;
          onChanged();
        }
        if (other.hasClusterId()) {
          bitField0_ |= 0x00000040;
          clusterId_ = other.clusterId_;
          onChanged();
        }
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000080;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        if (other.hasWebAddress()) {
          bitField0_ |= 0x00000100;
          webAddress_ = other.webAddress_;
          onChanged();
        }
        if (other.hasRpcAddress()) {
          bitField0_ |= 0x00000200;
          rpcAddress_ = other.rpcAddress_;
          onChanged();
        }
        if (other.hasServiceAddress()) {
          bitField0_ |= 0x00000400;
          serviceAddress_ = other.serviceAddress_;
          onChanged();
        }
        if (other.hasLifelineAddress()) {
          bitField0_ |= 0x00000800;
          lifelineAddress_ = other.lifelineAddress_;
          onChanged();
        }
        if (other.hasState()) {
          bitField0_ |= 0x00001000;
          state_ = other.state_;
          onChanged();
        }
        if (other.hasIsSafeMode()) {
          setIsSafeMode(other.getIsSafeMode());
        }
        if (other.hasStats()) {
          mergeStats(other.getStats());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint64 dateCreated = 1;
      private long dateCreated_ ;
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public boolean hasDateCreated() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public long getDateCreated() {
        return dateCreated_;
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public Builder setDateCreated(long value) {
        bitField0_ |= 0x00000001;
        dateCreated_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public Builder clearDateCreated() {
        bitField0_ = (bitField0_ & ~0x00000001);
        dateCreated_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 dateModified = 2;
      private long dateModified_ ;
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public boolean hasDateModified() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public long getDateModified() {
        return dateModified_;
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public Builder setDateModified(long value) {
        bitField0_ |= 0x00000002;
        dateModified_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public Builder clearDateModified() {
        bitField0_ = (bitField0_ & ~0x00000002);
        dateModified_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 lastContact = 3;
      private long lastContact_ ;
      /**
       * <code>optional uint64 lastContact = 3;</code>
       */
      public boolean hasLastContact() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 lastContact = 3;</code>
       */
      public long getLastContact() {
        return lastContact_;
      }
      /**
       * <code>optional uint64 lastContact = 3;</code>
       */
      public Builder setLastContact(long value) {
        bitField0_ |= 0x00000004;
        lastContact_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 lastContact = 3;</code>
       */
      public Builder clearLastContact() {
        bitField0_ = (bitField0_ & ~0x00000004);
        lastContact_ = 0L;
        onChanged();
        return this;
      }

      // optional string routerId = 4;
      private java.lang.Object routerId_ = "";
      /**
       * <code>optional string routerId = 4;</code>
       */
      public boolean hasRouterId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional string routerId = 4;</code>
       */
      public java.lang.String getRouterId() {
        java.lang.Object ref = routerId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          routerId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string routerId = 4;</code>
       */
      public com.google.protobuf.ByteString
          getRouterIdBytes() {
        java.lang.Object ref = routerId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          routerId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string routerId = 4;</code>
       */
      public Builder setRouterId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        routerId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string routerId = 4;</code>
       */
      public Builder clearRouterId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        routerId_ = getDefaultInstance().getRouterId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string routerId = 4;</code>
       */
      public Builder setRouterIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        routerId_ = value;
        onChanged();
        return this;
      }

      // optional string nameserviceId = 5;
      private java.lang.Object nameserviceId_ = "";
      /**
       * <code>optional string nameserviceId = 5;</code>
       */
      public boolean hasNameserviceId() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional string nameserviceId = 5;</code>
       */
      public java.lang.String getNameserviceId() {
        java.lang.Object ref = nameserviceId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          nameserviceId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 5;</code>
       */
      public com.google.protobuf.ByteString
          getNameserviceIdBytes() {
        java.lang.Object ref = nameserviceId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nameserviceId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 5;</code>
       */
      public Builder setNameserviceId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        nameserviceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 5;</code>
       */
      public Builder clearNameserviceId() {
        bitField0_ = (bitField0_ & ~0x00000010);
        nameserviceId_ = getDefaultInstance().getNameserviceId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 5;</code>
       */
      public Builder setNameserviceIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        nameserviceId_ = value;
        onChanged();
        return this;
      }

      // optional string namenodeId = 6;
      private java.lang.Object namenodeId_ = "";
      /**
       * <code>optional string namenodeId = 6;</code>
       */
      public boolean hasNamenodeId() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional string namenodeId = 6;</code>
       */
      public java.lang.String getNamenodeId() {
        java.lang.Object ref = namenodeId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          namenodeId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string namenodeId = 6;</code>
       */
      public com.google.protobuf.ByteString
          getNamenodeIdBytes() {
        java.lang.Object ref = namenodeId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          namenodeId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string namenodeId = 6;</code>
       */
      public Builder setNamenodeId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        namenodeId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string namenodeId = 6;</code>
       */
      public Builder clearNamenodeId() {
        bitField0_ = (bitField0_ & ~0x00000020);
        namenodeId_ = getDefaultInstance().getNamenodeId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string namenodeId = 6;</code>
       */
      public Builder setNamenodeIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        namenodeId_ = value;
        onChanged();
        return this;
      }

      // optional string clusterId = 7;
      private java.lang.Object clusterId_ = "";
      /**
       * <code>optional string clusterId = 7;</code>
       */
      public boolean hasClusterId() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional string clusterId = 7;</code>
       */
      public java.lang.String getClusterId() {
        java.lang.Object ref = clusterId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          clusterId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string clusterId = 7;</code>
       */
      public com.google.protobuf.ByteString
          getClusterIdBytes() {
        java.lang.Object ref = clusterId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          clusterId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string clusterId = 7;</code>
       */
      public Builder setClusterId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        clusterId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string clusterId = 7;</code>
       */
      public Builder clearClusterId() {
        bitField0_ = (bitField0_ & ~0x00000040);
        clusterId_ = getDefaultInstance().getClusterId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string clusterId = 7;</code>
       */
      public Builder setClusterIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        clusterId_ = value;
        onChanged();
        return this;
      }

      // optional string blockPoolId = 8;
      private java.lang.Object blockPoolId_ = "";
      /**
       * <code>optional string blockPoolId = 8;</code>
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional string blockPoolId = 8;</code>
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          blockPoolId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string blockPoolId = 8;</code>
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string blockPoolId = 8;</code>
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string blockPoolId = 8;</code>
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000080);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string blockPoolId = 8;</code>
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        blockPoolId_ = value;
        onChanged();
        return this;
      }

      // optional string webAddress = 9;
      private java.lang.Object webAddress_ = "";
      /**
       * <code>optional string webAddress = 9;</code>
       */
      public boolean hasWebAddress() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional string webAddress = 9;</code>
       */
      public java.lang.String getWebAddress() {
        java.lang.Object ref = webAddress_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          webAddress_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string webAddress = 9;</code>
       */
      public com.google.protobuf.ByteString
          getWebAddressBytes() {
        java.lang.Object ref = webAddress_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          webAddress_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string webAddress = 9;</code>
       */
      public Builder setWebAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000100;
        webAddress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string webAddress = 9;</code>
       */
      public Builder clearWebAddress() {
        bitField0_ = (bitField0_ & ~0x00000100);
        webAddress_ = getDefaultInstance().getWebAddress();
        onChanged();
        return this;
      }
      /**
       * <code>optional string webAddress = 9;</code>
       */
      public Builder setWebAddressBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000100;
        webAddress_ = value;
        onChanged();
        return this;
      }

      // optional string rpcAddress = 10;
      private java.lang.Object rpcAddress_ = "";
      /**
       * <code>optional string rpcAddress = 10;</code>
       */
      public boolean hasRpcAddress() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional string rpcAddress = 10;</code>
       */
      public java.lang.String getRpcAddress() {
        java.lang.Object ref = rpcAddress_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          rpcAddress_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string rpcAddress = 10;</code>
       */
      public com.google.protobuf.ByteString
          getRpcAddressBytes() {
        java.lang.Object ref = rpcAddress_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          rpcAddress_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string rpcAddress = 10;</code>
       */
      public Builder setRpcAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000200;
        rpcAddress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string rpcAddress = 10;</code>
       */
      public Builder clearRpcAddress() {
        bitField0_ = (bitField0_ & ~0x00000200);
        rpcAddress_ = getDefaultInstance().getRpcAddress();
        onChanged();
        return this;
      }
      /**
       * <code>optional string rpcAddress = 10;</code>
       */
      public Builder setRpcAddressBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000200;
        rpcAddress_ = value;
        onChanged();
        return this;
      }

      // optional string serviceAddress = 11;
      private java.lang.Object serviceAddress_ = "";
      /**
       * <code>optional string serviceAddress = 11;</code>
       */
      public boolean hasServiceAddress() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional string serviceAddress = 11;</code>
       */
      public java.lang.String getServiceAddress() {
        java.lang.Object ref = serviceAddress_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          serviceAddress_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string serviceAddress = 11;</code>
       */
      public com.google.protobuf.ByteString
          getServiceAddressBytes() {
        java.lang.Object ref = serviceAddress_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          serviceAddress_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string serviceAddress = 11;</code>
       */
      public Builder setServiceAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        serviceAddress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string serviceAddress = 11;</code>
       */
      public Builder clearServiceAddress() {
        bitField0_ = (bitField0_ & ~0x00000400);
        serviceAddress_ = getDefaultInstance().getServiceAddress();
        onChanged();
        return this;
      }
      /**
       * <code>optional string serviceAddress = 11;</code>
       */
      public Builder setServiceAddressBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        serviceAddress_ = value;
        onChanged();
        return this;
      }

      // optional string lifelineAddress = 12;
      private java.lang.Object lifelineAddress_ = "";
      /**
       * <code>optional string lifelineAddress = 12;</code>
       */
      public boolean hasLifelineAddress() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      /**
       * <code>optional string lifelineAddress = 12;</code>
       */
      public java.lang.String getLifelineAddress() {
        java.lang.Object ref = lifelineAddress_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          lifelineAddress_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string lifelineAddress = 12;</code>
       */
      public com.google.protobuf.ByteString
          getLifelineAddressBytes() {
        java.lang.Object ref = lifelineAddress_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          lifelineAddress_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string lifelineAddress = 12;</code>
       */
      public Builder setLifelineAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000800;
        lifelineAddress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string lifelineAddress = 12;</code>
       */
      public Builder clearLifelineAddress() {
        bitField0_ = (bitField0_ & ~0x00000800);
        lifelineAddress_ = getDefaultInstance().getLifelineAddress();
        onChanged();
        return this;
      }
      /**
       * <code>optional string lifelineAddress = 12;</code>
       */
      public Builder setLifelineAddressBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000800;
        lifelineAddress_ = value;
        onChanged();
        return this;
      }

      // optional string state = 13;
      private java.lang.Object state_ = "";
      /**
       * <code>optional string state = 13;</code>
       */
      public boolean hasState() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      /**
       * <code>optional string state = 13;</code>
       */
      public java.lang.String getState() {
        java.lang.Object ref = state_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          state_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string state = 13;</code>
       */
      public com.google.protobuf.ByteString
          getStateBytes() {
        java.lang.Object ref = state_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          state_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string state = 13;</code>
       */
      public Builder setState(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00001000;
        state_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string state = 13;</code>
       */
      public Builder clearState() {
        bitField0_ = (bitField0_ & ~0x00001000);
        state_ = getDefaultInstance().getState();
        onChanged();
        return this;
      }
      /**
       * <code>optional string state = 13;</code>
       */
      public Builder setStateBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00001000;
        state_ = value;
        onChanged();
        return this;
      }

      // optional bool isSafeMode = 14;
      private boolean isSafeMode_ ;
      /**
       * <code>optional bool isSafeMode = 14;</code>
       */
      public boolean hasIsSafeMode() {
        return ((bitField0_ & 0x00002000) == 0x00002000);
      }
      /**
       * <code>optional bool isSafeMode = 14;</code>
       */
      public boolean getIsSafeMode() {
        return isSafeMode_;
      }
      /**
       * <code>optional bool isSafeMode = 14;</code>
       */
      public Builder setIsSafeMode(boolean value) {
        bitField0_ |= 0x00002000;
        isSafeMode_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool isSafeMode = 14;</code>
       */
      public Builder clearIsSafeMode() {
        bitField0_ = (bitField0_ & ~0x00002000);
        isSafeMode_ = false;
        onChanged();
        return this;
      }

      // optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto stats_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder> statsBuilder_;
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public boolean hasStats() {
        return ((bitField0_ & 0x00004000) == 0x00004000);
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto getStats() {
        if (statsBuilder_ == null) {
          return stats_;
        } else {
          return statsBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public Builder setStats(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto value) {
        if (statsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          stats_ = value;
          onChanged();
        } else {
          statsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public Builder setStats(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder builderForValue) {
        if (statsBuilder_ == null) {
          stats_ = builderForValue.build();
          onChanged();
        } else {
          statsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public Builder mergeStats(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto value) {
        if (statsBuilder_ == null) {
          if (((bitField0_ & 0x00004000) == 0x00004000) &&
              stats_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.getDefaultInstance()) {
            stats_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.newBuilder(stats_).mergeFrom(value).buildPartial();
          } else {
            stats_ = value;
          }
          onChanged();
        } else {
          statsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public Builder clearStats() {
        if (statsBuilder_ == null) {
          stats_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.getDefaultInstance();
          onChanged();
        } else {
          statsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder getStatsBuilder() {
        bitField0_ |= 0x00004000;
        onChanged();
        return getStatsFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder getStatsOrBuilder() {
        if (statsBuilder_ != null) {
          return statsBuilder_.getMessageOrBuilder();
        } else {
          return stats_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipStatsRecordProto stats = 15;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder> 
          getStatsFieldBuilder() {
        if (statsBuilder_ == null) {
          statsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder>(
                  stats_,
                  getParentForChildren(),
                  isClean());
          stats_ = null;
        }
        return statsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.NamenodeMembershipRecordProto)
    }

    static {
      defaultInstance = new NamenodeMembershipRecordProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.NamenodeMembershipRecordProto)
  }

  public interface FederationNamespaceInfoProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string blockPoolId = 1;
    /**
     * <code>optional string blockPoolId = 1;</code>
     */
    boolean hasBlockPoolId();
    /**
     * <code>optional string blockPoolId = 1;</code>
     */
    java.lang.String getBlockPoolId();
    /**
     * <code>optional string blockPoolId = 1;</code>
     */
    com.google.protobuf.ByteString
        getBlockPoolIdBytes();

    // optional string clusterId = 2;
    /**
     * <code>optional string clusterId = 2;</code>
     */
    boolean hasClusterId();
    /**
     * <code>optional string clusterId = 2;</code>
     */
    java.lang.String getClusterId();
    /**
     * <code>optional string clusterId = 2;</code>
     */
    com.google.protobuf.ByteString
        getClusterIdBytes();

    // optional string nameserviceId = 3;
    /**
     * <code>optional string nameserviceId = 3;</code>
     */
    boolean hasNameserviceId();
    /**
     * <code>optional string nameserviceId = 3;</code>
     */
    java.lang.String getNameserviceId();
    /**
     * <code>optional string nameserviceId = 3;</code>
     */
    com.google.protobuf.ByteString
        getNameserviceIdBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.FederationNamespaceInfoProto}
   */
  public static final class FederationNamespaceInfoProto extends
      com.google.protobuf.GeneratedMessage
      implements FederationNamespaceInfoProtoOrBuilder {
    // Use FederationNamespaceInfoProto.newBuilder() to construct.
    private FederationNamespaceInfoProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FederationNamespaceInfoProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FederationNamespaceInfoProto defaultInstance;
    public static FederationNamespaceInfoProto getDefaultInstance() {
      return defaultInstance;
    }

    public FederationNamespaceInfoProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FederationNamespaceInfoProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              blockPoolId_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              clusterId_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              nameserviceId_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_FederationNamespaceInfoProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_FederationNamespaceInfoProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FederationNamespaceInfoProto> PARSER =
        new com.google.protobuf.AbstractParser<FederationNamespaceInfoProto>() {
      public FederationNamespaceInfoProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FederationNamespaceInfoProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FederationNamespaceInfoProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string blockPoolId = 1;
    public static final int BLOCKPOOLID_FIELD_NUMBER = 1;
    private java.lang.Object blockPoolId_;
    /**
     * <code>optional string blockPoolId = 1;</code>
     */
    public boolean hasBlockPoolId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string blockPoolId = 1;</code>
     */
    public java.lang.String getBlockPoolId() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          blockPoolId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string blockPoolId = 1;</code>
     */
    public com.google.protobuf.ByteString
        getBlockPoolIdBytes() {
      java.lang.Object ref = blockPoolId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        blockPoolId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string clusterId = 2;
    public static final int CLUSTERID_FIELD_NUMBER = 2;
    private java.lang.Object clusterId_;
    /**
     * <code>optional string clusterId = 2;</code>
     */
    public boolean hasClusterId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string clusterId = 2;</code>
     */
    public java.lang.String getClusterId() {
      java.lang.Object ref = clusterId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          clusterId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string clusterId = 2;</code>
     */
    public com.google.protobuf.ByteString
        getClusterIdBytes() {
      java.lang.Object ref = clusterId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        clusterId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string nameserviceId = 3;
    public static final int NAMESERVICEID_FIELD_NUMBER = 3;
    private java.lang.Object nameserviceId_;
    /**
     * <code>optional string nameserviceId = 3;</code>
     */
    public boolean hasNameserviceId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string nameserviceId = 3;</code>
     */
    public java.lang.String getNameserviceId() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nameserviceId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nameserviceId = 3;</code>
     */
    public com.google.protobuf.ByteString
        getNameserviceIdBytes() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nameserviceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      blockPoolId_ = "";
      clusterId_ = "";
      nameserviceId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getBlockPoolIdBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getClusterIdBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getNameserviceIdBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getBlockPoolIdBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getClusterIdBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getNameserviceIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto) obj;

      boolean result = true;
      result = result && (hasBlockPoolId() == other.hasBlockPoolId());
      if (hasBlockPoolId()) {
        result = result && getBlockPoolId()
            .equals(other.getBlockPoolId());
      }
      result = result && (hasClusterId() == other.hasClusterId());
      if (hasClusterId()) {
        result = result && getClusterId()
            .equals(other.getClusterId());
      }
      result = result && (hasNameserviceId() == other.hasNameserviceId());
      if (hasNameserviceId()) {
        result = result && getNameserviceId()
            .equals(other.getNameserviceId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasBlockPoolId()) {
        hash = (37 * hash) + BLOCKPOOLID_FIELD_NUMBER;
        hash = (53 * hash) + getBlockPoolId().hashCode();
      }
      if (hasClusterId()) {
        hash = (37 * hash) + CLUSTERID_FIELD_NUMBER;
        hash = (53 * hash) + getClusterId().hashCode();
      }
      if (hasNameserviceId()) {
        hash = (37 * hash) + NAMESERVICEID_FIELD_NUMBER;
        hash = (53 * hash) + getNameserviceId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.FederationNamespaceInfoProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_FederationNamespaceInfoProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_FederationNamespaceInfoProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        blockPoolId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        clusterId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        nameserviceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_FederationNamespaceInfoProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.blockPoolId_ = blockPoolId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.clusterId_ = clusterId_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.nameserviceId_ = nameserviceId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.getDefaultInstance()) return this;
        if (other.hasBlockPoolId()) {
          bitField0_ |= 0x00000001;
          blockPoolId_ = other.blockPoolId_;
          onChanged();
        }
        if (other.hasClusterId()) {
          bitField0_ |= 0x00000002;
          clusterId_ = other.clusterId_;
          onChanged();
        }
        if (other.hasNameserviceId()) {
          bitField0_ |= 0x00000004;
          nameserviceId_ = other.nameserviceId_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string blockPoolId = 1;
      private java.lang.Object blockPoolId_ = "";
      /**
       * <code>optional string blockPoolId = 1;</code>
       */
      public boolean hasBlockPoolId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string blockPoolId = 1;</code>
       */
      public java.lang.String getBlockPoolId() {
        java.lang.Object ref = blockPoolId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          blockPoolId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string blockPoolId = 1;</code>
       */
      public com.google.protobuf.ByteString
          getBlockPoolIdBytes() {
        java.lang.Object ref = blockPoolId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          blockPoolId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string blockPoolId = 1;</code>
       */
      public Builder setBlockPoolId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        blockPoolId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string blockPoolId = 1;</code>
       */
      public Builder clearBlockPoolId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        blockPoolId_ = getDefaultInstance().getBlockPoolId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string blockPoolId = 1;</code>
       */
      public Builder setBlockPoolIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        blockPoolId_ = value;
        onChanged();
        return this;
      }

      // optional string clusterId = 2;
      private java.lang.Object clusterId_ = "";
      /**
       * <code>optional string clusterId = 2;</code>
       */
      public boolean hasClusterId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string clusterId = 2;</code>
       */
      public java.lang.String getClusterId() {
        java.lang.Object ref = clusterId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          clusterId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string clusterId = 2;</code>
       */
      public com.google.protobuf.ByteString
          getClusterIdBytes() {
        java.lang.Object ref = clusterId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          clusterId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string clusterId = 2;</code>
       */
      public Builder setClusterId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        clusterId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string clusterId = 2;</code>
       */
      public Builder clearClusterId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        clusterId_ = getDefaultInstance().getClusterId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string clusterId = 2;</code>
       */
      public Builder setClusterIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        clusterId_ = value;
        onChanged();
        return this;
      }

      // optional string nameserviceId = 3;
      private java.lang.Object nameserviceId_ = "";
      /**
       * <code>optional string nameserviceId = 3;</code>
       */
      public boolean hasNameserviceId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string nameserviceId = 3;</code>
       */
      public java.lang.String getNameserviceId() {
        java.lang.Object ref = nameserviceId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          nameserviceId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 3;</code>
       */
      public com.google.protobuf.ByteString
          getNameserviceIdBytes() {
        java.lang.Object ref = nameserviceId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nameserviceId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 3;</code>
       */
      public Builder setNameserviceId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        nameserviceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 3;</code>
       */
      public Builder clearNameserviceId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        nameserviceId_ = getDefaultInstance().getNameserviceId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 3;</code>
       */
      public Builder setNameserviceIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        nameserviceId_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.FederationNamespaceInfoProto)
    }

    static {
      defaultInstance = new FederationNamespaceInfoProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.FederationNamespaceInfoProto)
  }

  public interface GetNamenodeRegistrationsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
     */
    boolean hasMembership();
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getMembership();
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getMembershipOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetNamenodeRegistrationsRequestProto}
   */
  public static final class GetNamenodeRegistrationsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNamenodeRegistrationsRequestProtoOrBuilder {
    // Use GetNamenodeRegistrationsRequestProto.newBuilder() to construct.
    private GetNamenodeRegistrationsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNamenodeRegistrationsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNamenodeRegistrationsRequestProto defaultInstance;
    public static GetNamenodeRegistrationsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNamenodeRegistrationsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNamenodeRegistrationsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = membership_.toBuilder();
              }
              membership_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(membership_);
                membership_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNamenodeRegistrationsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNamenodeRegistrationsRequestProto>() {
      public GetNamenodeRegistrationsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNamenodeRegistrationsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNamenodeRegistrationsRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;
    public static final int MEMBERSHIP_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto membership_;
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
     */
    public boolean hasMembership() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getMembership() {
      return membership_;
    }
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getMembershipOrBuilder() {
      return membership_;
    }

    private void initFields() {
      membership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, membership_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, membership_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto) obj;

      boolean result = true;
      result = result && (hasMembership() == other.hasMembership());
      if (hasMembership()) {
        result = result && getMembership()
            .equals(other.getMembership());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasMembership()) {
        hash = (37 * hash) + MEMBERSHIP_FIELD_NUMBER;
        hash = (53 * hash) + getMembership().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetNamenodeRegistrationsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMembershipFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (membershipBuilder_ == null) {
          membership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
        } else {
          membershipBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (membershipBuilder_ == null) {
          result.membership_ = membership_;
        } else {
          result.membership_ = membershipBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.getDefaultInstance()) return this;
        if (other.hasMembership()) {
          mergeMembership(other.getMembership());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto membership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> membershipBuilder_;
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public boolean hasMembership() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getMembership() {
        if (membershipBuilder_ == null) {
          return membership_;
        } else {
          return membershipBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public Builder setMembership(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto value) {
        if (membershipBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          membership_ = value;
          onChanged();
        } else {
          membershipBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public Builder setMembership(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder builderForValue) {
        if (membershipBuilder_ == null) {
          membership_ = builderForValue.build();
          onChanged();
        } else {
          membershipBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public Builder mergeMembership(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto value) {
        if (membershipBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              membership_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance()) {
            membership_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.newBuilder(membership_).mergeFrom(value).buildPartial();
          } else {
            membership_ = value;
          }
          onChanged();
        } else {
          membershipBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public Builder clearMembership() {
        if (membershipBuilder_ == null) {
          membership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
          onChanged();
        } else {
          membershipBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder getMembershipBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getMembershipFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getMembershipOrBuilder() {
        if (membershipBuilder_ != null) {
          return membershipBuilder_.getMessageOrBuilder();
        } else {
          return membership_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto membership = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> 
          getMembershipFieldBuilder() {
        if (membershipBuilder_ == null) {
          membershipBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder>(
                  membership_,
                  getParentForChildren(),
                  isClean());
          membership_ = null;
        }
        return membershipBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetNamenodeRegistrationsRequestProto)
    }

    static {
      defaultInstance = new GetNamenodeRegistrationsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetNamenodeRegistrationsRequestProto)
  }

  public interface GetNamenodeRegistrationsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto> 
        getNamenodeMembershipsList();
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getNamenodeMemberships(int index);
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    int getNamenodeMembershipsCount();
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> 
        getNamenodeMembershipsOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getNamenodeMembershipsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetNamenodeRegistrationsResponseProto}
   */
  public static final class GetNamenodeRegistrationsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNamenodeRegistrationsResponseProtoOrBuilder {
    // Use GetNamenodeRegistrationsResponseProto.newBuilder() to construct.
    private GetNamenodeRegistrationsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNamenodeRegistrationsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNamenodeRegistrationsResponseProto defaultInstance;
    public static GetNamenodeRegistrationsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNamenodeRegistrationsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNamenodeRegistrationsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                namenodeMemberships_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              namenodeMemberships_.add(input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          namenodeMemberships_ = java.util.Collections.unmodifiableList(namenodeMemberships_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNamenodeRegistrationsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNamenodeRegistrationsResponseProto>() {
      public GetNamenodeRegistrationsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNamenodeRegistrationsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNamenodeRegistrationsResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;
    public static final int NAMENODEMEMBERSHIPS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto> namenodeMemberships_;
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto> getNamenodeMembershipsList() {
      return namenodeMemberships_;
    }
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> 
        getNamenodeMembershipsOrBuilderList() {
      return namenodeMemberships_;
    }
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    public int getNamenodeMembershipsCount() {
      return namenodeMemberships_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getNamenodeMemberships(int index) {
      return namenodeMemberships_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getNamenodeMembershipsOrBuilder(
        int index) {
      return namenodeMemberships_.get(index);
    }

    private void initFields() {
      namenodeMemberships_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < namenodeMemberships_.size(); i++) {
        output.writeMessage(1, namenodeMemberships_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < namenodeMemberships_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, namenodeMemberships_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto) obj;

      boolean result = true;
      result = result && getNamenodeMembershipsList()
          .equals(other.getNamenodeMembershipsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNamenodeMembershipsCount() > 0) {
        hash = (37 * hash) + NAMENODEMEMBERSHIPS_FIELD_NUMBER;
        hash = (53 * hash) + getNamenodeMembershipsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetNamenodeRegistrationsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNamenodeMembershipsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (namenodeMembershipsBuilder_ == null) {
          namenodeMemberships_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          namenodeMembershipsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (namenodeMembershipsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            namenodeMemberships_ = java.util.Collections.unmodifiableList(namenodeMemberships_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.namenodeMemberships_ = namenodeMemberships_;
        } else {
          result.namenodeMemberships_ = namenodeMembershipsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.getDefaultInstance()) return this;
        if (namenodeMembershipsBuilder_ == null) {
          if (!other.namenodeMemberships_.isEmpty()) {
            if (namenodeMemberships_.isEmpty()) {
              namenodeMemberships_ = other.namenodeMemberships_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNamenodeMembershipsIsMutable();
              namenodeMemberships_.addAll(other.namenodeMemberships_);
            }
            onChanged();
          }
        } else {
          if (!other.namenodeMemberships_.isEmpty()) {
            if (namenodeMembershipsBuilder_.isEmpty()) {
              namenodeMembershipsBuilder_.dispose();
              namenodeMembershipsBuilder_ = null;
              namenodeMemberships_ = other.namenodeMemberships_;
              bitField0_ = (bitField0_ & ~0x00000001);
              namenodeMembershipsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNamenodeMembershipsFieldBuilder() : null;
            } else {
              namenodeMembershipsBuilder_.addAllMessages(other.namenodeMemberships_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;
      private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto> namenodeMemberships_ =
        java.util.Collections.emptyList();
      private void ensureNamenodeMembershipsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          namenodeMemberships_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto>(namenodeMemberships_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> namenodeMembershipsBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto> getNamenodeMembershipsList() {
        if (namenodeMembershipsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(namenodeMemberships_);
        } else {
          return namenodeMembershipsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public int getNamenodeMembershipsCount() {
        if (namenodeMembershipsBuilder_ == null) {
          return namenodeMemberships_.size();
        } else {
          return namenodeMembershipsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getNamenodeMemberships(int index) {
        if (namenodeMembershipsBuilder_ == null) {
          return namenodeMemberships_.get(index);
        } else {
          return namenodeMembershipsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder setNamenodeMemberships(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto value) {
        if (namenodeMembershipsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamenodeMembershipsIsMutable();
          namenodeMemberships_.set(index, value);
          onChanged();
        } else {
          namenodeMembershipsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder setNamenodeMemberships(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder builderForValue) {
        if (namenodeMembershipsBuilder_ == null) {
          ensureNamenodeMembershipsIsMutable();
          namenodeMemberships_.set(index, builderForValue.build());
          onChanged();
        } else {
          namenodeMembershipsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder addNamenodeMemberships(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto value) {
        if (namenodeMembershipsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamenodeMembershipsIsMutable();
          namenodeMemberships_.add(value);
          onChanged();
        } else {
          namenodeMembershipsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder addNamenodeMemberships(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto value) {
        if (namenodeMembershipsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamenodeMembershipsIsMutable();
          namenodeMemberships_.add(index, value);
          onChanged();
        } else {
          namenodeMembershipsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder addNamenodeMemberships(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder builderForValue) {
        if (namenodeMembershipsBuilder_ == null) {
          ensureNamenodeMembershipsIsMutable();
          namenodeMemberships_.add(builderForValue.build());
          onChanged();
        } else {
          namenodeMembershipsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder addNamenodeMemberships(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder builderForValue) {
        if (namenodeMembershipsBuilder_ == null) {
          ensureNamenodeMembershipsIsMutable();
          namenodeMemberships_.add(index, builderForValue.build());
          onChanged();
        } else {
          namenodeMembershipsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder addAllNamenodeMemberships(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto> values) {
        if (namenodeMembershipsBuilder_ == null) {
          ensureNamenodeMembershipsIsMutable();
          super.addAll(values, namenodeMemberships_);
          onChanged();
        } else {
          namenodeMembershipsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder clearNamenodeMemberships() {
        if (namenodeMembershipsBuilder_ == null) {
          namenodeMemberships_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          namenodeMembershipsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public Builder removeNamenodeMemberships(int index) {
        if (namenodeMembershipsBuilder_ == null) {
          ensureNamenodeMembershipsIsMutable();
          namenodeMemberships_.remove(index);
          onChanged();
        } else {
          namenodeMembershipsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder getNamenodeMembershipsBuilder(
          int index) {
        return getNamenodeMembershipsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getNamenodeMembershipsOrBuilder(
          int index) {
        if (namenodeMembershipsBuilder_ == null) {
          return namenodeMemberships_.get(index);  } else {
          return namenodeMembershipsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> 
           getNamenodeMembershipsOrBuilderList() {
        if (namenodeMembershipsBuilder_ != null) {
          return namenodeMembershipsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(namenodeMemberships_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder addNamenodeMembershipsBuilder() {
        return getNamenodeMembershipsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder addNamenodeMembershipsBuilder(
          int index) {
        return getNamenodeMembershipsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMemberships = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder> 
           getNamenodeMembershipsBuilderList() {
        return getNamenodeMembershipsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> 
          getNamenodeMembershipsFieldBuilder() {
        if (namenodeMembershipsBuilder_ == null) {
          namenodeMembershipsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder>(
                  namenodeMemberships_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          namenodeMemberships_ = null;
        }
        return namenodeMembershipsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetNamenodeRegistrationsResponseProto)
    }

    static {
      defaultInstance = new GetNamenodeRegistrationsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetNamenodeRegistrationsResponseProto)
  }

  public interface GetExpiredRegistrationsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetExpiredRegistrationsRequestProto}
   */
  public static final class GetExpiredRegistrationsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetExpiredRegistrationsRequestProtoOrBuilder {
    // Use GetExpiredRegistrationsRequestProto.newBuilder() to construct.
    private GetExpiredRegistrationsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetExpiredRegistrationsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetExpiredRegistrationsRequestProto defaultInstance;
    public static GetExpiredRegistrationsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetExpiredRegistrationsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetExpiredRegistrationsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetExpiredRegistrationsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetExpiredRegistrationsRequestProto>() {
      public GetExpiredRegistrationsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetExpiredRegistrationsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetExpiredRegistrationsRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetExpiredRegistrationsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetExpiredRegistrationsRequestProto)
    }

    static {
      defaultInstance = new GetExpiredRegistrationsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetExpiredRegistrationsRequestProto)
  }

  public interface GetNamespaceInfoRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetNamespaceInfoRequestProto}
   */
  public static final class GetNamespaceInfoRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNamespaceInfoRequestProtoOrBuilder {
    // Use GetNamespaceInfoRequestProto.newBuilder() to construct.
    private GetNamespaceInfoRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNamespaceInfoRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNamespaceInfoRequestProto defaultInstance;
    public static GetNamespaceInfoRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNamespaceInfoRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNamespaceInfoRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNamespaceInfoRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNamespaceInfoRequestProto>() {
      public GetNamespaceInfoRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNamespaceInfoRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNamespaceInfoRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetNamespaceInfoRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetNamespaceInfoRequestProto)
    }

    static {
      defaultInstance = new GetNamespaceInfoRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetNamespaceInfoRequestProto)
  }

  public interface GetNamespaceInfoResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto> 
        getNamespaceInfosList();
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto getNamespaceInfos(int index);
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    int getNamespaceInfosCount();
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder> 
        getNamespaceInfosOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder getNamespaceInfosOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetNamespaceInfoResponseProto}
   */
  public static final class GetNamespaceInfoResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNamespaceInfoResponseProtoOrBuilder {
    // Use GetNamespaceInfoResponseProto.newBuilder() to construct.
    private GetNamespaceInfoResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNamespaceInfoResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNamespaceInfoResponseProto defaultInstance;
    public static GetNamespaceInfoResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNamespaceInfoResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNamespaceInfoResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                namespaceInfos_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              namespaceInfos_.add(input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          namespaceInfos_ = java.util.Collections.unmodifiableList(namespaceInfos_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNamespaceInfoResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNamespaceInfoResponseProto>() {
      public GetNamespaceInfoResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNamespaceInfoResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNamespaceInfoResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;
    public static final int NAMESPACEINFOS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto> namespaceInfos_;
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto> getNamespaceInfosList() {
      return namespaceInfos_;
    }
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder> 
        getNamespaceInfosOrBuilderList() {
      return namespaceInfos_;
    }
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    public int getNamespaceInfosCount() {
      return namespaceInfos_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto getNamespaceInfos(int index) {
      return namespaceInfos_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder getNamespaceInfosOrBuilder(
        int index) {
      return namespaceInfos_.get(index);
    }

    private void initFields() {
      namespaceInfos_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < namespaceInfos_.size(); i++) {
        output.writeMessage(1, namespaceInfos_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < namespaceInfos_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, namespaceInfos_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto) obj;

      boolean result = true;
      result = result && getNamespaceInfosList()
          .equals(other.getNamespaceInfosList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNamespaceInfosCount() > 0) {
        hash = (37 * hash) + NAMESPACEINFOS_FIELD_NUMBER;
        hash = (53 * hash) + getNamespaceInfosList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetNamespaceInfoResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNamespaceInfosFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (namespaceInfosBuilder_ == null) {
          namespaceInfos_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          namespaceInfosBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (namespaceInfosBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            namespaceInfos_ = java.util.Collections.unmodifiableList(namespaceInfos_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.namespaceInfos_ = namespaceInfos_;
        } else {
          result.namespaceInfos_ = namespaceInfosBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto.getDefaultInstance()) return this;
        if (namespaceInfosBuilder_ == null) {
          if (!other.namespaceInfos_.isEmpty()) {
            if (namespaceInfos_.isEmpty()) {
              namespaceInfos_ = other.namespaceInfos_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNamespaceInfosIsMutable();
              namespaceInfos_.addAll(other.namespaceInfos_);
            }
            onChanged();
          }
        } else {
          if (!other.namespaceInfos_.isEmpty()) {
            if (namespaceInfosBuilder_.isEmpty()) {
              namespaceInfosBuilder_.dispose();
              namespaceInfosBuilder_ = null;
              namespaceInfos_ = other.namespaceInfos_;
              bitField0_ = (bitField0_ & ~0x00000001);
              namespaceInfosBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNamespaceInfosFieldBuilder() : null;
            } else {
              namespaceInfosBuilder_.addAllMessages(other.namespaceInfos_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetNamespaceInfoResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;
      private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto> namespaceInfos_ =
        java.util.Collections.emptyList();
      private void ensureNamespaceInfosIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          namespaceInfos_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto>(namespaceInfos_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder> namespaceInfosBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto> getNamespaceInfosList() {
        if (namespaceInfosBuilder_ == null) {
          return java.util.Collections.unmodifiableList(namespaceInfos_);
        } else {
          return namespaceInfosBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public int getNamespaceInfosCount() {
        if (namespaceInfosBuilder_ == null) {
          return namespaceInfos_.size();
        } else {
          return namespaceInfosBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto getNamespaceInfos(int index) {
        if (namespaceInfosBuilder_ == null) {
          return namespaceInfos_.get(index);
        } else {
          return namespaceInfosBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder setNamespaceInfos(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto value) {
        if (namespaceInfosBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamespaceInfosIsMutable();
          namespaceInfos_.set(index, value);
          onChanged();
        } else {
          namespaceInfosBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder setNamespaceInfos(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder builderForValue) {
        if (namespaceInfosBuilder_ == null) {
          ensureNamespaceInfosIsMutable();
          namespaceInfos_.set(index, builderForValue.build());
          onChanged();
        } else {
          namespaceInfosBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder addNamespaceInfos(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto value) {
        if (namespaceInfosBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamespaceInfosIsMutable();
          namespaceInfos_.add(value);
          onChanged();
        } else {
          namespaceInfosBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder addNamespaceInfos(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto value) {
        if (namespaceInfosBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNamespaceInfosIsMutable();
          namespaceInfos_.add(index, value);
          onChanged();
        } else {
          namespaceInfosBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder addNamespaceInfos(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder builderForValue) {
        if (namespaceInfosBuilder_ == null) {
          ensureNamespaceInfosIsMutable();
          namespaceInfos_.add(builderForValue.build());
          onChanged();
        } else {
          namespaceInfosBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder addNamespaceInfos(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder builderForValue) {
        if (namespaceInfosBuilder_ == null) {
          ensureNamespaceInfosIsMutable();
          namespaceInfos_.add(index, builderForValue.build());
          onChanged();
        } else {
          namespaceInfosBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder addAllNamespaceInfos(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto> values) {
        if (namespaceInfosBuilder_ == null) {
          ensureNamespaceInfosIsMutable();
          super.addAll(values, namespaceInfos_);
          onChanged();
        } else {
          namespaceInfosBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder clearNamespaceInfos() {
        if (namespaceInfosBuilder_ == null) {
          namespaceInfos_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          namespaceInfosBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public Builder removeNamespaceInfos(int index) {
        if (namespaceInfosBuilder_ == null) {
          ensureNamespaceInfosIsMutable();
          namespaceInfos_.remove(index);
          onChanged();
        } else {
          namespaceInfosBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder getNamespaceInfosBuilder(
          int index) {
        return getNamespaceInfosFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder getNamespaceInfosOrBuilder(
          int index) {
        if (namespaceInfosBuilder_ == null) {
          return namespaceInfos_.get(index);  } else {
          return namespaceInfosBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder> 
           getNamespaceInfosOrBuilderList() {
        if (namespaceInfosBuilder_ != null) {
          return namespaceInfosBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(namespaceInfos_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder addNamespaceInfosBuilder() {
        return getNamespaceInfosFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder addNamespaceInfosBuilder(
          int index) {
        return getNamespaceInfosFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.FederationNamespaceInfoProto namespaceInfos = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder> 
           getNamespaceInfosBuilderList() {
        return getNamespaceInfosFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder> 
          getNamespaceInfosFieldBuilder() {
        if (namespaceInfosBuilder_ == null) {
          namespaceInfosBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder>(
                  namespaceInfos_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          namespaceInfos_ = null;
        }
        return namespaceInfosBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetNamespaceInfoResponseProto)
    }

    static {
      defaultInstance = new GetNamespaceInfoResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetNamespaceInfoResponseProto)
  }

  public interface UpdateNamenodeRegistrationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string nameserviceId = 1;
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    boolean hasNameserviceId();
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    java.lang.String getNameserviceId();
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameserviceIdBytes();

    // optional string namenodeId = 2;
    /**
     * <code>optional string namenodeId = 2;</code>
     */
    boolean hasNamenodeId();
    /**
     * <code>optional string namenodeId = 2;</code>
     */
    java.lang.String getNamenodeId();
    /**
     * <code>optional string namenodeId = 2;</code>
     */
    com.google.protobuf.ByteString
        getNamenodeIdBytes();

    // optional string state = 3;
    /**
     * <code>optional string state = 3;</code>
     */
    boolean hasState();
    /**
     * <code>optional string state = 3;</code>
     */
    java.lang.String getState();
    /**
     * <code>optional string state = 3;</code>
     */
    com.google.protobuf.ByteString
        getStateBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.UpdateNamenodeRegistrationRequestProto}
   */
  public static final class UpdateNamenodeRegistrationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateNamenodeRegistrationRequestProtoOrBuilder {
    // Use UpdateNamenodeRegistrationRequestProto.newBuilder() to construct.
    private UpdateNamenodeRegistrationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateNamenodeRegistrationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateNamenodeRegistrationRequestProto defaultInstance;
    public static UpdateNamenodeRegistrationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateNamenodeRegistrationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateNamenodeRegistrationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              nameserviceId_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              namenodeId_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              state_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateNamenodeRegistrationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateNamenodeRegistrationRequestProto>() {
      public UpdateNamenodeRegistrationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateNamenodeRegistrationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateNamenodeRegistrationRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string nameserviceId = 1;
    public static final int NAMESERVICEID_FIELD_NUMBER = 1;
    private java.lang.Object nameserviceId_;
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    public boolean hasNameserviceId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    public java.lang.String getNameserviceId() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nameserviceId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameserviceIdBytes() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nameserviceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string namenodeId = 2;
    public static final int NAMENODEID_FIELD_NUMBER = 2;
    private java.lang.Object namenodeId_;
    /**
     * <code>optional string namenodeId = 2;</code>
     */
    public boolean hasNamenodeId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string namenodeId = 2;</code>
     */
    public java.lang.String getNamenodeId() {
      java.lang.Object ref = namenodeId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          namenodeId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string namenodeId = 2;</code>
     */
    public com.google.protobuf.ByteString
        getNamenodeIdBytes() {
      java.lang.Object ref = namenodeId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        namenodeId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string state = 3;
    public static final int STATE_FIELD_NUMBER = 3;
    private java.lang.Object state_;
    /**
     * <code>optional string state = 3;</code>
     */
    public boolean hasState() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string state = 3;</code>
     */
    public java.lang.String getState() {
      java.lang.Object ref = state_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          state_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string state = 3;</code>
     */
    public com.google.protobuf.ByteString
        getStateBytes() {
      java.lang.Object ref = state_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        state_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      nameserviceId_ = "";
      namenodeId_ = "";
      state_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getNameserviceIdBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getNamenodeIdBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getStateBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getNameserviceIdBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getNamenodeIdBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getStateBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto) obj;

      boolean result = true;
      result = result && (hasNameserviceId() == other.hasNameserviceId());
      if (hasNameserviceId()) {
        result = result && getNameserviceId()
            .equals(other.getNameserviceId());
      }
      result = result && (hasNamenodeId() == other.hasNamenodeId());
      if (hasNamenodeId()) {
        result = result && getNamenodeId()
            .equals(other.getNamenodeId());
      }
      result = result && (hasState() == other.hasState());
      if (hasState()) {
        result = result && getState()
            .equals(other.getState());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNameserviceId()) {
        hash = (37 * hash) + NAMESERVICEID_FIELD_NUMBER;
        hash = (53 * hash) + getNameserviceId().hashCode();
      }
      if (hasNamenodeId()) {
        hash = (37 * hash) + NAMENODEID_FIELD_NUMBER;
        hash = (53 * hash) + getNamenodeId().hashCode();
      }
      if (hasState()) {
        hash = (37 * hash) + STATE_FIELD_NUMBER;
        hash = (53 * hash) + getState().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.UpdateNamenodeRegistrationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nameserviceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        namenodeId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        state_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.nameserviceId_ = nameserviceId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.namenodeId_ = namenodeId_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.state_ = state_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.getDefaultInstance()) return this;
        if (other.hasNameserviceId()) {
          bitField0_ |= 0x00000001;
          nameserviceId_ = other.nameserviceId_;
          onChanged();
        }
        if (other.hasNamenodeId()) {
          bitField0_ |= 0x00000002;
          namenodeId_ = other.namenodeId_;
          onChanged();
        }
        if (other.hasState()) {
          bitField0_ |= 0x00000004;
          state_ = other.state_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string nameserviceId = 1;
      private java.lang.Object nameserviceId_ = "";
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public boolean hasNameserviceId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public java.lang.String getNameserviceId() {
        java.lang.Object ref = nameserviceId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          nameserviceId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameserviceIdBytes() {
        java.lang.Object ref = nameserviceId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nameserviceId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public Builder setNameserviceId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameserviceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public Builder clearNameserviceId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        nameserviceId_ = getDefaultInstance().getNameserviceId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public Builder setNameserviceIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameserviceId_ = value;
        onChanged();
        return this;
      }

      // optional string namenodeId = 2;
      private java.lang.Object namenodeId_ = "";
      /**
       * <code>optional string namenodeId = 2;</code>
       */
      public boolean hasNamenodeId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string namenodeId = 2;</code>
       */
      public java.lang.String getNamenodeId() {
        java.lang.Object ref = namenodeId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          namenodeId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string namenodeId = 2;</code>
       */
      public com.google.protobuf.ByteString
          getNamenodeIdBytes() {
        java.lang.Object ref = namenodeId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          namenodeId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string namenodeId = 2;</code>
       */
      public Builder setNamenodeId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        namenodeId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string namenodeId = 2;</code>
       */
      public Builder clearNamenodeId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        namenodeId_ = getDefaultInstance().getNamenodeId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string namenodeId = 2;</code>
       */
      public Builder setNamenodeIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        namenodeId_ = value;
        onChanged();
        return this;
      }

      // optional string state = 3;
      private java.lang.Object state_ = "";
      /**
       * <code>optional string state = 3;</code>
       */
      public boolean hasState() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string state = 3;</code>
       */
      public java.lang.String getState() {
        java.lang.Object ref = state_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          state_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string state = 3;</code>
       */
      public com.google.protobuf.ByteString
          getStateBytes() {
        java.lang.Object ref = state_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          state_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string state = 3;</code>
       */
      public Builder setState(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        state_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string state = 3;</code>
       */
      public Builder clearState() {
        bitField0_ = (bitField0_ & ~0x00000004);
        state_ = getDefaultInstance().getState();
        onChanged();
        return this;
      }
      /**
       * <code>optional string state = 3;</code>
       */
      public Builder setStateBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        state_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.UpdateNamenodeRegistrationRequestProto)
    }

    static {
      defaultInstance = new UpdateNamenodeRegistrationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdateNamenodeRegistrationRequestProto)
  }

  public interface UpdateNamenodeRegistrationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.UpdateNamenodeRegistrationResponseProto}
   */
  public static final class UpdateNamenodeRegistrationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateNamenodeRegistrationResponseProtoOrBuilder {
    // Use UpdateNamenodeRegistrationResponseProto.newBuilder() to construct.
    private UpdateNamenodeRegistrationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateNamenodeRegistrationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateNamenodeRegistrationResponseProto defaultInstance;
    public static UpdateNamenodeRegistrationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateNamenodeRegistrationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateNamenodeRegistrationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateNamenodeRegistrationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateNamenodeRegistrationResponseProto>() {
      public UpdateNamenodeRegistrationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateNamenodeRegistrationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateNamenodeRegistrationResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.UpdateNamenodeRegistrationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.UpdateNamenodeRegistrationResponseProto)
    }

    static {
      defaultInstance = new UpdateNamenodeRegistrationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdateNamenodeRegistrationResponseProto)
  }

  public interface NamenodeHeartbeatRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
     */
    boolean hasNamenodeMembership();
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getNamenodeMembership();
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getNamenodeMembershipOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.NamenodeHeartbeatRequestProto}
   */
  public static final class NamenodeHeartbeatRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements NamenodeHeartbeatRequestProtoOrBuilder {
    // Use NamenodeHeartbeatRequestProto.newBuilder() to construct.
    private NamenodeHeartbeatRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private NamenodeHeartbeatRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final NamenodeHeartbeatRequestProto defaultInstance;
    public static NamenodeHeartbeatRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public NamenodeHeartbeatRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private NamenodeHeartbeatRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = namenodeMembership_.toBuilder();
              }
              namenodeMembership_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(namenodeMembership_);
                namenodeMembership_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<NamenodeHeartbeatRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<NamenodeHeartbeatRequestProto>() {
      public NamenodeHeartbeatRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NamenodeHeartbeatRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<NamenodeHeartbeatRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;
    public static final int NAMENODEMEMBERSHIP_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto namenodeMembership_;
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
     */
    public boolean hasNamenodeMembership() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getNamenodeMembership() {
      return namenodeMembership_;
    }
    /**
     * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getNamenodeMembershipOrBuilder() {
      return namenodeMembership_;
    }

    private void initFields() {
      namenodeMembership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, namenodeMembership_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, namenodeMembership_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto) obj;

      boolean result = true;
      result = result && (hasNamenodeMembership() == other.hasNamenodeMembership());
      if (hasNamenodeMembership()) {
        result = result && getNamenodeMembership()
            .equals(other.getNamenodeMembership());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNamenodeMembership()) {
        hash = (37 * hash) + NAMENODEMEMBERSHIP_FIELD_NUMBER;
        hash = (53 * hash) + getNamenodeMembership().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.NamenodeHeartbeatRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNamenodeMembershipFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (namenodeMembershipBuilder_ == null) {
          namenodeMembership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
        } else {
          namenodeMembershipBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (namenodeMembershipBuilder_ == null) {
          result.namenodeMembership_ = namenodeMembership_;
        } else {
          result.namenodeMembership_ = namenodeMembershipBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.getDefaultInstance()) return this;
        if (other.hasNamenodeMembership()) {
          mergeNamenodeMembership(other.getNamenodeMembership());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto namenodeMembership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> namenodeMembershipBuilder_;
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public boolean hasNamenodeMembership() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto getNamenodeMembership() {
        if (namenodeMembershipBuilder_ == null) {
          return namenodeMembership_;
        } else {
          return namenodeMembershipBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public Builder setNamenodeMembership(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto value) {
        if (namenodeMembershipBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          namenodeMembership_ = value;
          onChanged();
        } else {
          namenodeMembershipBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public Builder setNamenodeMembership(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder builderForValue) {
        if (namenodeMembershipBuilder_ == null) {
          namenodeMembership_ = builderForValue.build();
          onChanged();
        } else {
          namenodeMembershipBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public Builder mergeNamenodeMembership(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto value) {
        if (namenodeMembershipBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              namenodeMembership_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance()) {
            namenodeMembership_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.newBuilder(namenodeMembership_).mergeFrom(value).buildPartial();
          } else {
            namenodeMembership_ = value;
          }
          onChanged();
        } else {
          namenodeMembershipBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public Builder clearNamenodeMembership() {
        if (namenodeMembershipBuilder_ == null) {
          namenodeMembership_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.getDefaultInstance();
          onChanged();
        } else {
          namenodeMembershipBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder getNamenodeMembershipBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getNamenodeMembershipFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder getNamenodeMembershipOrBuilder() {
        if (namenodeMembershipBuilder_ != null) {
          return namenodeMembershipBuilder_.getMessageOrBuilder();
        } else {
          return namenodeMembership_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.NamenodeMembershipRecordProto namenodeMembership = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder> 
          getNamenodeMembershipFieldBuilder() {
        if (namenodeMembershipBuilder_ == null) {
          namenodeMembershipBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder>(
                  namenodeMembership_,
                  getParentForChildren(),
                  isClean());
          namenodeMembership_ = null;
        }
        return namenodeMembershipBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.NamenodeHeartbeatRequestProto)
    }

    static {
      defaultInstance = new NamenodeHeartbeatRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.NamenodeHeartbeatRequestProto)
  }

  public interface NamenodeHeartbeatResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.NamenodeHeartbeatResponseProto}
   */
  public static final class NamenodeHeartbeatResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements NamenodeHeartbeatResponseProtoOrBuilder {
    // Use NamenodeHeartbeatResponseProto.newBuilder() to construct.
    private NamenodeHeartbeatResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private NamenodeHeartbeatResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final NamenodeHeartbeatResponseProto defaultInstance;
    public static NamenodeHeartbeatResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public NamenodeHeartbeatResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private NamenodeHeartbeatResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<NamenodeHeartbeatResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<NamenodeHeartbeatResponseProto>() {
      public NamenodeHeartbeatResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NamenodeHeartbeatResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<NamenodeHeartbeatResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.NamenodeHeartbeatResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.NamenodeHeartbeatResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.NamenodeHeartbeatResponseProto)
    }

    static {
      defaultInstance = new NamenodeHeartbeatResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.NamenodeHeartbeatResponseProto)
  }

  public interface RemoteLocationProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string nameserviceId = 1;
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    boolean hasNameserviceId();
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    java.lang.String getNameserviceId();
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameserviceIdBytes();

    // optional string path = 2;
    /**
     * <code>optional string path = 2;</code>
     */
    boolean hasPath();
    /**
     * <code>optional string path = 2;</code>
     */
    java.lang.String getPath();
    /**
     * <code>optional string path = 2;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.RemoteLocationProto}
   */
  public static final class RemoteLocationProto extends
      com.google.protobuf.GeneratedMessage
      implements RemoteLocationProtoOrBuilder {
    // Use RemoteLocationProto.newBuilder() to construct.
    private RemoteLocationProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RemoteLocationProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RemoteLocationProto defaultInstance;
    public static RemoteLocationProto getDefaultInstance() {
      return defaultInstance;
    }

    public RemoteLocationProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RemoteLocationProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              nameserviceId_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              path_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoteLocationProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoteLocationProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RemoteLocationProto> PARSER =
        new com.google.protobuf.AbstractParser<RemoteLocationProto>() {
      public RemoteLocationProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RemoteLocationProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RemoteLocationProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string nameserviceId = 1;
    public static final int NAMESERVICEID_FIELD_NUMBER = 1;
    private java.lang.Object nameserviceId_;
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    public boolean hasNameserviceId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    public java.lang.String getNameserviceId() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nameserviceId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nameserviceId = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameserviceIdBytes() {
      java.lang.Object ref = nameserviceId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nameserviceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string path = 2;
    public static final int PATH_FIELD_NUMBER = 2;
    private java.lang.Object path_;
    /**
     * <code>optional string path = 2;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string path = 2;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string path = 2;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      nameserviceId_ = "";
      path_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getNameserviceIdBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getPathBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getNameserviceIdBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getPathBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto) obj;

      boolean result = true;
      result = result && (hasNameserviceId() == other.hasNameserviceId());
      if (hasNameserviceId()) {
        result = result && getNameserviceId()
            .equals(other.getNameserviceId());
      }
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNameserviceId()) {
        hash = (37 * hash) + NAMESERVICEID_FIELD_NUMBER;
        hash = (53 * hash) + getNameserviceId().hashCode();
      }
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.RemoteLocationProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoteLocationProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoteLocationProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nameserviceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoteLocationProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.nameserviceId_ = nameserviceId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.path_ = path_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.getDefaultInstance()) return this;
        if (other.hasNameserviceId()) {
          bitField0_ |= 0x00000001;
          nameserviceId_ = other.nameserviceId_;
          onChanged();
        }
        if (other.hasPath()) {
          bitField0_ |= 0x00000002;
          path_ = other.path_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string nameserviceId = 1;
      private java.lang.Object nameserviceId_ = "";
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public boolean hasNameserviceId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public java.lang.String getNameserviceId() {
        java.lang.Object ref = nameserviceId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          nameserviceId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameserviceIdBytes() {
        java.lang.Object ref = nameserviceId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nameserviceId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public Builder setNameserviceId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameserviceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public Builder clearNameserviceId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        nameserviceId_ = getDefaultInstance().getNameserviceId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameserviceId = 1;</code>
       */
      public Builder setNameserviceIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameserviceId_ = value;
        onChanged();
        return this;
      }

      // optional string path = 2;
      private java.lang.Object path_ = "";
      /**
       * <code>optional string path = 2;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string path = 2;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string path = 2;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string path = 2;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string path = 2;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000002);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>optional string path = 2;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        path_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.RemoteLocationProto)
    }

    static {
      defaultInstance = new RemoteLocationProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.RemoteLocationProto)
  }

  public interface MountTableRecordProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string srcPath = 1;
    /**
     * <code>optional string srcPath = 1;</code>
     */
    boolean hasSrcPath();
    /**
     * <code>optional string srcPath = 1;</code>
     */
    java.lang.String getSrcPath();
    /**
     * <code>optional string srcPath = 1;</code>
     */
    com.google.protobuf.ByteString
        getSrcPathBytes();

    // repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto> 
        getDestinationsList();
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto getDestinations(int index);
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    int getDestinationsCount();
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder> 
        getDestinationsOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder getDestinationsOrBuilder(
        int index);

    // optional uint64 dateCreated = 3;
    /**
     * <code>optional uint64 dateCreated = 3;</code>
     */
    boolean hasDateCreated();
    /**
     * <code>optional uint64 dateCreated = 3;</code>
     */
    long getDateCreated();

    // optional uint64 dateModified = 4;
    /**
     * <code>optional uint64 dateModified = 4;</code>
     */
    boolean hasDateModified();
    /**
     * <code>optional uint64 dateModified = 4;</code>
     */
    long getDateModified();

    // optional bool readOnly = 5 [default = false];
    /**
     * <code>optional bool readOnly = 5 [default = false];</code>
     */
    boolean hasReadOnly();
    /**
     * <code>optional bool readOnly = 5 [default = false];</code>
     */
    boolean getReadOnly();

    // optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
     */
    boolean hasDestOrder();
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder getDestOrder();

    // optional string ownerName = 10;
    /**
     * <code>optional string ownerName = 10;</code>
     */
    boolean hasOwnerName();
    /**
     * <code>optional string ownerName = 10;</code>
     */
    java.lang.String getOwnerName();
    /**
     * <code>optional string ownerName = 10;</code>
     */
    com.google.protobuf.ByteString
        getOwnerNameBytes();

    // optional string groupName = 11;
    /**
     * <code>optional string groupName = 11;</code>
     */
    boolean hasGroupName();
    /**
     * <code>optional string groupName = 11;</code>
     */
    java.lang.String getGroupName();
    /**
     * <code>optional string groupName = 11;</code>
     */
    com.google.protobuf.ByteString
        getGroupNameBytes();

    // optional int32 mode = 12;
    /**
     * <code>optional int32 mode = 12;</code>
     */
    boolean hasMode();
    /**
     * <code>optional int32 mode = 12;</code>
     */
    int getMode();

    // optional .hadoop.hdfs.QuotaUsageProto quota = 13;
    /**
     * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
     */
    boolean hasQuota();
    /**
     * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto getQuota();
    /**
     * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProtoOrBuilder getQuotaOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.MountTableRecordProto}
   */
  public static final class MountTableRecordProto extends
      com.google.protobuf.GeneratedMessage
      implements MountTableRecordProtoOrBuilder {
    // Use MountTableRecordProto.newBuilder() to construct.
    private MountTableRecordProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private MountTableRecordProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final MountTableRecordProto defaultInstance;
    public static MountTableRecordProto getDefaultInstance() {
      return defaultInstance;
    }

    public MountTableRecordProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private MountTableRecordProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              srcPath_ = input.readBytes();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                destinations_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              destinations_.add(input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.PARSER, extensionRegistry));
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              dateCreated_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              dateModified_ = input.readUInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000008;
              readOnly_ = input.readBool();
              break;
            }
            case 48: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder value = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(6, rawValue);
              } else {
                bitField0_ |= 0x00000010;
                destOrder_ = value;
              }
              break;
            }
            case 82: {
              bitField0_ |= 0x00000020;
              ownerName_ = input.readBytes();
              break;
            }
            case 90: {
              bitField0_ |= 0x00000040;
              groupName_ = input.readBytes();
              break;
            }
            case 96: {
              bitField0_ |= 0x00000080;
              mode_ = input.readInt32();
              break;
            }
            case 106: {
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) == 0x00000100)) {
                subBuilder = quota_.toBuilder();
              }
              quota_ = input.readMessage(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(quota_);
                quota_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          destinations_ = java.util.Collections.unmodifiableList(destinations_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_MountTableRecordProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_MountTableRecordProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder.class);
    }

    public static com.google.protobuf.Parser<MountTableRecordProto> PARSER =
        new com.google.protobuf.AbstractParser<MountTableRecordProto>() {
      public MountTableRecordProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new MountTableRecordProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<MountTableRecordProto> getParserForType() {
      return PARSER;
    }

    /**
     * Protobuf enum {@code hadoop.hdfs.MountTableRecordProto.DestOrder}
     */
    public enum DestOrder
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>HASH = 0;</code>
       */
      HASH(0, 0),
      /**
       * <code>LOCAL = 1;</code>
       */
      LOCAL(1, 1),
      /**
       * <code>RANDOM = 2;</code>
       */
      RANDOM(2, 2),
      /**
       * <code>HASH_ALL = 3;</code>
       */
      HASH_ALL(3, 3),
      /**
       * <code>SPACE = 4;</code>
       */
      SPACE(4, 4),
      ;

      /**
       * <code>HASH = 0;</code>
       */
      public static final int HASH_VALUE = 0;
      /**
       * <code>LOCAL = 1;</code>
       */
      public static final int LOCAL_VALUE = 1;
      /**
       * <code>RANDOM = 2;</code>
       */
      public static final int RANDOM_VALUE = 2;
      /**
       * <code>HASH_ALL = 3;</code>
       */
      public static final int HASH_ALL_VALUE = 3;
      /**
       * <code>SPACE = 4;</code>
       */
      public static final int SPACE_VALUE = 4;


      public final int getNumber() { return value; }

      public static DestOrder valueOf(int value) {
        switch (value) {
          case 0: return HASH;
          case 1: return LOCAL;
          case 2: return RANDOM;
          case 3: return HASH_ALL;
          case 4: return SPACE;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<DestOrder>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static com.google.protobuf.Internal.EnumLiteMap<DestOrder>
          internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<DestOrder>() {
              public DestOrder findValueByNumber(int number) {
                return DestOrder.valueOf(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(index);
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDescriptor().getEnumTypes().get(0);
      }

      private static final DestOrder[] VALUES = values();

      public static DestOrder valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int index;
      private final int value;

      private DestOrder(int index, int value) {
        this.index = index;
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:hadoop.hdfs.MountTableRecordProto.DestOrder)
    }

    private int bitField0_;
    // optional string srcPath = 1;
    public static final int SRCPATH_FIELD_NUMBER = 1;
    private java.lang.Object srcPath_;
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public boolean hasSrcPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public java.lang.String getSrcPath() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          srcPath_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public com.google.protobuf.ByteString
        getSrcPathBytes() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        srcPath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;
    public static final int DESTINATIONS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto> destinations_;
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto> getDestinationsList() {
      return destinations_;
    }
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder> 
        getDestinationsOrBuilderList() {
      return destinations_;
    }
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    public int getDestinationsCount() {
      return destinations_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto getDestinations(int index) {
      return destinations_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder getDestinationsOrBuilder(
        int index) {
      return destinations_.get(index);
    }

    // optional uint64 dateCreated = 3;
    public static final int DATECREATED_FIELD_NUMBER = 3;
    private long dateCreated_;
    /**
     * <code>optional uint64 dateCreated = 3;</code>
     */
    public boolean hasDateCreated() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 dateCreated = 3;</code>
     */
    public long getDateCreated() {
      return dateCreated_;
    }

    // optional uint64 dateModified = 4;
    public static final int DATEMODIFIED_FIELD_NUMBER = 4;
    private long dateModified_;
    /**
     * <code>optional uint64 dateModified = 4;</code>
     */
    public boolean hasDateModified() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint64 dateModified = 4;</code>
     */
    public long getDateModified() {
      return dateModified_;
    }

    // optional bool readOnly = 5 [default = false];
    public static final int READONLY_FIELD_NUMBER = 5;
    private boolean readOnly_;
    /**
     * <code>optional bool readOnly = 5 [default = false];</code>
     */
    public boolean hasReadOnly() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional bool readOnly = 5 [default = false];</code>
     */
    public boolean getReadOnly() {
      return readOnly_;
    }

    // optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];
    public static final int DESTORDER_FIELD_NUMBER = 6;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder destOrder_;
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
     */
    public boolean hasDestOrder() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder getDestOrder() {
      return destOrder_;
    }

    // optional string ownerName = 10;
    public static final int OWNERNAME_FIELD_NUMBER = 10;
    private java.lang.Object ownerName_;
    /**
     * <code>optional string ownerName = 10;</code>
     */
    public boolean hasOwnerName() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional string ownerName = 10;</code>
     */
    public java.lang.String getOwnerName() {
      java.lang.Object ref = ownerName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          ownerName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string ownerName = 10;</code>
     */
    public com.google.protobuf.ByteString
        getOwnerNameBytes() {
      java.lang.Object ref = ownerName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        ownerName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string groupName = 11;
    public static final int GROUPNAME_FIELD_NUMBER = 11;
    private java.lang.Object groupName_;
    /**
     * <code>optional string groupName = 11;</code>
     */
    public boolean hasGroupName() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional string groupName = 11;</code>
     */
    public java.lang.String getGroupName() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          groupName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string groupName = 11;</code>
     */
    public com.google.protobuf.ByteString
        getGroupNameBytes() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        groupName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional int32 mode = 12;
    public static final int MODE_FIELD_NUMBER = 12;
    private int mode_;
    /**
     * <code>optional int32 mode = 12;</code>
     */
    public boolean hasMode() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional int32 mode = 12;</code>
     */
    public int getMode() {
      return mode_;
    }

    // optional .hadoop.hdfs.QuotaUsageProto quota = 13;
    public static final int QUOTA_FIELD_NUMBER = 13;
    private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto quota_;
    /**
     * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
     */
    public boolean hasQuota() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto getQuota() {
      return quota_;
    }
    /**
     * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProtoOrBuilder getQuotaOrBuilder() {
      return quota_;
    }

    private void initFields() {
      srcPath_ = "";
      destinations_ = java.util.Collections.emptyList();
      dateCreated_ = 0L;
      dateModified_ = 0L;
      readOnly_ = false;
      destOrder_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder.HASH;
      ownerName_ = "";
      groupName_ = "";
      mode_ = 0;
      quota_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasQuota()) {
        if (!getQuota().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getSrcPathBytes());
      }
      for (int i = 0; i < destinations_.size(); i++) {
        output.writeMessage(2, destinations_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(3, dateCreated_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(4, dateModified_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBool(5, readOnly_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeEnum(6, destOrder_.getNumber());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeBytes(10, getOwnerNameBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeBytes(11, getGroupNameBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeInt32(12, mode_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeMessage(13, quota_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getSrcPathBytes());
      }
      for (int i = 0; i < destinations_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, destinations_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, dateCreated_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, dateModified_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, readOnly_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(6, destOrder_.getNumber());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(10, getOwnerNameBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(11, getGroupNameBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(12, mode_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, quota_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto) obj;

      boolean result = true;
      result = result && (hasSrcPath() == other.hasSrcPath());
      if (hasSrcPath()) {
        result = result && getSrcPath()
            .equals(other.getSrcPath());
      }
      result = result && getDestinationsList()
          .equals(other.getDestinationsList());
      result = result && (hasDateCreated() == other.hasDateCreated());
      if (hasDateCreated()) {
        result = result && (getDateCreated()
            == other.getDateCreated());
      }
      result = result && (hasDateModified() == other.hasDateModified());
      if (hasDateModified()) {
        result = result && (getDateModified()
            == other.getDateModified());
      }
      result = result && (hasReadOnly() == other.hasReadOnly());
      if (hasReadOnly()) {
        result = result && (getReadOnly()
            == other.getReadOnly());
      }
      result = result && (hasDestOrder() == other.hasDestOrder());
      if (hasDestOrder()) {
        result = result &&
            (getDestOrder() == other.getDestOrder());
      }
      result = result && (hasOwnerName() == other.hasOwnerName());
      if (hasOwnerName()) {
        result = result && getOwnerName()
            .equals(other.getOwnerName());
      }
      result = result && (hasGroupName() == other.hasGroupName());
      if (hasGroupName()) {
        result = result && getGroupName()
            .equals(other.getGroupName());
      }
      result = result && (hasMode() == other.hasMode());
      if (hasMode()) {
        result = result && (getMode()
            == other.getMode());
      }
      result = result && (hasQuota() == other.hasQuota());
      if (hasQuota()) {
        result = result && getQuota()
            .equals(other.getQuota());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasSrcPath()) {
        hash = (37 * hash) + SRCPATH_FIELD_NUMBER;
        hash = (53 * hash) + getSrcPath().hashCode();
      }
      if (getDestinationsCount() > 0) {
        hash = (37 * hash) + DESTINATIONS_FIELD_NUMBER;
        hash = (53 * hash) + getDestinationsList().hashCode();
      }
      if (hasDateCreated()) {
        hash = (37 * hash) + DATECREATED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateCreated());
      }
      if (hasDateModified()) {
        hash = (37 * hash) + DATEMODIFIED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateModified());
      }
      if (hasReadOnly()) {
        hash = (37 * hash) + READONLY_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getReadOnly());
      }
      if (hasDestOrder()) {
        hash = (37 * hash) + DESTORDER_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getDestOrder());
      }
      if (hasOwnerName()) {
        hash = (37 * hash) + OWNERNAME_FIELD_NUMBER;
        hash = (53 * hash) + getOwnerName().hashCode();
      }
      if (hasGroupName()) {
        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
        hash = (53 * hash) + getGroupName().hashCode();
      }
      if (hasMode()) {
        hash = (37 * hash) + MODE_FIELD_NUMBER;
        hash = (53 * hash) + getMode();
      }
      if (hasQuota()) {
        hash = (37 * hash) + QUOTA_FIELD_NUMBER;
        hash = (53 * hash) + getQuota().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.MountTableRecordProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_MountTableRecordProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_MountTableRecordProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getDestinationsFieldBuilder();
          getQuotaFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        srcPath_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (destinationsBuilder_ == null) {
          destinations_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          destinationsBuilder_.clear();
        }
        dateCreated_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        dateModified_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        readOnly_ = false;
        bitField0_ = (bitField0_ & ~0x00000010);
        destOrder_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder.HASH;
        bitField0_ = (bitField0_ & ~0x00000020);
        ownerName_ = "";
        bitField0_ = (bitField0_ & ~0x00000040);
        groupName_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        mode_ = 0;
        bitField0_ = (bitField0_ & ~0x00000100);
        if (quotaBuilder_ == null) {
          quota_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.getDefaultInstance();
        } else {
          quotaBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_MountTableRecordProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.srcPath_ = srcPath_;
        if (destinationsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            destinations_ = java.util.Collections.unmodifiableList(destinations_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.destinations_ = destinations_;
        } else {
          result.destinations_ = destinationsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000002;
        }
        result.dateCreated_ = dateCreated_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000004;
        }
        result.dateModified_ = dateModified_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000008;
        }
        result.readOnly_ = readOnly_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000010;
        }
        result.destOrder_ = destOrder_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000020;
        }
        result.ownerName_ = ownerName_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000040;
        }
        result.groupName_ = groupName_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000080;
        }
        result.mode_ = mode_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000100;
        }
        if (quotaBuilder_ == null) {
          result.quota_ = quota_;
        } else {
          result.quota_ = quotaBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance()) return this;
        if (other.hasSrcPath()) {
          bitField0_ |= 0x00000001;
          srcPath_ = other.srcPath_;
          onChanged();
        }
        if (destinationsBuilder_ == null) {
          if (!other.destinations_.isEmpty()) {
            if (destinations_.isEmpty()) {
              destinations_ = other.destinations_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureDestinationsIsMutable();
              destinations_.addAll(other.destinations_);
            }
            onChanged();
          }
        } else {
          if (!other.destinations_.isEmpty()) {
            if (destinationsBuilder_.isEmpty()) {
              destinationsBuilder_.dispose();
              destinationsBuilder_ = null;
              destinations_ = other.destinations_;
              bitField0_ = (bitField0_ & ~0x00000002);
              destinationsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getDestinationsFieldBuilder() : null;
            } else {
              destinationsBuilder_.addAllMessages(other.destinations_);
            }
          }
        }
        if (other.hasDateCreated()) {
          setDateCreated(other.getDateCreated());
        }
        if (other.hasDateModified()) {
          setDateModified(other.getDateModified());
        }
        if (other.hasReadOnly()) {
          setReadOnly(other.getReadOnly());
        }
        if (other.hasDestOrder()) {
          setDestOrder(other.getDestOrder());
        }
        if (other.hasOwnerName()) {
          bitField0_ |= 0x00000040;
          ownerName_ = other.ownerName_;
          onChanged();
        }
        if (other.hasGroupName()) {
          bitField0_ |= 0x00000080;
          groupName_ = other.groupName_;
          onChanged();
        }
        if (other.hasMode()) {
          setMode(other.getMode());
        }
        if (other.hasQuota()) {
          mergeQuota(other.getQuota());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasQuota()) {
          if (!getQuota().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string srcPath = 1;
      private java.lang.Object srcPath_ = "";
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public boolean hasSrcPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public java.lang.String getSrcPath() {
        java.lang.Object ref = srcPath_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          srcPath_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public com.google.protobuf.ByteString
          getSrcPathBytes() {
        java.lang.Object ref = srcPath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          srcPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder setSrcPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder clearSrcPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        srcPath_ = getDefaultInstance().getSrcPath();
        onChanged();
        return this;
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder setSrcPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }

      // repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;
      private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto> destinations_ =
        java.util.Collections.emptyList();
      private void ensureDestinationsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          destinations_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto>(destinations_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder> destinationsBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto> getDestinationsList() {
        if (destinationsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(destinations_);
        } else {
          return destinationsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public int getDestinationsCount() {
        if (destinationsBuilder_ == null) {
          return destinations_.size();
        } else {
          return destinationsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto getDestinations(int index) {
        if (destinationsBuilder_ == null) {
          return destinations_.get(index);
        } else {
          return destinationsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder setDestinations(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto value) {
        if (destinationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDestinationsIsMutable();
          destinations_.set(index, value);
          onChanged();
        } else {
          destinationsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder setDestinations(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder builderForValue) {
        if (destinationsBuilder_ == null) {
          ensureDestinationsIsMutable();
          destinations_.set(index, builderForValue.build());
          onChanged();
        } else {
          destinationsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder addDestinations(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto value) {
        if (destinationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDestinationsIsMutable();
          destinations_.add(value);
          onChanged();
        } else {
          destinationsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder addDestinations(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto value) {
        if (destinationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDestinationsIsMutable();
          destinations_.add(index, value);
          onChanged();
        } else {
          destinationsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder addDestinations(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder builderForValue) {
        if (destinationsBuilder_ == null) {
          ensureDestinationsIsMutable();
          destinations_.add(builderForValue.build());
          onChanged();
        } else {
          destinationsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder addDestinations(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder builderForValue) {
        if (destinationsBuilder_ == null) {
          ensureDestinationsIsMutable();
          destinations_.add(index, builderForValue.build());
          onChanged();
        } else {
          destinationsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder addAllDestinations(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto> values) {
        if (destinationsBuilder_ == null) {
          ensureDestinationsIsMutable();
          super.addAll(values, destinations_);
          onChanged();
        } else {
          destinationsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder clearDestinations() {
        if (destinationsBuilder_ == null) {
          destinations_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          destinationsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public Builder removeDestinations(int index) {
        if (destinationsBuilder_ == null) {
          ensureDestinationsIsMutable();
          destinations_.remove(index);
          onChanged();
        } else {
          destinationsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder getDestinationsBuilder(
          int index) {
        return getDestinationsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder getDestinationsOrBuilder(
          int index) {
        if (destinationsBuilder_ == null) {
          return destinations_.get(index);  } else {
          return destinationsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder> 
           getDestinationsOrBuilderList() {
        if (destinationsBuilder_ != null) {
          return destinationsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(destinations_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder addDestinationsBuilder() {
        return getDestinationsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder addDestinationsBuilder(
          int index) {
        return getDestinationsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.RemoteLocationProto destinations = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder> 
           getDestinationsBuilderList() {
        return getDestinationsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder> 
          getDestinationsFieldBuilder() {
        if (destinationsBuilder_ == null) {
          destinationsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoteLocationProtoOrBuilder>(
                  destinations_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          destinations_ = null;
        }
        return destinationsBuilder_;
      }

      // optional uint64 dateCreated = 3;
      private long dateCreated_ ;
      /**
       * <code>optional uint64 dateCreated = 3;</code>
       */
      public boolean hasDateCreated() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 dateCreated = 3;</code>
       */
      public long getDateCreated() {
        return dateCreated_;
      }
      /**
       * <code>optional uint64 dateCreated = 3;</code>
       */
      public Builder setDateCreated(long value) {
        bitField0_ |= 0x00000004;
        dateCreated_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateCreated = 3;</code>
       */
      public Builder clearDateCreated() {
        bitField0_ = (bitField0_ & ~0x00000004);
        dateCreated_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 dateModified = 4;
      private long dateModified_ ;
      /**
       * <code>optional uint64 dateModified = 4;</code>
       */
      public boolean hasDateModified() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 dateModified = 4;</code>
       */
      public long getDateModified() {
        return dateModified_;
      }
      /**
       * <code>optional uint64 dateModified = 4;</code>
       */
      public Builder setDateModified(long value) {
        bitField0_ |= 0x00000008;
        dateModified_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateModified = 4;</code>
       */
      public Builder clearDateModified() {
        bitField0_ = (bitField0_ & ~0x00000008);
        dateModified_ = 0L;
        onChanged();
        return this;
      }

      // optional bool readOnly = 5 [default = false];
      private boolean readOnly_ ;
      /**
       * <code>optional bool readOnly = 5 [default = false];</code>
       */
      public boolean hasReadOnly() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional bool readOnly = 5 [default = false];</code>
       */
      public boolean getReadOnly() {
        return readOnly_;
      }
      /**
       * <code>optional bool readOnly = 5 [default = false];</code>
       */
      public Builder setReadOnly(boolean value) {
        bitField0_ |= 0x00000010;
        readOnly_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool readOnly = 5 [default = false];</code>
       */
      public Builder clearReadOnly() {
        bitField0_ = (bitField0_ & ~0x00000010);
        readOnly_ = false;
        onChanged();
        return this;
      }

      // optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder destOrder_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder.HASH;
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
       */
      public boolean hasDestOrder() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder getDestOrder() {
        return destOrder_;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
       */
      public Builder setDestOrder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000020;
        destOrder_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto.DestOrder destOrder = 6 [default = HASH];</code>
       */
      public Builder clearDestOrder() {
        bitField0_ = (bitField0_ & ~0x00000020);
        destOrder_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.DestOrder.HASH;
        onChanged();
        return this;
      }

      // optional string ownerName = 10;
      private java.lang.Object ownerName_ = "";
      /**
       * <code>optional string ownerName = 10;</code>
       */
      public boolean hasOwnerName() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional string ownerName = 10;</code>
       */
      public java.lang.String getOwnerName() {
        java.lang.Object ref = ownerName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          ownerName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string ownerName = 10;</code>
       */
      public com.google.protobuf.ByteString
          getOwnerNameBytes() {
        java.lang.Object ref = ownerName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          ownerName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string ownerName = 10;</code>
       */
      public Builder setOwnerName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        ownerName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string ownerName = 10;</code>
       */
      public Builder clearOwnerName() {
        bitField0_ = (bitField0_ & ~0x00000040);
        ownerName_ = getDefaultInstance().getOwnerName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string ownerName = 10;</code>
       */
      public Builder setOwnerNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        ownerName_ = value;
        onChanged();
        return this;
      }

      // optional string groupName = 11;
      private java.lang.Object groupName_ = "";
      /**
       * <code>optional string groupName = 11;</code>
       */
      public boolean hasGroupName() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional string groupName = 11;</code>
       */
      public java.lang.String getGroupName() {
        java.lang.Object ref = groupName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          groupName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string groupName = 11;</code>
       */
      public com.google.protobuf.ByteString
          getGroupNameBytes() {
        java.lang.Object ref = groupName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          groupName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string groupName = 11;</code>
       */
      public Builder setGroupName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        groupName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string groupName = 11;</code>
       */
      public Builder clearGroupName() {
        bitField0_ = (bitField0_ & ~0x00000080);
        groupName_ = getDefaultInstance().getGroupName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string groupName = 11;</code>
       */
      public Builder setGroupNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        groupName_ = value;
        onChanged();
        return this;
      }

      // optional int32 mode = 12;
      private int mode_ ;
      /**
       * <code>optional int32 mode = 12;</code>
       */
      public boolean hasMode() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional int32 mode = 12;</code>
       */
      public int getMode() {
        return mode_;
      }
      /**
       * <code>optional int32 mode = 12;</code>
       */
      public Builder setMode(int value) {
        bitField0_ |= 0x00000100;
        mode_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 mode = 12;</code>
       */
      public Builder clearMode() {
        bitField0_ = (bitField0_ & ~0x00000100);
        mode_ = 0;
        onChanged();
        return this;
      }

      // optional .hadoop.hdfs.QuotaUsageProto quota = 13;
      private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto quota_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProtoOrBuilder> quotaBuilder_;
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public boolean hasQuota() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto getQuota() {
        if (quotaBuilder_ == null) {
          return quota_;
        } else {
          return quotaBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public Builder setQuota(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto value) {
        if (quotaBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          quota_ = value;
          onChanged();
        } else {
          quotaBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public Builder setQuota(
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.Builder builderForValue) {
        if (quotaBuilder_ == null) {
          quota_ = builderForValue.build();
          onChanged();
        } else {
          quotaBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public Builder mergeQuota(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto value) {
        if (quotaBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200) &&
              quota_ != org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.getDefaultInstance()) {
            quota_ =
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.newBuilder(quota_).mergeFrom(value).buildPartial();
          } else {
            quota_ = value;
          }
          onChanged();
        } else {
          quotaBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public Builder clearQuota() {
        if (quotaBuilder_ == null) {
          quota_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.getDefaultInstance();
          onChanged();
        } else {
          quotaBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.Builder getQuotaBuilder() {
        bitField0_ |= 0x00000200;
        onChanged();
        return getQuotaFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProtoOrBuilder getQuotaOrBuilder() {
        if (quotaBuilder_ != null) {
          return quotaBuilder_.getMessageOrBuilder();
        } else {
          return quota_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.QuotaUsageProto quota = 13;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProtoOrBuilder> 
          getQuotaFieldBuilder() {
        if (quotaBuilder_ == null) {
          quotaBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.QuotaUsageProtoOrBuilder>(
                  quota_,
                  getParentForChildren(),
                  isClean());
          quota_ = null;
        }
        return quotaBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.MountTableRecordProto)
    }

    static {
      defaultInstance = new MountTableRecordProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.MountTableRecordProto)
  }

  public interface AddMountTableEntryRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.hdfs.MountTableRecordProto entry = 1;
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    boolean hasEntry();
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntry();
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntryOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.AddMountTableEntryRequestProto}
   */
  public static final class AddMountTableEntryRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements AddMountTableEntryRequestProtoOrBuilder {
    // Use AddMountTableEntryRequestProto.newBuilder() to construct.
    private AddMountTableEntryRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AddMountTableEntryRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AddMountTableEntryRequestProto defaultInstance;
    public static AddMountTableEntryRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public AddMountTableEntryRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AddMountTableEntryRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = entry_.toBuilder();
              }
              entry_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(entry_);
                entry_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AddMountTableEntryRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<AddMountTableEntryRequestProto>() {
      public AddMountTableEntryRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AddMountTableEntryRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AddMountTableEntryRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.hdfs.MountTableRecordProto entry = 1;
    public static final int ENTRY_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto entry_;
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    public boolean hasEntry() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntry() {
      return entry_;
    }
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntryOrBuilder() {
      return entry_;
    }

    private void initFields() {
      entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasEntry()) {
        if (!getEntry().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, entry_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, entry_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto) obj;

      boolean result = true;
      result = result && (hasEntry() == other.hasEntry());
      if (hasEntry()) {
        result = result && getEntry()
            .equals(other.getEntry());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasEntry()) {
        hash = (37 * hash) + ENTRY_FIELD_NUMBER;
        hash = (53 * hash) + getEntry().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.AddMountTableEntryRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getEntryFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (entryBuilder_ == null) {
          entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
        } else {
          entryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (entryBuilder_ == null) {
          result.entry_ = entry_;
        } else {
          result.entry_ = entryBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto.getDefaultInstance()) return this;
        if (other.hasEntry()) {
          mergeEntry(other.getEntry());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasEntry()) {
          if (!getEntry().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.hdfs.MountTableRecordProto entry = 1;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> entryBuilder_;
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public boolean hasEntry() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntry() {
        if (entryBuilder_ == null) {
          return entry_;
        } else {
          return entryBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder setEntry(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto value) {
        if (entryBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          entry_ = value;
          onChanged();
        } else {
          entryBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder setEntry(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder builderForValue) {
        if (entryBuilder_ == null) {
          entry_ = builderForValue.build();
          onChanged();
        } else {
          entryBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder mergeEntry(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto value) {
        if (entryBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              entry_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance()) {
            entry_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.newBuilder(entry_).mergeFrom(value).buildPartial();
          } else {
            entry_ = value;
          }
          onChanged();
        } else {
          entryBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder clearEntry() {
        if (entryBuilder_ == null) {
          entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
          onChanged();
        } else {
          entryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder getEntryBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getEntryFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntryOrBuilder() {
        if (entryBuilder_ != null) {
          return entryBuilder_.getMessageOrBuilder();
        } else {
          return entry_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> 
          getEntryFieldBuilder() {
        if (entryBuilder_ == null) {
          entryBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder>(
                  entry_,
                  getParentForChildren(),
                  isClean());
          entry_ = null;
        }
        return entryBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.AddMountTableEntryRequestProto)
    }

    static {
      defaultInstance = new AddMountTableEntryRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.AddMountTableEntryRequestProto)
  }

  public interface AddMountTableEntryResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.AddMountTableEntryResponseProto}
   */
  public static final class AddMountTableEntryResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements AddMountTableEntryResponseProtoOrBuilder {
    // Use AddMountTableEntryResponseProto.newBuilder() to construct.
    private AddMountTableEntryResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AddMountTableEntryResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AddMountTableEntryResponseProto defaultInstance;
    public static AddMountTableEntryResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public AddMountTableEntryResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AddMountTableEntryResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AddMountTableEntryResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<AddMountTableEntryResponseProto>() {
      public AddMountTableEntryResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AddMountTableEntryResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AddMountTableEntryResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.AddMountTableEntryResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.AddMountTableEntryResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.AddMountTableEntryResponseProto)
    }

    static {
      defaultInstance = new AddMountTableEntryResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.AddMountTableEntryResponseProto)
  }

  public interface UpdateMountTableEntryRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.hdfs.MountTableRecordProto entry = 1;
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    boolean hasEntry();
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntry();
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntryOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.UpdateMountTableEntryRequestProto}
   */
  public static final class UpdateMountTableEntryRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateMountTableEntryRequestProtoOrBuilder {
    // Use UpdateMountTableEntryRequestProto.newBuilder() to construct.
    private UpdateMountTableEntryRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateMountTableEntryRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateMountTableEntryRequestProto defaultInstance;
    public static UpdateMountTableEntryRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateMountTableEntryRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateMountTableEntryRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = entry_.toBuilder();
              }
              entry_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(entry_);
                entry_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateMountTableEntryRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateMountTableEntryRequestProto>() {
      public UpdateMountTableEntryRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateMountTableEntryRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateMountTableEntryRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.hdfs.MountTableRecordProto entry = 1;
    public static final int ENTRY_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto entry_;
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    public boolean hasEntry() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntry() {
      return entry_;
    }
    /**
     * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntryOrBuilder() {
      return entry_;
    }

    private void initFields() {
      entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasEntry()) {
        if (!getEntry().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, entry_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, entry_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto) obj;

      boolean result = true;
      result = result && (hasEntry() == other.hasEntry());
      if (hasEntry()) {
        result = result && getEntry()
            .equals(other.getEntry());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasEntry()) {
        hash = (37 * hash) + ENTRY_FIELD_NUMBER;
        hash = (53 * hash) + getEntry().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.UpdateMountTableEntryRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getEntryFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (entryBuilder_ == null) {
          entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
        } else {
          entryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (entryBuilder_ == null) {
          result.entry_ = entry_;
        } else {
          result.entry_ = entryBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.getDefaultInstance()) return this;
        if (other.hasEntry()) {
          mergeEntry(other.getEntry());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasEntry()) {
          if (!getEntry().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.hdfs.MountTableRecordProto entry = 1;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> entryBuilder_;
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public boolean hasEntry() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntry() {
        if (entryBuilder_ == null) {
          return entry_;
        } else {
          return entryBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder setEntry(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto value) {
        if (entryBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          entry_ = value;
          onChanged();
        } else {
          entryBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder setEntry(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder builderForValue) {
        if (entryBuilder_ == null) {
          entry_ = builderForValue.build();
          onChanged();
        } else {
          entryBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder mergeEntry(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto value) {
        if (entryBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              entry_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance()) {
            entry_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.newBuilder(entry_).mergeFrom(value).buildPartial();
          } else {
            entry_ = value;
          }
          onChanged();
        } else {
          entryBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public Builder clearEntry() {
        if (entryBuilder_ == null) {
          entry_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance();
          onChanged();
        } else {
          entryBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder getEntryBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getEntryFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntryOrBuilder() {
        if (entryBuilder_ != null) {
          return entryBuilder_.getMessageOrBuilder();
        } else {
          return entry_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.MountTableRecordProto entry = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> 
          getEntryFieldBuilder() {
        if (entryBuilder_ == null) {
          entryBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder>(
                  entry_,
                  getParentForChildren(),
                  isClean());
          entry_ = null;
        }
        return entryBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.UpdateMountTableEntryRequestProto)
    }

    static {
      defaultInstance = new UpdateMountTableEntryRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdateMountTableEntryRequestProto)
  }

  public interface UpdateMountTableEntryResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.UpdateMountTableEntryResponseProto}
   */
  public static final class UpdateMountTableEntryResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateMountTableEntryResponseProtoOrBuilder {
    // Use UpdateMountTableEntryResponseProto.newBuilder() to construct.
    private UpdateMountTableEntryResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateMountTableEntryResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateMountTableEntryResponseProto defaultInstance;
    public static UpdateMountTableEntryResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateMountTableEntryResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateMountTableEntryResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateMountTableEntryResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateMountTableEntryResponseProto>() {
      public UpdateMountTableEntryResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateMountTableEntryResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateMountTableEntryResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.UpdateMountTableEntryResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.UpdateMountTableEntryResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.UpdateMountTableEntryResponseProto)
    }

    static {
      defaultInstance = new UpdateMountTableEntryResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.UpdateMountTableEntryResponseProto)
  }

  public interface RemoveMountTableEntryRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string srcPath = 1;
    /**
     * <code>optional string srcPath = 1;</code>
     */
    boolean hasSrcPath();
    /**
     * <code>optional string srcPath = 1;</code>
     */
    java.lang.String getSrcPath();
    /**
     * <code>optional string srcPath = 1;</code>
     */
    com.google.protobuf.ByteString
        getSrcPathBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.RemoveMountTableEntryRequestProto}
   */
  public static final class RemoveMountTableEntryRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements RemoveMountTableEntryRequestProtoOrBuilder {
    // Use RemoveMountTableEntryRequestProto.newBuilder() to construct.
    private RemoveMountTableEntryRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RemoveMountTableEntryRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RemoveMountTableEntryRequestProto defaultInstance;
    public static RemoveMountTableEntryRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public RemoveMountTableEntryRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RemoveMountTableEntryRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              srcPath_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RemoveMountTableEntryRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<RemoveMountTableEntryRequestProto>() {
      public RemoveMountTableEntryRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RemoveMountTableEntryRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RemoveMountTableEntryRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string srcPath = 1;
    public static final int SRCPATH_FIELD_NUMBER = 1;
    private java.lang.Object srcPath_;
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public boolean hasSrcPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public java.lang.String getSrcPath() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          srcPath_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public com.google.protobuf.ByteString
        getSrcPathBytes() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        srcPath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      srcPath_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getSrcPathBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getSrcPathBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto) obj;

      boolean result = true;
      result = result && (hasSrcPath() == other.hasSrcPath());
      if (hasSrcPath()) {
        result = result && getSrcPath()
            .equals(other.getSrcPath());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasSrcPath()) {
        hash = (37 * hash) + SRCPATH_FIELD_NUMBER;
        hash = (53 * hash) + getSrcPath().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.RemoveMountTableEntryRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        srcPath_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.srcPath_ = srcPath_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.getDefaultInstance()) return this;
        if (other.hasSrcPath()) {
          bitField0_ |= 0x00000001;
          srcPath_ = other.srcPath_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string srcPath = 1;
      private java.lang.Object srcPath_ = "";
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public boolean hasSrcPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public java.lang.String getSrcPath() {
        java.lang.Object ref = srcPath_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          srcPath_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public com.google.protobuf.ByteString
          getSrcPathBytes() {
        java.lang.Object ref = srcPath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          srcPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder setSrcPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder clearSrcPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        srcPath_ = getDefaultInstance().getSrcPath();
        onChanged();
        return this;
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder setSrcPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.RemoveMountTableEntryRequestProto)
    }

    static {
      defaultInstance = new RemoveMountTableEntryRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.RemoveMountTableEntryRequestProto)
  }

  public interface RemoveMountTableEntryResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.RemoveMountTableEntryResponseProto}
   */
  public static final class RemoveMountTableEntryResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements RemoveMountTableEntryResponseProtoOrBuilder {
    // Use RemoveMountTableEntryResponseProto.newBuilder() to construct.
    private RemoveMountTableEntryResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RemoveMountTableEntryResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RemoveMountTableEntryResponseProto defaultInstance;
    public static RemoveMountTableEntryResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public RemoveMountTableEntryResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RemoveMountTableEntryResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RemoveMountTableEntryResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<RemoveMountTableEntryResponseProto>() {
      public RemoveMountTableEntryResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RemoveMountTableEntryResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RemoveMountTableEntryResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.RemoveMountTableEntryResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RemoveMountTableEntryResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.RemoveMountTableEntryResponseProto)
    }

    static {
      defaultInstance = new RemoveMountTableEntryResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.RemoveMountTableEntryResponseProto)
  }

  public interface GetMountTableEntriesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string srcPath = 1;
    /**
     * <code>optional string srcPath = 1;</code>
     */
    boolean hasSrcPath();
    /**
     * <code>optional string srcPath = 1;</code>
     */
    java.lang.String getSrcPath();
    /**
     * <code>optional string srcPath = 1;</code>
     */
    com.google.protobuf.ByteString
        getSrcPathBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetMountTableEntriesRequestProto}
   */
  public static final class GetMountTableEntriesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetMountTableEntriesRequestProtoOrBuilder {
    // Use GetMountTableEntriesRequestProto.newBuilder() to construct.
    private GetMountTableEntriesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetMountTableEntriesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetMountTableEntriesRequestProto defaultInstance;
    public static GetMountTableEntriesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetMountTableEntriesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetMountTableEntriesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              srcPath_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetMountTableEntriesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetMountTableEntriesRequestProto>() {
      public GetMountTableEntriesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetMountTableEntriesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetMountTableEntriesRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string srcPath = 1;
    public static final int SRCPATH_FIELD_NUMBER = 1;
    private java.lang.Object srcPath_;
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public boolean hasSrcPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public java.lang.String getSrcPath() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          srcPath_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string srcPath = 1;</code>
     */
    public com.google.protobuf.ByteString
        getSrcPathBytes() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        srcPath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      srcPath_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getSrcPathBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getSrcPathBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto) obj;

      boolean result = true;
      result = result && (hasSrcPath() == other.hasSrcPath());
      if (hasSrcPath()) {
        result = result && getSrcPath()
            .equals(other.getSrcPath());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasSrcPath()) {
        hash = (37 * hash) + SRCPATH_FIELD_NUMBER;
        hash = (53 * hash) + getSrcPath().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetMountTableEntriesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        srcPath_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.srcPath_ = srcPath_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto.getDefaultInstance()) return this;
        if (other.hasSrcPath()) {
          bitField0_ |= 0x00000001;
          srcPath_ = other.srcPath_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string srcPath = 1;
      private java.lang.Object srcPath_ = "";
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public boolean hasSrcPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public java.lang.String getSrcPath() {
        java.lang.Object ref = srcPath_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          srcPath_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public com.google.protobuf.ByteString
          getSrcPathBytes() {
        java.lang.Object ref = srcPath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          srcPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder setSrcPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder clearSrcPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        srcPath_ = getDefaultInstance().getSrcPath();
        onChanged();
        return this;
      }
      /**
       * <code>optional string srcPath = 1;</code>
       */
      public Builder setSrcPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetMountTableEntriesRequestProto)
    }

    static {
      defaultInstance = new GetMountTableEntriesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetMountTableEntriesRequestProto)
  }

  public interface GetMountTableEntriesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.hdfs.MountTableRecordProto entries = 1;
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto> 
        getEntriesList();
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntries(int index);
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    int getEntriesCount();
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> 
        getEntriesOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntriesOrBuilder(
        int index);

    // optional uint64 timestamp = 2;
    /**
     * <code>optional uint64 timestamp = 2;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>optional uint64 timestamp = 2;</code>
     */
    long getTimestamp();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetMountTableEntriesResponseProto}
   */
  public static final class GetMountTableEntriesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetMountTableEntriesResponseProtoOrBuilder {
    // Use GetMountTableEntriesResponseProto.newBuilder() to construct.
    private GetMountTableEntriesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetMountTableEntriesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetMountTableEntriesResponseProto defaultInstance;
    public static GetMountTableEntriesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetMountTableEntriesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetMountTableEntriesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                entries_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              entries_.add(input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.PARSER, extensionRegistry));
              break;
            }
            case 16: {
              bitField0_ |= 0x00000001;
              timestamp_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          entries_ = java.util.Collections.unmodifiableList(entries_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetMountTableEntriesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetMountTableEntriesResponseProto>() {
      public GetMountTableEntriesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetMountTableEntriesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetMountTableEntriesResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // repeated .hadoop.hdfs.MountTableRecordProto entries = 1;
    public static final int ENTRIES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto> entries_;
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto> getEntriesList() {
      return entries_;
    }
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> 
        getEntriesOrBuilderList() {
      return entries_;
    }
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    public int getEntriesCount() {
      return entries_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntries(int index) {
      return entries_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntriesOrBuilder(
        int index) {
      return entries_.get(index);
    }

    // optional uint64 timestamp = 2;
    public static final int TIMESTAMP_FIELD_NUMBER = 2;
    private long timestamp_;
    /**
     * <code>optional uint64 timestamp = 2;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 timestamp = 2;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    private void initFields() {
      entries_ = java.util.Collections.emptyList();
      timestamp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getEntriesCount(); i++) {
        if (!getEntries(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < entries_.size(); i++) {
        output.writeMessage(1, entries_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(2, timestamp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < entries_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, entries_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, timestamp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto) obj;

      boolean result = true;
      result = result && getEntriesList()
          .equals(other.getEntriesList());
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getEntriesCount() > 0) {
        hash = (37 * hash) + ENTRIES_FIELD_NUMBER;
        hash = (53 * hash) + getEntriesList().hashCode();
      }
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetMountTableEntriesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getEntriesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (entriesBuilder_ == null) {
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          entriesBuilder_.clear();
        }
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (entriesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            entries_ = java.util.Collections.unmodifiableList(entries_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.entries_ = entries_;
        } else {
          result.entries_ = entriesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000001;
        }
        result.timestamp_ = timestamp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto.getDefaultInstance()) return this;
        if (entriesBuilder_ == null) {
          if (!other.entries_.isEmpty()) {
            if (entries_.isEmpty()) {
              entries_ = other.entries_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureEntriesIsMutable();
              entries_.addAll(other.entries_);
            }
            onChanged();
          }
        } else {
          if (!other.entries_.isEmpty()) {
            if (entriesBuilder_.isEmpty()) {
              entriesBuilder_.dispose();
              entriesBuilder_ = null;
              entries_ = other.entries_;
              bitField0_ = (bitField0_ & ~0x00000001);
              entriesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getEntriesFieldBuilder() : null;
            } else {
              entriesBuilder_.addAllMessages(other.entries_);
            }
          }
        }
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getEntriesCount(); i++) {
          if (!getEntries(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetMountTableEntriesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.hdfs.MountTableRecordProto entries = 1;
      private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto> entries_ =
        java.util.Collections.emptyList();
      private void ensureEntriesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          entries_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto>(entries_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> entriesBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto> getEntriesList() {
        if (entriesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(entries_);
        } else {
          return entriesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public int getEntriesCount() {
        if (entriesBuilder_ == null) {
          return entries_.size();
        } else {
          return entriesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto getEntries(int index) {
        if (entriesBuilder_ == null) {
          return entries_.get(index);
        } else {
          return entriesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder setEntries(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.set(index, value);
          onChanged();
        } else {
          entriesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder setEntries(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.set(index, builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder addEntries(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.add(value);
          onChanged();
        } else {
          entriesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder addEntries(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.add(index, value);
          onChanged();
        } else {
          entriesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder addEntries(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.add(builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder addEntries(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.add(index, builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder addAllEntries(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto> values) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          super.addAll(values, entries_);
          onChanged();
        } else {
          entriesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder clearEntries() {
        if (entriesBuilder_ == null) {
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          entriesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public Builder removeEntries(int index) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.remove(index);
          onChanged();
        } else {
          entriesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder getEntriesBuilder(
          int index) {
        return getEntriesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder getEntriesOrBuilder(
          int index) {
        if (entriesBuilder_ == null) {
          return entries_.get(index);  } else {
          return entriesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> 
           getEntriesOrBuilderList() {
        if (entriesBuilder_ != null) {
          return entriesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(entries_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder addEntriesBuilder() {
        return getEntriesFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder addEntriesBuilder(
          int index) {
        return getEntriesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.MountTableRecordProto entries = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder> 
           getEntriesBuilderList() {
        return getEntriesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder> 
          getEntriesFieldBuilder() {
        if (entriesBuilder_ == null) {
          entriesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.MountTableRecordProtoOrBuilder>(
                  entries_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          entries_ = null;
        }
        return entriesBuilder_;
      }

      // optional uint64 timestamp = 2;
      private long timestamp_ ;
      /**
       * <code>optional uint64 timestamp = 2;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 timestamp = 2;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>optional uint64 timestamp = 2;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000002;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 timestamp = 2;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000002);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetMountTableEntriesResponseProto)
    }

    static {
      defaultInstance = new GetMountTableEntriesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetMountTableEntriesResponseProto)
  }

  public interface StateStoreVersionRecordProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint64 membershipVersion = 1;
    /**
     * <code>optional uint64 membershipVersion = 1;</code>
     */
    boolean hasMembershipVersion();
    /**
     * <code>optional uint64 membershipVersion = 1;</code>
     */
    long getMembershipVersion();

    // optional uint64 mountTableVersion = 2;
    /**
     * <code>optional uint64 mountTableVersion = 2;</code>
     */
    boolean hasMountTableVersion();
    /**
     * <code>optional uint64 mountTableVersion = 2;</code>
     */
    long getMountTableVersion();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.StateStoreVersionRecordProto}
   */
  public static final class StateStoreVersionRecordProto extends
      com.google.protobuf.GeneratedMessage
      implements StateStoreVersionRecordProtoOrBuilder {
    // Use StateStoreVersionRecordProto.newBuilder() to construct.
    private StateStoreVersionRecordProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StateStoreVersionRecordProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StateStoreVersionRecordProto defaultInstance;
    public static StateStoreVersionRecordProto getDefaultInstance() {
      return defaultInstance;
    }

    public StateStoreVersionRecordProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StateStoreVersionRecordProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              membershipVersion_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              mountTableVersion_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_StateStoreVersionRecordProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_StateStoreVersionRecordProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StateStoreVersionRecordProto> PARSER =
        new com.google.protobuf.AbstractParser<StateStoreVersionRecordProto>() {
      public StateStoreVersionRecordProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StateStoreVersionRecordProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StateStoreVersionRecordProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint64 membershipVersion = 1;
    public static final int MEMBERSHIPVERSION_FIELD_NUMBER = 1;
    private long membershipVersion_;
    /**
     * <code>optional uint64 membershipVersion = 1;</code>
     */
    public boolean hasMembershipVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 membershipVersion = 1;</code>
     */
    public long getMembershipVersion() {
      return membershipVersion_;
    }

    // optional uint64 mountTableVersion = 2;
    public static final int MOUNTTABLEVERSION_FIELD_NUMBER = 2;
    private long mountTableVersion_;
    /**
     * <code>optional uint64 mountTableVersion = 2;</code>
     */
    public boolean hasMountTableVersion() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 mountTableVersion = 2;</code>
     */
    public long getMountTableVersion() {
      return mountTableVersion_;
    }

    private void initFields() {
      membershipVersion_ = 0L;
      mountTableVersion_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, membershipVersion_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, mountTableVersion_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, membershipVersion_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, mountTableVersion_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto) obj;

      boolean result = true;
      result = result && (hasMembershipVersion() == other.hasMembershipVersion());
      if (hasMembershipVersion()) {
        result = result && (getMembershipVersion()
            == other.getMembershipVersion());
      }
      result = result && (hasMountTableVersion() == other.hasMountTableVersion());
      if (hasMountTableVersion()) {
        result = result && (getMountTableVersion()
            == other.getMountTableVersion());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasMembershipVersion()) {
        hash = (37 * hash) + MEMBERSHIPVERSION_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getMembershipVersion());
      }
      if (hasMountTableVersion()) {
        hash = (37 * hash) + MOUNTTABLEVERSION_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getMountTableVersion());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.StateStoreVersionRecordProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_StateStoreVersionRecordProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_StateStoreVersionRecordProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        membershipVersion_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        mountTableVersion_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_StateStoreVersionRecordProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.membershipVersion_ = membershipVersion_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.mountTableVersion_ = mountTableVersion_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.getDefaultInstance()) return this;
        if (other.hasMembershipVersion()) {
          setMembershipVersion(other.getMembershipVersion());
        }
        if (other.hasMountTableVersion()) {
          setMountTableVersion(other.getMountTableVersion());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint64 membershipVersion = 1;
      private long membershipVersion_ ;
      /**
       * <code>optional uint64 membershipVersion = 1;</code>
       */
      public boolean hasMembershipVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 membershipVersion = 1;</code>
       */
      public long getMembershipVersion() {
        return membershipVersion_;
      }
      /**
       * <code>optional uint64 membershipVersion = 1;</code>
       */
      public Builder setMembershipVersion(long value) {
        bitField0_ |= 0x00000001;
        membershipVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 membershipVersion = 1;</code>
       */
      public Builder clearMembershipVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        membershipVersion_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 mountTableVersion = 2;
      private long mountTableVersion_ ;
      /**
       * <code>optional uint64 mountTableVersion = 2;</code>
       */
      public boolean hasMountTableVersion() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 mountTableVersion = 2;</code>
       */
      public long getMountTableVersion() {
        return mountTableVersion_;
      }
      /**
       * <code>optional uint64 mountTableVersion = 2;</code>
       */
      public Builder setMountTableVersion(long value) {
        bitField0_ |= 0x00000002;
        mountTableVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 mountTableVersion = 2;</code>
       */
      public Builder clearMountTableVersion() {
        bitField0_ = (bitField0_ & ~0x00000002);
        mountTableVersion_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.StateStoreVersionRecordProto)
    }

    static {
      defaultInstance = new StateStoreVersionRecordProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.StateStoreVersionRecordProto)
  }

  public interface RouterRecordProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint64 dateCreated = 1;
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    boolean hasDateCreated();
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    long getDateCreated();

    // optional uint64 dateModified = 2;
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    boolean hasDateModified();
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    long getDateModified();

    // optional string address = 3;
    /**
     * <code>optional string address = 3;</code>
     */
    boolean hasAddress();
    /**
     * <code>optional string address = 3;</code>
     */
    java.lang.String getAddress();
    /**
     * <code>optional string address = 3;</code>
     */
    com.google.protobuf.ByteString
        getAddressBytes();

    // optional string status = 4;
    /**
     * <code>optional string status = 4;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional string status = 4;</code>
     */
    java.lang.String getStatus();
    /**
     * <code>optional string status = 4;</code>
     */
    com.google.protobuf.ByteString
        getStatusBytes();

    // optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;
    /**
     * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
     */
    boolean hasStateStoreVersion();
    /**
     * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto getStateStoreVersion();
    /**
     * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder getStateStoreVersionOrBuilder();

    // optional string version = 6;
    /**
     * <code>optional string version = 6;</code>
     */
    boolean hasVersion();
    /**
     * <code>optional string version = 6;</code>
     */
    java.lang.String getVersion();
    /**
     * <code>optional string version = 6;</code>
     */
    com.google.protobuf.ByteString
        getVersionBytes();

    // optional string compileInfo = 7;
    /**
     * <code>optional string compileInfo = 7;</code>
     */
    boolean hasCompileInfo();
    /**
     * <code>optional string compileInfo = 7;</code>
     */
    java.lang.String getCompileInfo();
    /**
     * <code>optional string compileInfo = 7;</code>
     */
    com.google.protobuf.ByteString
        getCompileInfoBytes();

    // optional uint64 dateStarted = 8;
    /**
     * <code>optional uint64 dateStarted = 8;</code>
     */
    boolean hasDateStarted();
    /**
     * <code>optional uint64 dateStarted = 8;</code>
     */
    long getDateStarted();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.RouterRecordProto}
   */
  public static final class RouterRecordProto extends
      com.google.protobuf.GeneratedMessage
      implements RouterRecordProtoOrBuilder {
    // Use RouterRecordProto.newBuilder() to construct.
    private RouterRecordProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RouterRecordProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RouterRecordProto defaultInstance;
    public static RouterRecordProto getDefaultInstance() {
      return defaultInstance;
    }

    public RouterRecordProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RouterRecordProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              dateCreated_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              dateModified_ = input.readUInt64();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              address_ = input.readBytes();
              break;
            }
            case 34: {
              bitField0_ |= 0x00000008;
              status_ = input.readBytes();
              break;
            }
            case 42: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) == 0x00000010)) {
                subBuilder = stateStoreVersion_.toBuilder();
              }
              stateStoreVersion_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(stateStoreVersion_);
                stateStoreVersion_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 50: {
              bitField0_ |= 0x00000020;
              version_ = input.readBytes();
              break;
            }
            case 58: {
              bitField0_ |= 0x00000040;
              compileInfo_ = input.readBytes();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000080;
              dateStarted_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterRecordProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterRecordProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RouterRecordProto> PARSER =
        new com.google.protobuf.AbstractParser<RouterRecordProto>() {
      public RouterRecordProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RouterRecordProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RouterRecordProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint64 dateCreated = 1;
    public static final int DATECREATED_FIELD_NUMBER = 1;
    private long dateCreated_;
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    public boolean hasDateCreated() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    public long getDateCreated() {
      return dateCreated_;
    }

    // optional uint64 dateModified = 2;
    public static final int DATEMODIFIED_FIELD_NUMBER = 2;
    private long dateModified_;
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    public boolean hasDateModified() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    public long getDateModified() {
      return dateModified_;
    }

    // optional string address = 3;
    public static final int ADDRESS_FIELD_NUMBER = 3;
    private java.lang.Object address_;
    /**
     * <code>optional string address = 3;</code>
     */
    public boolean hasAddress() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string address = 3;</code>
     */
    public java.lang.String getAddress() {
      java.lang.Object ref = address_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          address_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string address = 3;</code>
     */
    public com.google.protobuf.ByteString
        getAddressBytes() {
      java.lang.Object ref = address_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        address_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string status = 4;
    public static final int STATUS_FIELD_NUMBER = 4;
    private java.lang.Object status_;
    /**
     * <code>optional string status = 4;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional string status = 4;</code>
     */
    public java.lang.String getStatus() {
      java.lang.Object ref = status_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          status_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string status = 4;</code>
     */
    public com.google.protobuf.ByteString
        getStatusBytes() {
      java.lang.Object ref = status_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        status_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;
    public static final int STATESTOREVERSION_FIELD_NUMBER = 5;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto stateStoreVersion_;
    /**
     * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
     */
    public boolean hasStateStoreVersion() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto getStateStoreVersion() {
      return stateStoreVersion_;
    }
    /**
     * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder getStateStoreVersionOrBuilder() {
      return stateStoreVersion_;
    }

    // optional string version = 6;
    public static final int VERSION_FIELD_NUMBER = 6;
    private java.lang.Object version_;
    /**
     * <code>optional string version = 6;</code>
     */
    public boolean hasVersion() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional string version = 6;</code>
     */
    public java.lang.String getVersion() {
      java.lang.Object ref = version_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          version_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string version = 6;</code>
     */
    public com.google.protobuf.ByteString
        getVersionBytes() {
      java.lang.Object ref = version_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        version_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string compileInfo = 7;
    public static final int COMPILEINFO_FIELD_NUMBER = 7;
    private java.lang.Object compileInfo_;
    /**
     * <code>optional string compileInfo = 7;</code>
     */
    public boolean hasCompileInfo() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional string compileInfo = 7;</code>
     */
    public java.lang.String getCompileInfo() {
      java.lang.Object ref = compileInfo_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          compileInfo_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string compileInfo = 7;</code>
     */
    public com.google.protobuf.ByteString
        getCompileInfoBytes() {
      java.lang.Object ref = compileInfo_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        compileInfo_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional uint64 dateStarted = 8;
    public static final int DATESTARTED_FIELD_NUMBER = 8;
    private long dateStarted_;
    /**
     * <code>optional uint64 dateStarted = 8;</code>
     */
    public boolean hasDateStarted() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional uint64 dateStarted = 8;</code>
     */
    public long getDateStarted() {
      return dateStarted_;
    }

    private void initFields() {
      dateCreated_ = 0L;
      dateModified_ = 0L;
      address_ = "";
      status_ = "";
      stateStoreVersion_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.getDefaultInstance();
      version_ = "";
      compileInfo_ = "";
      dateStarted_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, dateCreated_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, dateModified_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getAddressBytes());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(4, getStatusBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(5, stateStoreVersion_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeBytes(6, getVersionBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeBytes(7, getCompileInfoBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeUInt64(8, dateStarted_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, dateCreated_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, dateModified_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getAddressBytes());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(4, getStatusBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, stateStoreVersion_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, getVersionBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(7, getCompileInfoBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(8, dateStarted_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto) obj;

      boolean result = true;
      result = result && (hasDateCreated() == other.hasDateCreated());
      if (hasDateCreated()) {
        result = result && (getDateCreated()
            == other.getDateCreated());
      }
      result = result && (hasDateModified() == other.hasDateModified());
      if (hasDateModified()) {
        result = result && (getDateModified()
            == other.getDateModified());
      }
      result = result && (hasAddress() == other.hasAddress());
      if (hasAddress()) {
        result = result && getAddress()
            .equals(other.getAddress());
      }
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && getStatus()
            .equals(other.getStatus());
      }
      result = result && (hasStateStoreVersion() == other.hasStateStoreVersion());
      if (hasStateStoreVersion()) {
        result = result && getStateStoreVersion()
            .equals(other.getStateStoreVersion());
      }
      result = result && (hasVersion() == other.hasVersion());
      if (hasVersion()) {
        result = result && getVersion()
            .equals(other.getVersion());
      }
      result = result && (hasCompileInfo() == other.hasCompileInfo());
      if (hasCompileInfo()) {
        result = result && getCompileInfo()
            .equals(other.getCompileInfo());
      }
      result = result && (hasDateStarted() == other.hasDateStarted());
      if (hasDateStarted()) {
        result = result && (getDateStarted()
            == other.getDateStarted());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasDateCreated()) {
        hash = (37 * hash) + DATECREATED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateCreated());
      }
      if (hasDateModified()) {
        hash = (37 * hash) + DATEMODIFIED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateModified());
      }
      if (hasAddress()) {
        hash = (37 * hash) + ADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getAddress().hashCode();
      }
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getStatus().hashCode();
      }
      if (hasStateStoreVersion()) {
        hash = (37 * hash) + STATESTOREVERSION_FIELD_NUMBER;
        hash = (53 * hash) + getStateStoreVersion().hashCode();
      }
      if (hasVersion()) {
        hash = (37 * hash) + VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getVersion().hashCode();
      }
      if (hasCompileInfo()) {
        hash = (37 * hash) + COMPILEINFO_FIELD_NUMBER;
        hash = (53 * hash) + getCompileInfo().hashCode();
      }
      if (hasDateStarted()) {
        hash = (37 * hash) + DATESTARTED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateStarted());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.RouterRecordProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterRecordProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterRecordProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStateStoreVersionFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        dateCreated_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        dateModified_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        address_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        status_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        if (stateStoreVersionBuilder_ == null) {
          stateStoreVersion_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.getDefaultInstance();
        } else {
          stateStoreVersionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        version_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        compileInfo_ = "";
        bitField0_ = (bitField0_ & ~0x00000040);
        dateStarted_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterRecordProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.dateCreated_ = dateCreated_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.dateModified_ = dateModified_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.address_ = address_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.status_ = status_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        if (stateStoreVersionBuilder_ == null) {
          result.stateStoreVersion_ = stateStoreVersion_;
        } else {
          result.stateStoreVersion_ = stateStoreVersionBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.version_ = version_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.compileInfo_ = compileInfo_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        result.dateStarted_ = dateStarted_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance()) return this;
        if (other.hasDateCreated()) {
          setDateCreated(other.getDateCreated());
        }
        if (other.hasDateModified()) {
          setDateModified(other.getDateModified());
        }
        if (other.hasAddress()) {
          bitField0_ |= 0x00000004;
          address_ = other.address_;
          onChanged();
        }
        if (other.hasStatus()) {
          bitField0_ |= 0x00000008;
          status_ = other.status_;
          onChanged();
        }
        if (other.hasStateStoreVersion()) {
          mergeStateStoreVersion(other.getStateStoreVersion());
        }
        if (other.hasVersion()) {
          bitField0_ |= 0x00000020;
          version_ = other.version_;
          onChanged();
        }
        if (other.hasCompileInfo()) {
          bitField0_ |= 0x00000040;
          compileInfo_ = other.compileInfo_;
          onChanged();
        }
        if (other.hasDateStarted()) {
          setDateStarted(other.getDateStarted());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint64 dateCreated = 1;
      private long dateCreated_ ;
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public boolean hasDateCreated() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public long getDateCreated() {
        return dateCreated_;
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public Builder setDateCreated(long value) {
        bitField0_ |= 0x00000001;
        dateCreated_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public Builder clearDateCreated() {
        bitField0_ = (bitField0_ & ~0x00000001);
        dateCreated_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 dateModified = 2;
      private long dateModified_ ;
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public boolean hasDateModified() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public long getDateModified() {
        return dateModified_;
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public Builder setDateModified(long value) {
        bitField0_ |= 0x00000002;
        dateModified_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public Builder clearDateModified() {
        bitField0_ = (bitField0_ & ~0x00000002);
        dateModified_ = 0L;
        onChanged();
        return this;
      }

      // optional string address = 3;
      private java.lang.Object address_ = "";
      /**
       * <code>optional string address = 3;</code>
       */
      public boolean hasAddress() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string address = 3;</code>
       */
      public java.lang.String getAddress() {
        java.lang.Object ref = address_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          address_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string address = 3;</code>
       */
      public com.google.protobuf.ByteString
          getAddressBytes() {
        java.lang.Object ref = address_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          address_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string address = 3;</code>
       */
      public Builder setAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        address_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string address = 3;</code>
       */
      public Builder clearAddress() {
        bitField0_ = (bitField0_ & ~0x00000004);
        address_ = getDefaultInstance().getAddress();
        onChanged();
        return this;
      }
      /**
       * <code>optional string address = 3;</code>
       */
      public Builder setAddressBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        address_ = value;
        onChanged();
        return this;
      }

      // optional string status = 4;
      private java.lang.Object status_ = "";
      /**
       * <code>optional string status = 4;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional string status = 4;</code>
       */
      public java.lang.String getStatus() {
        java.lang.Object ref = status_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          status_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string status = 4;</code>
       */
      public com.google.protobuf.ByteString
          getStatusBytes() {
        java.lang.Object ref = status_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          status_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string status = 4;</code>
       */
      public Builder setStatus(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string status = 4;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000008);
        status_ = getDefaultInstance().getStatus();
        onChanged();
        return this;
      }
      /**
       * <code>optional string status = 4;</code>
       */
      public Builder setStatusBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        status_ = value;
        onChanged();
        return this;
      }

      // optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto stateStoreVersion_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder> stateStoreVersionBuilder_;
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public boolean hasStateStoreVersion() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto getStateStoreVersion() {
        if (stateStoreVersionBuilder_ == null) {
          return stateStoreVersion_;
        } else {
          return stateStoreVersionBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public Builder setStateStoreVersion(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto value) {
        if (stateStoreVersionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          stateStoreVersion_ = value;
          onChanged();
        } else {
          stateStoreVersionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public Builder setStateStoreVersion(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder builderForValue) {
        if (stateStoreVersionBuilder_ == null) {
          stateStoreVersion_ = builderForValue.build();
          onChanged();
        } else {
          stateStoreVersionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public Builder mergeStateStoreVersion(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto value) {
        if (stateStoreVersionBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              stateStoreVersion_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.getDefaultInstance()) {
            stateStoreVersion_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.newBuilder(stateStoreVersion_).mergeFrom(value).buildPartial();
          } else {
            stateStoreVersion_ = value;
          }
          onChanged();
        } else {
          stateStoreVersionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public Builder clearStateStoreVersion() {
        if (stateStoreVersionBuilder_ == null) {
          stateStoreVersion_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.getDefaultInstance();
          onChanged();
        } else {
          stateStoreVersionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder getStateStoreVersionBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getStateStoreVersionFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder getStateStoreVersionOrBuilder() {
        if (stateStoreVersionBuilder_ != null) {
          return stateStoreVersionBuilder_.getMessageOrBuilder();
        } else {
          return stateStoreVersion_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.StateStoreVersionRecordProto stateStoreVersion = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder> 
          getStateStoreVersionFieldBuilder() {
        if (stateStoreVersionBuilder_ == null) {
          stateStoreVersionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder>(
                  stateStoreVersion_,
                  getParentForChildren(),
                  isClean());
          stateStoreVersion_ = null;
        }
        return stateStoreVersionBuilder_;
      }

      // optional string version = 6;
      private java.lang.Object version_ = "";
      /**
       * <code>optional string version = 6;</code>
       */
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional string version = 6;</code>
       */
      public java.lang.String getVersion() {
        java.lang.Object ref = version_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          version_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string version = 6;</code>
       */
      public com.google.protobuf.ByteString
          getVersionBytes() {
        java.lang.Object ref = version_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          version_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string version = 6;</code>
       */
      public Builder setVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        version_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string version = 6;</code>
       */
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000020);
        version_ = getDefaultInstance().getVersion();
        onChanged();
        return this;
      }
      /**
       * <code>optional string version = 6;</code>
       */
      public Builder setVersionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        version_ = value;
        onChanged();
        return this;
      }

      // optional string compileInfo = 7;
      private java.lang.Object compileInfo_ = "";
      /**
       * <code>optional string compileInfo = 7;</code>
       */
      public boolean hasCompileInfo() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional string compileInfo = 7;</code>
       */
      public java.lang.String getCompileInfo() {
        java.lang.Object ref = compileInfo_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          compileInfo_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string compileInfo = 7;</code>
       */
      public com.google.protobuf.ByteString
          getCompileInfoBytes() {
        java.lang.Object ref = compileInfo_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          compileInfo_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string compileInfo = 7;</code>
       */
      public Builder setCompileInfo(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        compileInfo_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string compileInfo = 7;</code>
       */
      public Builder clearCompileInfo() {
        bitField0_ = (bitField0_ & ~0x00000040);
        compileInfo_ = getDefaultInstance().getCompileInfo();
        onChanged();
        return this;
      }
      /**
       * <code>optional string compileInfo = 7;</code>
       */
      public Builder setCompileInfoBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        compileInfo_ = value;
        onChanged();
        return this;
      }

      // optional uint64 dateStarted = 8;
      private long dateStarted_ ;
      /**
       * <code>optional uint64 dateStarted = 8;</code>
       */
      public boolean hasDateStarted() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional uint64 dateStarted = 8;</code>
       */
      public long getDateStarted() {
        return dateStarted_;
      }
      /**
       * <code>optional uint64 dateStarted = 8;</code>
       */
      public Builder setDateStarted(long value) {
        bitField0_ |= 0x00000080;
        dateStarted_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateStarted = 8;</code>
       */
      public Builder clearDateStarted() {
        bitField0_ = (bitField0_ & ~0x00000080);
        dateStarted_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.RouterRecordProto)
    }

    static {
      defaultInstance = new RouterRecordProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.RouterRecordProto)
  }

  public interface GetRouterRegistrationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string routerId = 1;
    /**
     * <code>optional string routerId = 1;</code>
     */
    boolean hasRouterId();
    /**
     * <code>optional string routerId = 1;</code>
     */
    java.lang.String getRouterId();
    /**
     * <code>optional string routerId = 1;</code>
     */
    com.google.protobuf.ByteString
        getRouterIdBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationRequestProto}
   */
  public static final class GetRouterRegistrationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetRouterRegistrationRequestProtoOrBuilder {
    // Use GetRouterRegistrationRequestProto.newBuilder() to construct.
    private GetRouterRegistrationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetRouterRegistrationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetRouterRegistrationRequestProto defaultInstance;
    public static GetRouterRegistrationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetRouterRegistrationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetRouterRegistrationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              routerId_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetRouterRegistrationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetRouterRegistrationRequestProto>() {
      public GetRouterRegistrationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetRouterRegistrationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetRouterRegistrationRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string routerId = 1;
    public static final int ROUTERID_FIELD_NUMBER = 1;
    private java.lang.Object routerId_;
    /**
     * <code>optional string routerId = 1;</code>
     */
    public boolean hasRouterId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string routerId = 1;</code>
     */
    public java.lang.String getRouterId() {
      java.lang.Object ref = routerId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          routerId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string routerId = 1;</code>
     */
    public com.google.protobuf.ByteString
        getRouterIdBytes() {
      java.lang.Object ref = routerId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        routerId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      routerId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getRouterIdBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getRouterIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto) obj;

      boolean result = true;
      result = result && (hasRouterId() == other.hasRouterId());
      if (hasRouterId()) {
        result = result && getRouterId()
            .equals(other.getRouterId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasRouterId()) {
        hash = (37 * hash) + ROUTERID_FIELD_NUMBER;
        hash = (53 * hash) + getRouterId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        routerId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.routerId_ = routerId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto.getDefaultInstance()) return this;
        if (other.hasRouterId()) {
          bitField0_ |= 0x00000001;
          routerId_ = other.routerId_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string routerId = 1;
      private java.lang.Object routerId_ = "";
      /**
       * <code>optional string routerId = 1;</code>
       */
      public boolean hasRouterId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string routerId = 1;</code>
       */
      public java.lang.String getRouterId() {
        java.lang.Object ref = routerId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          routerId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string routerId = 1;</code>
       */
      public com.google.protobuf.ByteString
          getRouterIdBytes() {
        java.lang.Object ref = routerId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          routerId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string routerId = 1;</code>
       */
      public Builder setRouterId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        routerId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string routerId = 1;</code>
       */
      public Builder clearRouterId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        routerId_ = getDefaultInstance().getRouterId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string routerId = 1;</code>
       */
      public Builder setRouterIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        routerId_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetRouterRegistrationRequestProto)
    }

    static {
      defaultInstance = new GetRouterRegistrationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetRouterRegistrationRequestProto)
  }

  public interface GetRouterRegistrationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.hdfs.RouterRecordProto router = 1;
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    boolean hasRouter();
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouter();
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRouterOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationResponseProto}
   */
  public static final class GetRouterRegistrationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetRouterRegistrationResponseProtoOrBuilder {
    // Use GetRouterRegistrationResponseProto.newBuilder() to construct.
    private GetRouterRegistrationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetRouterRegistrationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetRouterRegistrationResponseProto defaultInstance;
    public static GetRouterRegistrationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetRouterRegistrationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetRouterRegistrationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = router_.toBuilder();
              }
              router_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(router_);
                router_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetRouterRegistrationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetRouterRegistrationResponseProto>() {
      public GetRouterRegistrationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetRouterRegistrationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetRouterRegistrationResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.hdfs.RouterRecordProto router = 1;
    public static final int ROUTER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto router_;
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    public boolean hasRouter() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouter() {
      return router_;
    }
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRouterOrBuilder() {
      return router_;
    }

    private void initFields() {
      router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, router_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, router_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto) obj;

      boolean result = true;
      result = result && (hasRouter() == other.hasRouter());
      if (hasRouter()) {
        result = result && getRouter()
            .equals(other.getRouter());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasRouter()) {
        hash = (37 * hash) + ROUTER_FIELD_NUMBER;
        hash = (53 * hash) + getRouter().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRouterFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (routerBuilder_ == null) {
          router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
        } else {
          routerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (routerBuilder_ == null) {
          result.router_ = router_;
        } else {
          result.router_ = routerBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto.getDefaultInstance()) return this;
        if (other.hasRouter()) {
          mergeRouter(other.getRouter());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.hdfs.RouterRecordProto router = 1;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> routerBuilder_;
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public boolean hasRouter() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouter() {
        if (routerBuilder_ == null) {
          return router_;
        } else {
          return routerBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder setRouter(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto value) {
        if (routerBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          router_ = value;
          onChanged();
        } else {
          routerBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder setRouter(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder builderForValue) {
        if (routerBuilder_ == null) {
          router_ = builderForValue.build();
          onChanged();
        } else {
          routerBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder mergeRouter(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto value) {
        if (routerBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              router_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance()) {
            router_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.newBuilder(router_).mergeFrom(value).buildPartial();
          } else {
            router_ = value;
          }
          onChanged();
        } else {
          routerBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder clearRouter() {
        if (routerBuilder_ == null) {
          router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
          onChanged();
        } else {
          routerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder getRouterBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRouterFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRouterOrBuilder() {
        if (routerBuilder_ != null) {
          return routerBuilder_.getMessageOrBuilder();
        } else {
          return router_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> 
          getRouterFieldBuilder() {
        if (routerBuilder_ == null) {
          routerBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder>(
                  router_,
                  getParentForChildren(),
                  isClean());
          router_ = null;
        }
        return routerBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetRouterRegistrationResponseProto)
    }

    static {
      defaultInstance = new GetRouterRegistrationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetRouterRegistrationResponseProto)
  }

  public interface GetRouterRegistrationsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationsRequestProto}
   */
  public static final class GetRouterRegistrationsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetRouterRegistrationsRequestProtoOrBuilder {
    // Use GetRouterRegistrationsRequestProto.newBuilder() to construct.
    private GetRouterRegistrationsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetRouterRegistrationsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetRouterRegistrationsRequestProto defaultInstance;
    public static GetRouterRegistrationsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetRouterRegistrationsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetRouterRegistrationsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetRouterRegistrationsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetRouterRegistrationsRequestProto>() {
      public GetRouterRegistrationsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetRouterRegistrationsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetRouterRegistrationsRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetRouterRegistrationsRequestProto)
    }

    static {
      defaultInstance = new GetRouterRegistrationsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetRouterRegistrationsRequestProto)
  }

  public interface GetRouterRegistrationsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint64 timestamp = 1;
    /**
     * <code>optional uint64 timestamp = 1;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>optional uint64 timestamp = 1;</code>
     */
    long getTimestamp();

    // repeated .hadoop.hdfs.RouterRecordProto routers = 2;
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto> 
        getRoutersList();
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouters(int index);
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    int getRoutersCount();
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> 
        getRoutersOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRoutersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationsResponseProto}
   */
  public static final class GetRouterRegistrationsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetRouterRegistrationsResponseProtoOrBuilder {
    // Use GetRouterRegistrationsResponseProto.newBuilder() to construct.
    private GetRouterRegistrationsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetRouterRegistrationsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetRouterRegistrationsResponseProto defaultInstance;
    public static GetRouterRegistrationsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetRouterRegistrationsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetRouterRegistrationsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              timestamp_ = input.readUInt64();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                routers_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              routers_.add(input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          routers_ = java.util.Collections.unmodifiableList(routers_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetRouterRegistrationsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetRouterRegistrationsResponseProto>() {
      public GetRouterRegistrationsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetRouterRegistrationsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetRouterRegistrationsResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint64 timestamp = 1;
    public static final int TIMESTAMP_FIELD_NUMBER = 1;
    private long timestamp_;
    /**
     * <code>optional uint64 timestamp = 1;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 timestamp = 1;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    // repeated .hadoop.hdfs.RouterRecordProto routers = 2;
    public static final int ROUTERS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto> routers_;
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto> getRoutersList() {
      return routers_;
    }
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> 
        getRoutersOrBuilderList() {
      return routers_;
    }
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    public int getRoutersCount() {
      return routers_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouters(int index) {
      return routers_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRoutersOrBuilder(
        int index) {
      return routers_.get(index);
    }

    private void initFields() {
      timestamp_ = 0L;
      routers_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, timestamp_);
      }
      for (int i = 0; i < routers_.size(); i++) {
        output.writeMessage(2, routers_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, timestamp_);
      }
      for (int i = 0; i < routers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, routers_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto) obj;

      boolean result = true;
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result && getRoutersList()
          .equals(other.getRoutersList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      if (getRoutersCount() > 0) {
        hash = (37 * hash) + ROUTERS_FIELD_NUMBER;
        hash = (53 * hash) + getRoutersList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetRouterRegistrationsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRoutersFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (routersBuilder_ == null) {
          routers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          routersBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.timestamp_ = timestamp_;
        if (routersBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            routers_ = java.util.Collections.unmodifiableList(routers_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.routers_ = routers_;
        } else {
          result.routers_ = routersBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.getDefaultInstance()) return this;
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        if (routersBuilder_ == null) {
          if (!other.routers_.isEmpty()) {
            if (routers_.isEmpty()) {
              routers_ = other.routers_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureRoutersIsMutable();
              routers_.addAll(other.routers_);
            }
            onChanged();
          }
        } else {
          if (!other.routers_.isEmpty()) {
            if (routersBuilder_.isEmpty()) {
              routersBuilder_.dispose();
              routersBuilder_ = null;
              routers_ = other.routers_;
              bitField0_ = (bitField0_ & ~0x00000002);
              routersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getRoutersFieldBuilder() : null;
            } else {
              routersBuilder_.addAllMessages(other.routers_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetRouterRegistrationsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint64 timestamp = 1;
      private long timestamp_ ;
      /**
       * <code>optional uint64 timestamp = 1;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 timestamp = 1;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>optional uint64 timestamp = 1;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000001;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 timestamp = 1;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000001);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // repeated .hadoop.hdfs.RouterRecordProto routers = 2;
      private java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto> routers_ =
        java.util.Collections.emptyList();
      private void ensureRoutersIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          routers_ = new java.util.ArrayList<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto>(routers_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> routersBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto> getRoutersList() {
        if (routersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(routers_);
        } else {
          return routersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public int getRoutersCount() {
        if (routersBuilder_ == null) {
          return routers_.size();
        } else {
          return routersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouters(int index) {
        if (routersBuilder_ == null) {
          return routers_.get(index);
        } else {
          return routersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder setRouters(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto value) {
        if (routersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRoutersIsMutable();
          routers_.set(index, value);
          onChanged();
        } else {
          routersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder setRouters(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder builderForValue) {
        if (routersBuilder_ == null) {
          ensureRoutersIsMutable();
          routers_.set(index, builderForValue.build());
          onChanged();
        } else {
          routersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder addRouters(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto value) {
        if (routersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRoutersIsMutable();
          routers_.add(value);
          onChanged();
        } else {
          routersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder addRouters(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto value) {
        if (routersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRoutersIsMutable();
          routers_.add(index, value);
          onChanged();
        } else {
          routersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder addRouters(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder builderForValue) {
        if (routersBuilder_ == null) {
          ensureRoutersIsMutable();
          routers_.add(builderForValue.build());
          onChanged();
        } else {
          routersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder addRouters(
          int index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder builderForValue) {
        if (routersBuilder_ == null) {
          ensureRoutersIsMutable();
          routers_.add(index, builderForValue.build());
          onChanged();
        } else {
          routersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder addAllRouters(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto> values) {
        if (routersBuilder_ == null) {
          ensureRoutersIsMutable();
          super.addAll(values, routers_);
          onChanged();
        } else {
          routersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder clearRouters() {
        if (routersBuilder_ == null) {
          routers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          routersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public Builder removeRouters(int index) {
        if (routersBuilder_ == null) {
          ensureRoutersIsMutable();
          routers_.remove(index);
          onChanged();
        } else {
          routersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder getRoutersBuilder(
          int index) {
        return getRoutersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRoutersOrBuilder(
          int index) {
        if (routersBuilder_ == null) {
          return routers_.get(index);  } else {
          return routersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> 
           getRoutersOrBuilderList() {
        if (routersBuilder_ != null) {
          return routersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(routers_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder addRoutersBuilder() {
        return getRoutersFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder addRoutersBuilder(
          int index) {
        return getRoutersFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.RouterRecordProto routers = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder> 
           getRoutersBuilderList() {
        return getRoutersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> 
          getRoutersFieldBuilder() {
        if (routersBuilder_ == null) {
          routersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder>(
                  routers_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          routers_ = null;
        }
        return routersBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetRouterRegistrationsResponseProto)
    }

    static {
      defaultInstance = new GetRouterRegistrationsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetRouterRegistrationsResponseProto)
  }

  public interface RouterHeartbeatRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.hdfs.RouterRecordProto router = 1;
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    boolean hasRouter();
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouter();
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRouterOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.RouterHeartbeatRequestProto}
   */
  public static final class RouterHeartbeatRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements RouterHeartbeatRequestProtoOrBuilder {
    // Use RouterHeartbeatRequestProto.newBuilder() to construct.
    private RouterHeartbeatRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RouterHeartbeatRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RouterHeartbeatRequestProto defaultInstance;
    public static RouterHeartbeatRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public RouterHeartbeatRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RouterHeartbeatRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = router_.toBuilder();
              }
              router_ = input.readMessage(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(router_);
                router_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RouterHeartbeatRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<RouterHeartbeatRequestProto>() {
      public RouterHeartbeatRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RouterHeartbeatRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RouterHeartbeatRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.hdfs.RouterRecordProto router = 1;
    public static final int ROUTER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto router_;
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    public boolean hasRouter() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouter() {
      return router_;
    }
    /**
     * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
     */
    public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRouterOrBuilder() {
      return router_;
    }

    private void initFields() {
      router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, router_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, router_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto) obj;

      boolean result = true;
      result = result && (hasRouter() == other.hasRouter());
      if (hasRouter()) {
        result = result && getRouter()
            .equals(other.getRouter());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasRouter()) {
        hash = (37 * hash) + ROUTER_FIELD_NUMBER;
        hash = (53 * hash) + getRouter().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.RouterHeartbeatRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRouterFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (routerBuilder_ == null) {
          router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
        } else {
          routerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (routerBuilder_ == null) {
          result.router_ = router_;
        } else {
          result.router_ = routerBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto.getDefaultInstance()) return this;
        if (other.hasRouter()) {
          mergeRouter(other.getRouter());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.hdfs.RouterRecordProto router = 1;
      private org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> routerBuilder_;
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public boolean hasRouter() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto getRouter() {
        if (routerBuilder_ == null) {
          return router_;
        } else {
          return routerBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder setRouter(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto value) {
        if (routerBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          router_ = value;
          onChanged();
        } else {
          routerBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder setRouter(
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder builderForValue) {
        if (routerBuilder_ == null) {
          router_ = builderForValue.build();
          onChanged();
        } else {
          routerBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder mergeRouter(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto value) {
        if (routerBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              router_ != org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance()) {
            router_ =
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.newBuilder(router_).mergeFrom(value).buildPartial();
          } else {
            router_ = value;
          }
          onChanged();
        } else {
          routerBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public Builder clearRouter() {
        if (routerBuilder_ == null) {
          router_ = org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.getDefaultInstance();
          onChanged();
        } else {
          routerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder getRouterBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRouterFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder getRouterOrBuilder() {
        if (routerBuilder_ != null) {
          return routerBuilder_.getMessageOrBuilder();
        } else {
          return router_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.RouterRecordProto router = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder> 
          getRouterFieldBuilder() {
        if (routerBuilder_ == null) {
          routerBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProto.Builder, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterRecordProtoOrBuilder>(
                  router_,
                  getParentForChildren(),
                  isClean());
          router_ = null;
        }
        return routerBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.RouterHeartbeatRequestProto)
    }

    static {
      defaultInstance = new RouterHeartbeatRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.RouterHeartbeatRequestProto)
  }

  public interface RouterHeartbeatResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.RouterHeartbeatResponseProto}
   */
  public static final class RouterHeartbeatResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements RouterHeartbeatResponseProtoOrBuilder {
    // Use RouterHeartbeatResponseProto.newBuilder() to construct.
    private RouterHeartbeatResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RouterHeartbeatResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RouterHeartbeatResponseProto defaultInstance;
    public static RouterHeartbeatResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public RouterHeartbeatResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RouterHeartbeatResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RouterHeartbeatResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<RouterHeartbeatResponseProto>() {
      public RouterHeartbeatResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RouterHeartbeatResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RouterHeartbeatResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.RouterHeartbeatResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.RouterHeartbeatResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.RouterHeartbeatResponseProto)
    }

    static {
      defaultInstance = new RouterHeartbeatResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.RouterHeartbeatResponseProto)
  }

  public interface EnterSafeModeRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.EnterSafeModeRequestProto}
   */
  public static final class EnterSafeModeRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements EnterSafeModeRequestProtoOrBuilder {
    // Use EnterSafeModeRequestProto.newBuilder() to construct.
    private EnterSafeModeRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private EnterSafeModeRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final EnterSafeModeRequestProto defaultInstance;
    public static EnterSafeModeRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public EnterSafeModeRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private EnterSafeModeRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<EnterSafeModeRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<EnterSafeModeRequestProto>() {
      public EnterSafeModeRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EnterSafeModeRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<EnterSafeModeRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.EnterSafeModeRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.EnterSafeModeRequestProto)
    }

    static {
      defaultInstance = new EnterSafeModeRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.EnterSafeModeRequestProto)
  }

  public interface EnterSafeModeResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.EnterSafeModeResponseProto}
   */
  public static final class EnterSafeModeResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements EnterSafeModeResponseProtoOrBuilder {
    // Use EnterSafeModeResponseProto.newBuilder() to construct.
    private EnterSafeModeResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private EnterSafeModeResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final EnterSafeModeResponseProto defaultInstance;
    public static EnterSafeModeResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public EnterSafeModeResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private EnterSafeModeResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<EnterSafeModeResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<EnterSafeModeResponseProto>() {
      public EnterSafeModeResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EnterSafeModeResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<EnterSafeModeResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.EnterSafeModeResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnterSafeModeResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnterSafeModeResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.EnterSafeModeResponseProto)
    }

    static {
      defaultInstance = new EnterSafeModeResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.EnterSafeModeResponseProto)
  }

  public interface LeaveSafeModeRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.LeaveSafeModeRequestProto}
   */
  public static final class LeaveSafeModeRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements LeaveSafeModeRequestProtoOrBuilder {
    // Use LeaveSafeModeRequestProto.newBuilder() to construct.
    private LeaveSafeModeRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private LeaveSafeModeRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final LeaveSafeModeRequestProto defaultInstance;
    public static LeaveSafeModeRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public LeaveSafeModeRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private LeaveSafeModeRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<LeaveSafeModeRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<LeaveSafeModeRequestProto>() {
      public LeaveSafeModeRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new LeaveSafeModeRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<LeaveSafeModeRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.LeaveSafeModeRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.LeaveSafeModeRequestProto)
    }

    static {
      defaultInstance = new LeaveSafeModeRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.LeaveSafeModeRequestProto)
  }

  public interface LeaveSafeModeResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.LeaveSafeModeResponseProto}
   */
  public static final class LeaveSafeModeResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements LeaveSafeModeResponseProtoOrBuilder {
    // Use LeaveSafeModeResponseProto.newBuilder() to construct.
    private LeaveSafeModeResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private LeaveSafeModeResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final LeaveSafeModeResponseProto defaultInstance;
    public static LeaveSafeModeResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public LeaveSafeModeResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private LeaveSafeModeResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<LeaveSafeModeResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<LeaveSafeModeResponseProto>() {
      public LeaveSafeModeResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new LeaveSafeModeResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<LeaveSafeModeResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.LeaveSafeModeResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.LeaveSafeModeResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.LeaveSafeModeResponseProto)
    }

    static {
      defaultInstance = new LeaveSafeModeResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.LeaveSafeModeResponseProto)
  }

  public interface GetSafeModeRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetSafeModeRequestProto}
   */
  public static final class GetSafeModeRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetSafeModeRequestProtoOrBuilder {
    // Use GetSafeModeRequestProto.newBuilder() to construct.
    private GetSafeModeRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetSafeModeRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetSafeModeRequestProto defaultInstance;
    public static GetSafeModeRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetSafeModeRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetSafeModeRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetSafeModeRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetSafeModeRequestProto>() {
      public GetSafeModeRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetSafeModeRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetSafeModeRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetSafeModeRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetSafeModeRequestProto)
    }

    static {
      defaultInstance = new GetSafeModeRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetSafeModeRequestProto)
  }

  public interface GetSafeModeResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool isInSafeMode = 1;
    /**
     * <code>optional bool isInSafeMode = 1;</code>
     */
    boolean hasIsInSafeMode();
    /**
     * <code>optional bool isInSafeMode = 1;</code>
     */
    boolean getIsInSafeMode();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetSafeModeResponseProto}
   */
  public static final class GetSafeModeResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetSafeModeResponseProtoOrBuilder {
    // Use GetSafeModeResponseProto.newBuilder() to construct.
    private GetSafeModeResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetSafeModeResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetSafeModeResponseProto defaultInstance;
    public static GetSafeModeResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetSafeModeResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetSafeModeResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              isInSafeMode_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetSafeModeResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetSafeModeResponseProto>() {
      public GetSafeModeResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetSafeModeResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetSafeModeResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool isInSafeMode = 1;
    public static final int ISINSAFEMODE_FIELD_NUMBER = 1;
    private boolean isInSafeMode_;
    /**
     * <code>optional bool isInSafeMode = 1;</code>
     */
    public boolean hasIsInSafeMode() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool isInSafeMode = 1;</code>
     */
    public boolean getIsInSafeMode() {
      return isInSafeMode_;
    }

    private void initFields() {
      isInSafeMode_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, isInSafeMode_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, isInSafeMode_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto) obj;

      boolean result = true;
      result = result && (hasIsInSafeMode() == other.hasIsInSafeMode());
      if (hasIsInSafeMode()) {
        result = result && (getIsInSafeMode()
            == other.getIsInSafeMode());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasIsInSafeMode()) {
        hash = (37 * hash) + ISINSAFEMODE_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIsInSafeMode());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetSafeModeResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        isInSafeMode_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetSafeModeResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.isInSafeMode_ = isInSafeMode_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto.getDefaultInstance()) return this;
        if (other.hasIsInSafeMode()) {
          setIsInSafeMode(other.getIsInSafeMode());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetSafeModeResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool isInSafeMode = 1;
      private boolean isInSafeMode_ ;
      /**
       * <code>optional bool isInSafeMode = 1;</code>
       */
      public boolean hasIsInSafeMode() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool isInSafeMode = 1;</code>
       */
      public boolean getIsInSafeMode() {
        return isInSafeMode_;
      }
      /**
       * <code>optional bool isInSafeMode = 1;</code>
       */
      public Builder setIsInSafeMode(boolean value) {
        bitField0_ |= 0x00000001;
        isInSafeMode_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool isInSafeMode = 1;</code>
       */
      public Builder clearIsInSafeMode() {
        bitField0_ = (bitField0_ & ~0x00000001);
        isInSafeMode_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetSafeModeResponseProto)
    }

    static {
      defaultInstance = new GetSafeModeResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetSafeModeResponseProto)
  }

  public interface DisabledNameserviceRecordProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint64 dateCreated = 1;
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    boolean hasDateCreated();
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    long getDateCreated();

    // optional uint64 dateModified = 2;
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    boolean hasDateModified();
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    long getDateModified();

    // optional string nameServiceId = 3;
    /**
     * <code>optional string nameServiceId = 3;</code>
     */
    boolean hasNameServiceId();
    /**
     * <code>optional string nameServiceId = 3;</code>
     */
    java.lang.String getNameServiceId();
    /**
     * <code>optional string nameServiceId = 3;</code>
     */
    com.google.protobuf.ByteString
        getNameServiceIdBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.DisabledNameserviceRecordProto}
   */
  public static final class DisabledNameserviceRecordProto extends
      com.google.protobuf.GeneratedMessage
      implements DisabledNameserviceRecordProtoOrBuilder {
    // Use DisabledNameserviceRecordProto.newBuilder() to construct.
    private DisabledNameserviceRecordProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private DisabledNameserviceRecordProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final DisabledNameserviceRecordProto defaultInstance;
    public static DisabledNameserviceRecordProto getDefaultInstance() {
      return defaultInstance;
    }

    public DisabledNameserviceRecordProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private DisabledNameserviceRecordProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              dateCreated_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              dateModified_ = input.readUInt64();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              nameServiceId_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto.Builder.class);
    }

    public static com.google.protobuf.Parser<DisabledNameserviceRecordProto> PARSER =
        new com.google.protobuf.AbstractParser<DisabledNameserviceRecordProto>() {
      public DisabledNameserviceRecordProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new DisabledNameserviceRecordProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<DisabledNameserviceRecordProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint64 dateCreated = 1;
    public static final int DATECREATED_FIELD_NUMBER = 1;
    private long dateCreated_;
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    public boolean hasDateCreated() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 dateCreated = 1;</code>
     */
    public long getDateCreated() {
      return dateCreated_;
    }

    // optional uint64 dateModified = 2;
    public static final int DATEMODIFIED_FIELD_NUMBER = 2;
    private long dateModified_;
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    public boolean hasDateModified() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 dateModified = 2;</code>
     */
    public long getDateModified() {
      return dateModified_;
    }

    // optional string nameServiceId = 3;
    public static final int NAMESERVICEID_FIELD_NUMBER = 3;
    private java.lang.Object nameServiceId_;
    /**
     * <code>optional string nameServiceId = 3;</code>
     */
    public boolean hasNameServiceId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string nameServiceId = 3;</code>
     */
    public java.lang.String getNameServiceId() {
      java.lang.Object ref = nameServiceId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nameServiceId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nameServiceId = 3;</code>
     */
    public com.google.protobuf.ByteString
        getNameServiceIdBytes() {
      java.lang.Object ref = nameServiceId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nameServiceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      dateCreated_ = 0L;
      dateModified_ = 0L;
      nameServiceId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, dateCreated_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, dateModified_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getNameServiceIdBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, dateCreated_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, dateModified_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getNameServiceIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto) obj;

      boolean result = true;
      result = result && (hasDateCreated() == other.hasDateCreated());
      if (hasDateCreated()) {
        result = result && (getDateCreated()
            == other.getDateCreated());
      }
      result = result && (hasDateModified() == other.hasDateModified());
      if (hasDateModified()) {
        result = result && (getDateModified()
            == other.getDateModified());
      }
      result = result && (hasNameServiceId() == other.hasNameServiceId());
      if (hasNameServiceId()) {
        result = result && getNameServiceId()
            .equals(other.getNameServiceId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasDateCreated()) {
        hash = (37 * hash) + DATECREATED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateCreated());
      }
      if (hasDateModified()) {
        hash = (37 * hash) + DATEMODIFIED_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDateModified());
      }
      if (hasNameServiceId()) {
        hash = (37 * hash) + NAMESERVICEID_FIELD_NUMBER;
        hash = (53 * hash) + getNameServiceId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.DisabledNameserviceRecordProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        dateCreated_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        dateModified_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        nameServiceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.dateCreated_ = dateCreated_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.dateModified_ = dateModified_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.nameServiceId_ = nameServiceId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto.getDefaultInstance()) return this;
        if (other.hasDateCreated()) {
          setDateCreated(other.getDateCreated());
        }
        if (other.hasDateModified()) {
          setDateModified(other.getDateModified());
        }
        if (other.hasNameServiceId()) {
          bitField0_ |= 0x00000004;
          nameServiceId_ = other.nameServiceId_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisabledNameserviceRecordProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint64 dateCreated = 1;
      private long dateCreated_ ;
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public boolean hasDateCreated() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public long getDateCreated() {
        return dateCreated_;
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public Builder setDateCreated(long value) {
        bitField0_ |= 0x00000001;
        dateCreated_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateCreated = 1;</code>
       */
      public Builder clearDateCreated() {
        bitField0_ = (bitField0_ & ~0x00000001);
        dateCreated_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 dateModified = 2;
      private long dateModified_ ;
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public boolean hasDateModified() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public long getDateModified() {
        return dateModified_;
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public Builder setDateModified(long value) {
        bitField0_ |= 0x00000002;
        dateModified_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 dateModified = 2;</code>
       */
      public Builder clearDateModified() {
        bitField0_ = (bitField0_ & ~0x00000002);
        dateModified_ = 0L;
        onChanged();
        return this;
      }

      // optional string nameServiceId = 3;
      private java.lang.Object nameServiceId_ = "";
      /**
       * <code>optional string nameServiceId = 3;</code>
       */
      public boolean hasNameServiceId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string nameServiceId = 3;</code>
       */
      public java.lang.String getNameServiceId() {
        java.lang.Object ref = nameServiceId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          nameServiceId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nameServiceId = 3;</code>
       */
      public com.google.protobuf.ByteString
          getNameServiceIdBytes() {
        java.lang.Object ref = nameServiceId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nameServiceId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nameServiceId = 3;</code>
       */
      public Builder setNameServiceId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        nameServiceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameServiceId = 3;</code>
       */
      public Builder clearNameServiceId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        nameServiceId_ = getDefaultInstance().getNameServiceId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameServiceId = 3;</code>
       */
      public Builder setNameServiceIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        nameServiceId_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.DisabledNameserviceRecordProto)
    }

    static {
      defaultInstance = new DisabledNameserviceRecordProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.DisabledNameserviceRecordProto)
  }

  public interface DisableNameserviceRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string nameServiceId = 1;
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    boolean hasNameServiceId();
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    java.lang.String getNameServiceId();
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameServiceIdBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.DisableNameserviceRequestProto}
   */
  public static final class DisableNameserviceRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements DisableNameserviceRequestProtoOrBuilder {
    // Use DisableNameserviceRequestProto.newBuilder() to construct.
    private DisableNameserviceRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private DisableNameserviceRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final DisableNameserviceRequestProto defaultInstance;
    public static DisableNameserviceRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public DisableNameserviceRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private DisableNameserviceRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              nameServiceId_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<DisableNameserviceRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<DisableNameserviceRequestProto>() {
      public DisableNameserviceRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new DisableNameserviceRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<DisableNameserviceRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string nameServiceId = 1;
    public static final int NAMESERVICEID_FIELD_NUMBER = 1;
    private java.lang.Object nameServiceId_;
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    public boolean hasNameServiceId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    public java.lang.String getNameServiceId() {
      java.lang.Object ref = nameServiceId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nameServiceId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameServiceIdBytes() {
      java.lang.Object ref = nameServiceId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nameServiceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      nameServiceId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getNameServiceIdBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getNameServiceIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto) obj;

      boolean result = true;
      result = result && (hasNameServiceId() == other.hasNameServiceId());
      if (hasNameServiceId()) {
        result = result && getNameServiceId()
            .equals(other.getNameServiceId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNameServiceId()) {
        hash = (37 * hash) + NAMESERVICEID_FIELD_NUMBER;
        hash = (53 * hash) + getNameServiceId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.DisableNameserviceRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nameServiceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.nameServiceId_ = nameServiceId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto.getDefaultInstance()) return this;
        if (other.hasNameServiceId()) {
          bitField0_ |= 0x00000001;
          nameServiceId_ = other.nameServiceId_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string nameServiceId = 1;
      private java.lang.Object nameServiceId_ = "";
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public boolean hasNameServiceId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public java.lang.String getNameServiceId() {
        java.lang.Object ref = nameServiceId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          nameServiceId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameServiceIdBytes() {
        java.lang.Object ref = nameServiceId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nameServiceId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public Builder setNameServiceId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameServiceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public Builder clearNameServiceId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        nameServiceId_ = getDefaultInstance().getNameServiceId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public Builder setNameServiceIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameServiceId_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.DisableNameserviceRequestProto)
    }

    static {
      defaultInstance = new DisableNameserviceRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.DisableNameserviceRequestProto)
  }

  public interface DisableNameserviceResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.DisableNameserviceResponseProto}
   */
  public static final class DisableNameserviceResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements DisableNameserviceResponseProtoOrBuilder {
    // Use DisableNameserviceResponseProto.newBuilder() to construct.
    private DisableNameserviceResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private DisableNameserviceResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final DisableNameserviceResponseProto defaultInstance;
    public static DisableNameserviceResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public DisableNameserviceResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private DisableNameserviceResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<DisableNameserviceResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<DisableNameserviceResponseProto>() {
      public DisableNameserviceResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new DisableNameserviceResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<DisableNameserviceResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.DisableNameserviceResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_DisableNameserviceResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.DisableNameserviceResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.DisableNameserviceResponseProto)
    }

    static {
      defaultInstance = new DisableNameserviceResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.DisableNameserviceResponseProto)
  }

  public interface EnableNameserviceRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string nameServiceId = 1;
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    boolean hasNameServiceId();
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    java.lang.String getNameServiceId();
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameServiceIdBytes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.EnableNameserviceRequestProto}
   */
  public static final class EnableNameserviceRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements EnableNameserviceRequestProtoOrBuilder {
    // Use EnableNameserviceRequestProto.newBuilder() to construct.
    private EnableNameserviceRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private EnableNameserviceRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final EnableNameserviceRequestProto defaultInstance;
    public static EnableNameserviceRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public EnableNameserviceRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private EnableNameserviceRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              nameServiceId_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<EnableNameserviceRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<EnableNameserviceRequestProto>() {
      public EnableNameserviceRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EnableNameserviceRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<EnableNameserviceRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string nameServiceId = 1;
    public static final int NAMESERVICEID_FIELD_NUMBER = 1;
    private java.lang.Object nameServiceId_;
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    public boolean hasNameServiceId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    public java.lang.String getNameServiceId() {
      java.lang.Object ref = nameServiceId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nameServiceId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nameServiceId = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameServiceIdBytes() {
      java.lang.Object ref = nameServiceId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nameServiceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      nameServiceId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getNameServiceIdBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getNameServiceIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto) obj;

      boolean result = true;
      result = result && (hasNameServiceId() == other.hasNameServiceId());
      if (hasNameServiceId()) {
        result = result && getNameServiceId()
            .equals(other.getNameServiceId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNameServiceId()) {
        hash = (37 * hash) + NAMESERVICEID_FIELD_NUMBER;
        hash = (53 * hash) + getNameServiceId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.EnableNameserviceRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nameServiceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.nameServiceId_ = nameServiceId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto.getDefaultInstance()) return this;
        if (other.hasNameServiceId()) {
          bitField0_ |= 0x00000001;
          nameServiceId_ = other.nameServiceId_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string nameServiceId = 1;
      private java.lang.Object nameServiceId_ = "";
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public boolean hasNameServiceId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public java.lang.String getNameServiceId() {
        java.lang.Object ref = nameServiceId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          nameServiceId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameServiceIdBytes() {
        java.lang.Object ref = nameServiceId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nameServiceId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public Builder setNameServiceId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameServiceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public Builder clearNameServiceId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        nameServiceId_ = getDefaultInstance().getNameServiceId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nameServiceId = 1;</code>
       */
      public Builder setNameServiceIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        nameServiceId_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.EnableNameserviceRequestProto)
    }

    static {
      defaultInstance = new EnableNameserviceRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.EnableNameserviceRequestProto)
  }

  public interface EnableNameserviceResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool status = 1;
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional bool status = 1;</code>
     */
    boolean getStatus();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.EnableNameserviceResponseProto}
   */
  public static final class EnableNameserviceResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements EnableNameserviceResponseProtoOrBuilder {
    // Use EnableNameserviceResponseProto.newBuilder() to construct.
    private EnableNameserviceResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private EnableNameserviceResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final EnableNameserviceResponseProto defaultInstance;
    public static EnableNameserviceResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public EnableNameserviceResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private EnableNameserviceResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              status_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<EnableNameserviceResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<EnableNameserviceResponseProto>() {
      public EnableNameserviceResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EnableNameserviceResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<EnableNameserviceResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private boolean status_;
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool status = 1;</code>
     */
    public boolean getStatus() {
      return status_;
    }

    private void initFields() {
      status_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && (getStatus()
            == other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.EnableNameserviceResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        status_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_EnableNameserviceResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.status_ = status_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.EnableNameserviceResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool status = 1;
      private boolean status_ ;
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public boolean getStatus() {
        return status_;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder setStatus(boolean value) {
        bitField0_ |= 0x00000001;
        status_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool status = 1;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.EnableNameserviceResponseProto)
    }

    static {
      defaultInstance = new EnableNameserviceResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.EnableNameserviceResponseProto)
  }

  public interface GetDisabledNameservicesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetDisabledNameservicesRequestProto}
   */
  public static final class GetDisabledNameservicesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetDisabledNameservicesRequestProtoOrBuilder {
    // Use GetDisabledNameservicesRequestProto.newBuilder() to construct.
    private GetDisabledNameservicesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetDisabledNameservicesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetDisabledNameservicesRequestProto defaultInstance;
    public static GetDisabledNameservicesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetDisabledNameservicesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetDisabledNameservicesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetDisabledNameservicesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetDisabledNameservicesRequestProto>() {
      public GetDisabledNameservicesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetDisabledNameservicesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetDisabledNameservicesRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetDisabledNameservicesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetDisabledNameservicesRequestProto)
    }

    static {
      defaultInstance = new GetDisabledNameservicesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetDisabledNameservicesRequestProto)
  }

  public interface GetDisabledNameservicesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated string nameServiceIds = 1;
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    java.util.List<java.lang.String>
    getNameServiceIdsList();
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    int getNameServiceIdsCount();
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    java.lang.String getNameServiceIds(int index);
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameServiceIdsBytes(int index);
  }
  /**
   * Protobuf type {@code hadoop.hdfs.GetDisabledNameservicesResponseProto}
   */
  public static final class GetDisabledNameservicesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetDisabledNameservicesResponseProtoOrBuilder {
    // Use GetDisabledNameservicesResponseProto.newBuilder() to construct.
    private GetDisabledNameservicesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetDisabledNameservicesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetDisabledNameservicesResponseProto defaultInstance;
    public static GetDisabledNameservicesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetDisabledNameservicesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetDisabledNameservicesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                nameServiceIds_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              nameServiceIds_.add(input.readBytes());
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          nameServiceIds_ = new com.google.protobuf.UnmodifiableLazyStringList(nameServiceIds_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetDisabledNameservicesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetDisabledNameservicesResponseProto>() {
      public GetDisabledNameservicesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetDisabledNameservicesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetDisabledNameservicesResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated string nameServiceIds = 1;
    public static final int NAMESERVICEIDS_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList nameServiceIds_;
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    public java.util.List<java.lang.String>
        getNameServiceIdsList() {
      return nameServiceIds_;
    }
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    public int getNameServiceIdsCount() {
      return nameServiceIds_.size();
    }
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    public java.lang.String getNameServiceIds(int index) {
      return nameServiceIds_.get(index);
    }
    /**
     * <code>repeated string nameServiceIds = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameServiceIdsBytes(int index) {
      return nameServiceIds_.getByteString(index);
    }

    private void initFields() {
      nameServiceIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < nameServiceIds_.size(); i++) {
        output.writeBytes(1, nameServiceIds_.getByteString(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < nameServiceIds_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(nameServiceIds_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getNameServiceIdsList().size();
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto other = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto) obj;

      boolean result = true;
      result = result && getNameServiceIdsList()
          .equals(other.getNameServiceIdsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNameServiceIdsCount() > 0) {
        hash = (37 * hash) + NAMESERVICEIDS_FIELD_NUMBER;
        hash = (53 * hash) + getNameServiceIdsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.GetDisabledNameservicesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.class, org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nameServiceIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_descriptor;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto build() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto buildPartial() {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto result = new org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          nameServiceIds_ = new com.google.protobuf.UnmodifiableLazyStringList(
              nameServiceIds_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.nameServiceIds_ = nameServiceIds_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto) {
          return mergeFrom((org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto other) {
        if (other == org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.getDefaultInstance()) return this;
        if (!other.nameServiceIds_.isEmpty()) {
          if (nameServiceIds_.isEmpty()) {
            nameServiceIds_ = other.nameServiceIds_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureNameServiceIdsIsMutable();
            nameServiceIds_.addAll(other.nameServiceIds_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos.GetDisabledNameservicesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated string nameServiceIds = 1;
      private com.google.protobuf.LazyStringList nameServiceIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureNameServiceIdsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          nameServiceIds_ = new com.google.protobuf.LazyStringArrayList(nameServiceIds_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public java.util.List<java.lang.String>
          getNameServiceIdsList() {
        return java.util.Collections.unmodifiableList(nameServiceIds_);
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public int getNameServiceIdsCount() {
        return nameServiceIds_.size();
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public java.lang.String getNameServiceIds(int index) {
        return nameServiceIds_.get(index);
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameServiceIdsBytes(int index) {
        return nameServiceIds_.getByteString(index);
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public Builder setNameServiceIds(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNameServiceIdsIsMutable();
        nameServiceIds_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public Builder addNameServiceIds(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNameServiceIdsIsMutable();
        nameServiceIds_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public Builder addAllNameServiceIds(
          java.lang.Iterable<java.lang.String> values) {
        ensureNameServiceIdsIsMutable();
        super.addAll(values, nameServiceIds_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public Builder clearNameServiceIds() {
        nameServiceIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nameServiceIds = 1;</code>
       */
      public Builder addNameServiceIdsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNameServiceIdsIsMutable();
        nameServiceIds_.add(value);
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.GetDisabledNameservicesResponseProto)
    }

    static {
      defaultInstance = new GetDisabledNameservicesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.GetDisabledNameservicesResponseProto)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_FederationNamespaceInfoProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_FederationNamespaceInfoProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_RemoteLocationProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_RemoteLocationProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_MountTableRecordProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_MountTableRecordProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_StateStoreVersionRecordProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_StateStoreVersionRecordProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_RouterRecordProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_RouterRecordProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_EnterSafeModeRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_EnterSafeModeRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_EnterSafeModeResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_EnterSafeModeResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetSafeModeRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetSafeModeRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetSafeModeResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetSafeModeResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_DisableNameserviceRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_DisableNameserviceRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_DisableNameserviceResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_DisableNameserviceResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_EnableNameserviceRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_EnableNameserviceRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_EnableNameserviceResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_EnableNameserviceResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\030FederationProtocol.proto\022\013hadoop.hdfs\032" +
      "\nhdfs.proto\"\300\003\n\"NamenodeMembershipStatsR" +
      "ecordProto\022\022\n\ntotalSpace\030\001 \001(\004\022\026\n\016availa" +
      "bleSpace\030\002 \001(\004\022\025\n\rprovidedSpace\030\003 \001(\004\022\022\n" +
      "\nnumOfFiles\030\n \001(\004\022\023\n\013numOfBlocks\030\013 \001(\004\022\032" +
      "\n\022numOfBlocksMissing\030\014 \001(\004\022%\n\035numOfBlock" +
      "sPendingReplication\030\r \001(\004\022\"\n\032numOfBlocks" +
      "UnderReplicated\030\016 \001(\004\022\"\n\032numOfBlocksPend" +
      "ingDeletion\030\017 \001(\004\022\034\n\024numOfActiveDatanode" +
      "s\030\024 \001(\r\022\032\n\022numOfDeadDatanodes\030\025 \001(\r\022%\n\035n",
      "umOfDecommissioningDatanodes\030\026 \001(\r\022!\n\031nu" +
      "mOfDecomActiveDatanodes\030\027 \001(\r\022\037\n\027numOfDe" +
      "comDeadDatanodes\030\030 \001(\r\"\200\003\n\035NamenodeMembe" +
      "rshipRecordProto\022\023\n\013dateCreated\030\001 \001(\004\022\024\n" +
      "\014dateModified\030\002 \001(\004\022\023\n\013lastContact\030\003 \001(\004" +
      "\022\020\n\010routerId\030\004 \001(\t\022\025\n\rnameserviceId\030\005 \001(" +
      "\t\022\022\n\nnamenodeId\030\006 \001(\t\022\021\n\tclusterId\030\007 \001(\t" +
      "\022\023\n\013blockPoolId\030\010 \001(\t\022\022\n\nwebAddress\030\t \001(" +
      "\t\022\022\n\nrpcAddress\030\n \001(\t\022\026\n\016serviceAddress\030" +
      "\013 \001(\t\022\027\n\017lifelineAddress\030\014 \001(\t\022\r\n\005state\030",
      "\r \001(\t\022\022\n\nisSafeMode\030\016 \001(\010\022>\n\005stats\030\017 \001(\013" +
      "2/.hadoop.hdfs.NamenodeMembershipStatsRe" +
      "cordProto\"]\n\034FederationNamespaceInfoProt" +
      "o\022\023\n\013blockPoolId\030\001 \001(\t\022\021\n\tclusterId\030\002 \001(" +
      "\t\022\025\n\rnameserviceId\030\003 \001(\t\"f\n$GetNamenodeR" +
      "egistrationsRequestProto\022>\n\nmembership\030\001" +
      " \001(\0132*.hadoop.hdfs.NamenodeMembershipRec" +
      "ordProto\"p\n%GetNamenodeRegistrationsResp" +
      "onseProto\022G\n\023namenodeMemberships\030\001 \003(\0132*" +
      ".hadoop.hdfs.NamenodeMembershipRecordPro",
      "to\"%\n#GetExpiredRegistrationsRequestProt" +
      "o\"\036\n\034GetNamespaceInfoRequestProto\"b\n\035Get" +
      "NamespaceInfoResponseProto\022A\n\016namespaceI" +
      "nfos\030\001 \003(\0132).hadoop.hdfs.FederationNames" +
      "paceInfoProto\"b\n&UpdateNamenodeRegistrat" +
      "ionRequestProto\022\025\n\rnameserviceId\030\001 \001(\t\022\022" +
      "\n\nnamenodeId\030\002 \001(\t\022\r\n\005state\030\003 \001(\t\"9\n\'Upd" +
      "ateNamenodeRegistrationResponseProto\022\016\n\006" +
      "status\030\001 \001(\010\"g\n\035NamenodeHeartbeatRequest" +
      "Proto\022F\n\022namenodeMembership\030\001 \001(\0132*.hado",
      "op.hdfs.NamenodeMembershipRecordProto\"0\n" +
      "\036NamenodeHeartbeatResponseProto\022\016\n\006statu" +
      "s\030\001 \001(\010\":\n\023RemoteLocationProto\022\025\n\rnamese" +
      "rviceId\030\001 \001(\t\022\014\n\004path\030\002 \001(\t\"\223\003\n\025MountTab" +
      "leRecordProto\022\017\n\007srcPath\030\001 \001(\t\0226\n\014destin" +
      "ations\030\002 \003(\0132 .hadoop.hdfs.RemoteLocatio" +
      "nProto\022\023\n\013dateCreated\030\003 \001(\004\022\024\n\014dateModif" +
      "ied\030\004 \001(\004\022\027\n\010readOnly\030\005 \001(\010:\005false\022E\n\tde" +
      "stOrder\030\006 \001(\0162,.hadoop.hdfs.MountTableRe" +
      "cordProto.DestOrder:\004HASH\022\021\n\townerName\030\n",
      " \001(\t\022\021\n\tgroupName\030\013 \001(\t\022\014\n\004mode\030\014 \001(\005\022+\n" +
      "\005quota\030\r \001(\0132\034.hadoop.hdfs.QuotaUsagePro" +
      "to\"E\n\tDestOrder\022\010\n\004HASH\020\000\022\t\n\005LOCAL\020\001\022\n\n\006" +
      "RANDOM\020\002\022\014\n\010HASH_ALL\020\003\022\t\n\005SPACE\020\004\"S\n\036Add" +
      "MountTableEntryRequestProto\0221\n\005entry\030\001 \001" +
      "(\0132\".hadoop.hdfs.MountTableRecordProto\"1" +
      "\n\037AddMountTableEntryResponseProto\022\016\n\006sta" +
      "tus\030\001 \001(\010\"V\n!UpdateMountTableEntryReques" +
      "tProto\0221\n\005entry\030\001 \001(\0132\".hadoop.hdfs.Moun" +
      "tTableRecordProto\"4\n\"UpdateMountTableEnt",
      "ryResponseProto\022\016\n\006status\030\001 \001(\010\"4\n!Remov" +
      "eMountTableEntryRequestProto\022\017\n\007srcPath\030" +
      "\001 \001(\t\"4\n\"RemoveMountTableEntryResponsePr" +
      "oto\022\016\n\006status\030\001 \001(\010\"3\n GetMountTableEntr" +
      "iesRequestProto\022\017\n\007srcPath\030\001 \001(\t\"k\n!GetM" +
      "ountTableEntriesResponseProto\0223\n\007entries" +
      "\030\001 \003(\0132\".hadoop.hdfs.MountTableRecordPro" +
      "to\022\021\n\ttimestamp\030\002 \001(\004\"T\n\034StateStoreVersi" +
      "onRecordProto\022\031\n\021membershipVersion\030\001 \001(\004" +
      "\022\031\n\021mountTableVersion\030\002 \001(\004\"\340\001\n\021RouterRe",
      "cordProto\022\023\n\013dateCreated\030\001 \001(\004\022\024\n\014dateMo" +
      "dified\030\002 \001(\004\022\017\n\007address\030\003 \001(\t\022\016\n\006status\030" +
      "\004 \001(\t\022D\n\021stateStoreVersion\030\005 \001(\0132).hadoo" +
      "p.hdfs.StateStoreVersionRecordProto\022\017\n\007v" +
      "ersion\030\006 \001(\t\022\023\n\013compileInfo\030\007 \001(\t\022\023\n\013dat" +
      "eStarted\030\010 \001(\004\"5\n!GetRouterRegistrationR" +
      "equestProto\022\020\n\010routerId\030\001 \001(\t\"T\n\"GetRout" +
      "erRegistrationResponseProto\022.\n\006router\030\001 " +
      "\001(\0132\036.hadoop.hdfs.RouterRecordProto\"$\n\"G" +
      "etRouterRegistrationsRequestProto\"i\n#Get",
      "RouterRegistrationsResponseProto\022\021\n\ttime" +
      "stamp\030\001 \001(\004\022/\n\007routers\030\002 \003(\0132\036.hadoop.hd" +
      "fs.RouterRecordProto\"M\n\033RouterHeartbeatR" +
      "equestProto\022.\n\006router\030\001 \001(\0132\036.hadoop.hdf" +
      "s.RouterRecordProto\".\n\034RouterHeartbeatRe" +
      "sponseProto\022\016\n\006status\030\001 \001(\010\"\033\n\031EnterSafe" +
      "ModeRequestProto\",\n\032EnterSafeModeRespons" +
      "eProto\022\016\n\006status\030\001 \001(\010\"\033\n\031LeaveSafeModeR" +
      "equestProto\",\n\032LeaveSafeModeResponseProt" +
      "o\022\016\n\006status\030\001 \001(\010\"\031\n\027GetSafeModeRequestP",
      "roto\"0\n\030GetSafeModeResponseProto\022\024\n\014isIn" +
      "SafeMode\030\001 \001(\010\"b\n\036DisabledNameserviceRec" +
      "ordProto\022\023\n\013dateCreated\030\001 \001(\004\022\024\n\014dateMod" +
      "ified\030\002 \001(\004\022\025\n\rnameServiceId\030\003 \001(\t\"7\n\036Di" +
      "sableNameserviceRequestProto\022\025\n\rnameServ" +
      "iceId\030\001 \001(\t\"1\n\037DisableNameserviceRespons" +
      "eProto\022\016\n\006status\030\001 \001(\010\"6\n\035EnableNameserv" +
      "iceRequestProto\022\025\n\rnameServiceId\030\001 \001(\t\"0" +
      "\n\036EnableNameserviceResponseProto\022\016\n\006stat" +
      "us\030\001 \001(\010\"%\n#GetDisabledNameservicesReque",
      "stProto\">\n$GetDisabledNameservicesRespon" +
      "seProto\022\026\n\016nameServiceIds\030\001 \003(\tBT\n0org.a" +
      "pache.hadoop.hdfs.federation.protocol.pr" +
      "otoB\032HdfsServerFederationProtos\210\001\001\240\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_NamenodeMembershipStatsRecordProto_descriptor,
              new java.lang.String[] { "TotalSpace", "AvailableSpace", "ProvidedSpace", "NumOfFiles", "NumOfBlocks", "NumOfBlocksMissing", "NumOfBlocksPendingReplication", "NumOfBlocksUnderReplicated", "NumOfBlocksPendingDeletion", "NumOfActiveDatanodes", "NumOfDeadDatanodes", "NumOfDecommissioningDatanodes", "NumOfDecomActiveDatanodes", "NumOfDecomDeadDatanodes", });
          internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_NamenodeMembershipRecordProto_descriptor,
              new java.lang.String[] { "DateCreated", "DateModified", "LastContact", "RouterId", "NameserviceId", "NamenodeId", "ClusterId", "BlockPoolId", "WebAddress", "RpcAddress", "ServiceAddress", "LifelineAddress", "State", "IsSafeMode", "Stats", });
          internal_static_hadoop_hdfs_FederationNamespaceInfoProto_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_hadoop_hdfs_FederationNamespaceInfoProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_FederationNamespaceInfoProto_descriptor,
              new java.lang.String[] { "BlockPoolId", "ClusterId", "NameserviceId", });
          internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetNamenodeRegistrationsRequestProto_descriptor,
              new java.lang.String[] { "Membership", });
          internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetNamenodeRegistrationsResponseProto_descriptor,
              new java.lang.String[] { "NamenodeMemberships", });
          internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetExpiredRegistrationsRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetNamespaceInfoRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetNamespaceInfoResponseProto_descriptor,
              new java.lang.String[] { "NamespaceInfos", });
          internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_UpdateNamenodeRegistrationRequestProto_descriptor,
              new java.lang.String[] { "NameserviceId", "NamenodeId", "State", });
          internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_UpdateNamenodeRegistrationResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(10);
          internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_NamenodeHeartbeatRequestProto_descriptor,
              new java.lang.String[] { "NamenodeMembership", });
          internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(11);
          internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_NamenodeHeartbeatResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_RemoteLocationProto_descriptor =
            getDescriptor().getMessageTypes().get(12);
          internal_static_hadoop_hdfs_RemoteLocationProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_RemoteLocationProto_descriptor,
              new java.lang.String[] { "NameserviceId", "Path", });
          internal_static_hadoop_hdfs_MountTableRecordProto_descriptor =
            getDescriptor().getMessageTypes().get(13);
          internal_static_hadoop_hdfs_MountTableRecordProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_MountTableRecordProto_descriptor,
              new java.lang.String[] { "SrcPath", "Destinations", "DateCreated", "DateModified", "ReadOnly", "DestOrder", "OwnerName", "GroupName", "Mode", "Quota", });
          internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(14);
          internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_AddMountTableEntryRequestProto_descriptor,
              new java.lang.String[] { "Entry", });
          internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(15);
          internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_AddMountTableEntryResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(16);
          internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_UpdateMountTableEntryRequestProto_descriptor,
              new java.lang.String[] { "Entry", });
          internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(17);
          internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_UpdateMountTableEntryResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(18);
          internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_RemoveMountTableEntryRequestProto_descriptor,
              new java.lang.String[] { "SrcPath", });
          internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(19);
          internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_RemoveMountTableEntryResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(20);
          internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetMountTableEntriesRequestProto_descriptor,
              new java.lang.String[] { "SrcPath", });
          internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(21);
          internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetMountTableEntriesResponseProto_descriptor,
              new java.lang.String[] { "Entries", "Timestamp", });
          internal_static_hadoop_hdfs_StateStoreVersionRecordProto_descriptor =
            getDescriptor().getMessageTypes().get(22);
          internal_static_hadoop_hdfs_StateStoreVersionRecordProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_StateStoreVersionRecordProto_descriptor,
              new java.lang.String[] { "MembershipVersion", "MountTableVersion", });
          internal_static_hadoop_hdfs_RouterRecordProto_descriptor =
            getDescriptor().getMessageTypes().get(23);
          internal_static_hadoop_hdfs_RouterRecordProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_RouterRecordProto_descriptor,
              new java.lang.String[] { "DateCreated", "DateModified", "Address", "Status", "StateStoreVersion", "Version", "CompileInfo", "DateStarted", });
          internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(24);
          internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetRouterRegistrationRequestProto_descriptor,
              new java.lang.String[] { "RouterId", });
          internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(25);
          internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetRouterRegistrationResponseProto_descriptor,
              new java.lang.String[] { "Router", });
          internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(26);
          internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetRouterRegistrationsRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(27);
          internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetRouterRegistrationsResponseProto_descriptor,
              new java.lang.String[] { "Timestamp", "Routers", });
          internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(28);
          internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_RouterHeartbeatRequestProto_descriptor,
              new java.lang.String[] { "Router", });
          internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(29);
          internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_RouterHeartbeatResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_EnterSafeModeRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(30);
          internal_static_hadoop_hdfs_EnterSafeModeRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_EnterSafeModeRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_EnterSafeModeResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(31);
          internal_static_hadoop_hdfs_EnterSafeModeResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_EnterSafeModeResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(32);
          internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_LeaveSafeModeRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(33);
          internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_LeaveSafeModeResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_GetSafeModeRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(34);
          internal_static_hadoop_hdfs_GetSafeModeRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetSafeModeRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_GetSafeModeResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(35);
          internal_static_hadoop_hdfs_GetSafeModeResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetSafeModeResponseProto_descriptor,
              new java.lang.String[] { "IsInSafeMode", });
          internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_descriptor =
            getDescriptor().getMessageTypes().get(36);
          internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_DisabledNameserviceRecordProto_descriptor,
              new java.lang.String[] { "DateCreated", "DateModified", "NameServiceId", });
          internal_static_hadoop_hdfs_DisableNameserviceRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(37);
          internal_static_hadoop_hdfs_DisableNameserviceRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_DisableNameserviceRequestProto_descriptor,
              new java.lang.String[] { "NameServiceId", });
          internal_static_hadoop_hdfs_DisableNameserviceResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(38);
          internal_static_hadoop_hdfs_DisableNameserviceResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_DisableNameserviceResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_EnableNameserviceRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(39);
          internal_static_hadoop_hdfs_EnableNameserviceRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_EnableNameserviceRequestProto_descriptor,
              new java.lang.String[] { "NameServiceId", });
          internal_static_hadoop_hdfs_EnableNameserviceResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(40);
          internal_static_hadoop_hdfs_EnableNameserviceResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_EnableNameserviceResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(41);
          internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetDisabledNameservicesRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(42);
          internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_GetDisabledNameservicesResponseProto_descriptor,
              new java.lang.String[] { "NameServiceIds", });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
